{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Importing required libraries -"
      ],
      "metadata": {
        "id": "f3vxh25bpWr0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "KezhPSA2ltD2"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import f1_score, accuracy_score\n",
        "from sklearn.model_selection import StratifiedKFold, train_test_split"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Importing & exploring Bank Note (i.e. bank note authentication) dataset (downloaded from UCI ML repo) -"
      ],
      "metadata": {
        "id": "Vr-19h99pZWp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/data_banknote_authentication.txt', header=None)\n",
        "print(df.shape)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223
        },
        "id": "CGKnWXs0l50v",
        "outputId": "55fb525b-da8b-4e6d-a405-35115a89c38b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1372, 5)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         0       1       2        3  4\n",
              "0  3.62160  8.6661 -2.8073 -0.44699  0\n",
              "1  4.54590  8.1674 -2.4586 -1.46210  0\n",
              "2  3.86600 -2.6383  1.9242  0.10645  0\n",
              "3  3.45660  9.5228 -4.0112 -3.59440  0\n",
              "4  0.32924 -4.4552  4.5718 -0.98880  0"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-2680e183-db5b-45e3-85ce-3b90a8930ae1\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.62160</td>\n",
              "      <td>8.6661</td>\n",
              "      <td>-2.8073</td>\n",
              "      <td>-0.44699</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.54590</td>\n",
              "      <td>8.1674</td>\n",
              "      <td>-2.4586</td>\n",
              "      <td>-1.46210</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.86600</td>\n",
              "      <td>-2.6383</td>\n",
              "      <td>1.9242</td>\n",
              "      <td>0.10645</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.45660</td>\n",
              "      <td>9.5228</td>\n",
              "      <td>-4.0112</td>\n",
              "      <td>-3.59440</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.32924</td>\n",
              "      <td>-4.4552</td>\n",
              "      <td>4.5718</td>\n",
              "      <td>-0.98880</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2680e183-db5b-45e3-85ce-3b90a8930ae1')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-d2b69151-7c84-4638-ab2e-39de68eb9343\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-d2b69151-7c84-4638-ab2e-39de68eb9343')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-d2b69151-7c84-4638-ab2e-39de68eb9343 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-2680e183-db5b-45e3-85ce-3b90a8930ae1 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-2680e183-db5b-45e3-85ce-3b90a8930ae1');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Renaming column names -\n",
        "df.rename(columns={0:'f1', 1:'f2', 2:'f3', 3:'f4', 4:'target'}, inplace=True)\n",
        "df.head()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "nGz4k9aWqtPT",
        "outputId": "0426b918-6bc8-4cb1-eba9-06dac330f63f"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "        f1      f2      f3       f4  target\n",
              "0  3.62160  8.6661 -2.8073 -0.44699       0\n",
              "1  4.54590  8.1674 -2.4586 -1.46210       0\n",
              "2  3.86600 -2.6383  1.9242  0.10645       0\n",
              "3  3.45660  9.5228 -4.0112 -3.59440       0\n",
              "4  0.32924 -4.4552  4.5718 -0.98880       0"
            ],
            "text/html": [
              "\n",
              "\n",
              "  <div id=\"df-d1522cb3-d605-4391-9feb-49d1d40b1b36\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>f1</th>\n",
              "      <th>f2</th>\n",
              "      <th>f3</th>\n",
              "      <th>f4</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>3.62160</td>\n",
              "      <td>8.6661</td>\n",
              "      <td>-2.8073</td>\n",
              "      <td>-0.44699</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.54590</td>\n",
              "      <td>8.1674</td>\n",
              "      <td>-2.4586</td>\n",
              "      <td>-1.46210</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>3.86600</td>\n",
              "      <td>-2.6383</td>\n",
              "      <td>1.9242</td>\n",
              "      <td>0.10645</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3.45660</td>\n",
              "      <td>9.5228</td>\n",
              "      <td>-4.0112</td>\n",
              "      <td>-3.59440</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.32924</td>\n",
              "      <td>-4.4552</td>\n",
              "      <td>4.5718</td>\n",
              "      <td>-0.98880</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d1522cb3-d605-4391-9feb-49d1d40b1b36')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "\n",
              "\n",
              "\n",
              "    <div id=\"df-10a31f66-18cc-4ea5-9a33-d5a255155be7\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-10a31f66-18cc-4ea5-9a33-d5a255155be7')\"\n",
              "              title=\"Suggest charts.\"\n",
              "              style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "    </div>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "    background-color: #E8F0FE;\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: #1967D2;\n",
              "    height: 32px;\n",
              "    padding: 0 0 0 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: #E2EBFA;\n",
              "    box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: #174EA6;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "    background-color: #3B4455;\n",
              "    fill: #D2E3FC;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart:hover {\n",
              "    background-color: #434B5C;\n",
              "    box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "    filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "    fill: #FFFFFF;\n",
              "  }\n",
              "</style>\n",
              "\n",
              "    <script>\n",
              "      async function quickchart(key) {\n",
              "        const containerElement = document.querySelector('#' + key);\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      }\n",
              "    </script>\n",
              "\n",
              "      <script>\n",
              "\n",
              "function displayQuickchartButton(domScope) {\n",
              "  let quickchartButtonEl =\n",
              "    domScope.querySelector('#df-10a31f66-18cc-4ea5-9a33-d5a255155be7 button.colab-df-quickchart');\n",
              "  quickchartButtonEl.style.display =\n",
              "    google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "}\n",
              "\n",
              "        displayQuickchartButton(document);\n",
              "      </script>\n",
              "      <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d1522cb3-d605-4391-9feb-49d1d40b1b36 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d1522cb3-d605-4391-9feb-49d1d40b1b36');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking datatypes and missing values -\n",
        "print(df.info())\n",
        "print(df.isnull().sum())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7hQUjchEl53u",
        "outputId": "9458694c-167f-47c7-c2ec-fc88357aa5ca"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 1372 entries, 0 to 1371\n",
            "Data columns (total 5 columns):\n",
            " #   Column  Non-Null Count  Dtype  \n",
            "---  ------  --------------  -----  \n",
            " 0   f1      1372 non-null   float64\n",
            " 1   f2      1372 non-null   float64\n",
            " 2   f3      1372 non-null   float64\n",
            " 3   f4      1372 non-null   float64\n",
            " 4   target  1372 non-null   int64  \n",
            "dtypes: float64(4), int64(1)\n",
            "memory usage: 53.7 KB\n",
            "None\n",
            "f1        0\n",
            "f2        0\n",
            "f3        0\n",
            "f4        0\n",
            "target    0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### There are no missing values and the datatypes are appropriate"
      ],
      "metadata": {
        "id": "WMVQ9G1irgpI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Visualizing target distribution -\n",
        "round(df.target.value_counts(normalize=True).sort_index(ascending=True)*100, 2)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PAaBM3ugl56f",
        "outputId": "30ad06fc-9481-493a-82ed-122ef307e64e"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    55.54\n",
              "1    44.46\n",
              "Name: target, dtype: float64"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### From the above we can see the slight imbalance in the target distribution which will be taken care further by using the right metrics and performing K Fold Stratified Cross Validation"
      ],
      "metadata": {
        "id": "w_dOO0_XtitF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dividing the dataset into k folds (k=5) -"
      ],
      "metadata": {
        "id": "uHXuXDAut5oM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['kfold'] = -1\n",
        "df = df.sample(frac=1, random_state=100).reset_index(drop=True)\n",
        "skf = StratifiedKFold(n_splits=5)\n",
        "\n",
        "# Filling the new kfold column -\n",
        "for f, (t_, v_) in enumerate(skf.split(X=df, y=df.target)):\n",
        "    df.loc[v_, 'kfold'] = f"
      ],
      "metadata": {
        "id": "SJM4QRjPl59D"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking the no. of samples in each fold -\n",
        "df.kfold.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PGv9i3r--g_N",
        "outputId": "e463c6a8-e739-47cf-8423-c856164659db"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    275\n",
              "1    275\n",
              "2    274\n",
              "3    274\n",
              "4    274\n",
              "Name: kfold, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defining some helper functions to build and train the Artificial Neural Network (ANN) -"
      ],
      "metadata": {
        "id": "kBa3eCe2_2P1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Given: To create an ANN with 1 hidden layer and 1 output layer.\n",
        "### The activation function of the hidden layer is represented by the general form: g(x) = k0 + k1 * x, and its parameters are learnable. Activation function of the output layer will be sigmoid since its a binary classification problem."
      ],
      "metadata": {
        "id": "x_M7NJyRAbxW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the no. of layers -\n",
        "def layer_size(hid_nodes=8):\n",
        "    n_x = 4 # no. of input layer neurons = no. of features of the dataset\n",
        "    n_h1 = hid_nodes # no. of hidden layer neurons will be the no. that gives the best performance i.e. best value of metrics and least overfitting\n",
        "    n_op = 1 # no. of output layer neurons = 1, since binary classification problem\n",
        "    return (n_x, n_h1, n_op)"
      ],
      "metadata": {
        "id": "_NvQdxcF-hNg"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# No. of hidden + output layers -\n",
        "n_layers = 2"
      ],
      "metadata": {
        "id": "DeMVzass-hRK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initializing the weights and biases -\n",
        "def init_params(*args):\n",
        "    np.random.seed(1)\n",
        "    model_params = dict()\n",
        "    for i in range(1, len(args)):\n",
        "        model_params[f'W{i}'] = np.random.randn(args[i], args[i - 1]) * 0.01\n",
        "        model_params[f'b{i}'] = np.zeros((args[i], 1))\n",
        "    model_params[f'K'] = np.random.randn(2) # since according to given assignment, there are 2 parameters: k0 and k1\n",
        "    return model_params"
      ],
      "metadata": {
        "id": "NHrsvr4A-hSB"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the sigmoid function -\n",
        "def sigmoid(z):\n",
        "    return (1 / (1 + np.exp(-z)))"
      ],
      "metadata": {
        "id": "W5i32AGEDI8A"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining adaptable activation function (for the 1st hidden layer) and its derivative -\n",
        "def ada_act(k0, k1, z):\n",
        "  return k0 + (k1 * z) # as per the provided assignment\n",
        "\n",
        "# dK1 (naming convention as per assignment provided) array below will have 2 elements since there are 2 learnable parameters in the ada_act above\n",
        "def d_ada_act(da1, z1):\n",
        "  return np.array([np.mean(da1.flatten()), np.mean(np.multiply(da1, z1).flatten())])"
      ],
      "metadata": {
        "id": "GqqZEXuwLoPY"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Forward propagation -\n",
        "def fwd_prop(X, model_params):\n",
        "    catch = dict() # to store computed zi, ai values\n",
        "    for i in range(n_layers):\n",
        "        if i == 0:\n",
        "            catch[f'Z{i + 1}'] = np.matmul(model_params[f'W{i + 1}'], X) + model_params[f'b{i + 1}']\n",
        "        else:\n",
        "            catch[f'Z{i + 1}'] = np.matmul(model_params[f'W{i + 1}'], catch[f'a{i}']) + model_params[f'b{i + 1}']\n",
        "\n",
        "        if i != (n_layers - 1):\n",
        "            catch[f'a{i + 1}'] = ada_act(model_params[f'K'][0], model_params[f'K'][1], catch[f'Z{i + 1}'])\n",
        "        else:\n",
        "            catch[f'a{i + 1}'] = sigmoid(catch[f'Z{i + 1}'])\n",
        "\n",
        "    return catch"
      ],
      "metadata": {
        "id": "RsRKZ-miDJPU"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the Binary Cross Entropy cost function -\n",
        "def compute_cost(a2, y):\n",
        "    m = y.size\n",
        "    cost = -np.sum(np.multiply(y, np.log(a2)) + np.multiply((1 - y), np.log(1 - a2))) / m\n",
        "    return cost"
      ],
      "metadata": {
        "id": "xvG6psPft7uv"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Defining the function for backward propagation -\n",
        "def bwd_prop(model_params, catch, X, y):\n",
        "    m = y.size\n",
        "    gradients = dict()\n",
        "    W1, b1 = model_params['W1'], model_params['b1']\n",
        "    W2, b2 = model_params['W2'], model_params['b2']\n",
        "    k0, k1 = model_params['K'][0], model_params['K'][1]\n",
        "\n",
        "    a1, a2 = catch['a1'], catch['a2']\n",
        "    Z1, Z2 = catch['Z1'], catch['Z2']\n",
        "\n",
        "    dZ2 = a2 - y\n",
        "    dW2 = np.matmul(dZ2, a1.T) / m\n",
        "    db2 = np.sum(dZ2, axis=1, keepdims=True) / m\n",
        "    da1 = np.matmul(W2.T, dZ2)\n",
        "\n",
        "    dZ1 = k1 * da1 # derivative of k0 + k1*z wrt z is k1\n",
        "    dW1 = np.matmul(dZ1, X.T) / m\n",
        "    db1 = np.sum(dZ1, axis=1, keepdims=True) / m\n",
        "    dK1 = d_ada_act(da1,  Z1)\n",
        "\n",
        "    gradients = {\"dW1\":dW1, \"db1\":db1, \"dW2\":dW2, \"db2\":db2, \"dK\":dK1} # dK = dK1, since only 1 hidden layer\n",
        "\n",
        "    return gradients"
      ],
      "metadata": {
        "id": "SyJ3HYR36YNw"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Updating the model parameters -\n",
        "def update(model_params, gradients, lr):\n",
        "    for i in range(n_layers, 0, -1):\n",
        "        model_params[f'W{i}'] = model_params[f'W{i}'] - (lr * gradients[f'dW{i}'])\n",
        "        model_params[f'b{i}'] = model_params[f'b{i}'] - (lr * gradients[f'db{i}'])\n",
        "    model_params[f'K'] = model_params[f'K'] - (lr * gradients[f'dK'])\n",
        "\n",
        "    return model_params"
      ],
      "metadata": {
        "id": "Dv8FBFiy6YPb"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Main driver function for Batch Gradient Descent -\n",
        "def NN(X_train, y_train, X_test, y_test, alpha, itr, hid_nodes):\n",
        "    past_cost_tr, past_cost_te = [], []\n",
        "    n_x, n_h1, n_op = layer_size(hid_nodes)\n",
        "    model_params = init_params(n_x, n_h1, n_op)\n",
        "    print(f\"Initial params: {model_params}\")\n",
        "\n",
        "    for i in range(0, itr):\n",
        "        print(f\"Epoch {i+1} -\")\n",
        "        catch = fwd_prop(X_train, model_params)\n",
        "        tr_cost = compute_cost(catch['a2'], y_train)\n",
        "        gradients = bwd_prop(model_params, catch, X_train, y_train)\n",
        "        model_params = update(model_params, gradients, alpha)\n",
        "        print(f\"Updated K params after epoch {i+1}: {model_params['K']}\")\n",
        "        past_cost_tr.append(tr_cost)\n",
        "\n",
        "        # Evaluating on test data after every epoch-\n",
        "        catch = fwd_prop(X_test, model_params)\n",
        "        te_cost = compute_cost(catch['a2'], y_test)\n",
        "        print(f\"Train loss: {tr_cost}, Test loss: {te_cost}\")\n",
        "        past_cost_te.append(te_cost)\n",
        "        print(\"\\n\")\n",
        "\n",
        "    return past_cost_tr, model_params, past_cost_te"
      ],
      "metadata": {
        "id": "weFDdGJBE9Su"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Performing K fold Cross Validation -"
      ],
      "metadata": {
        "id": "dOYddHH4Ip74"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Rough work (shapes) -\n",
        "#### num of hidden layer nodes = 8\n",
        "X -> (4, 1000), w1 -> (8, 4), b1 -> (8, 1), z1/a1 -> (8, 1000)\n",
        "w2 -> (1, 8), b2 -> (1, 1), z2/a2 -> (1, 1000)"
      ],
      "metadata": {
        "id": "4XxMQ2dNPWMe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for kfold in range(df.kfold.nunique()):\n",
        "    print(f\"Fold {kfold+1} -\")\n",
        "    train_f1s, test_f1s = [], []\n",
        "    train_accs, test_accs = [], []\n",
        "\n",
        "    X_train, y_train = df.loc[df['kfold'] != kfold, :'f4'].reset_index(drop=True), df.loc[df['kfold'] != kfold, 'target'].reset_index(drop=True)\n",
        "    X_test, y_test = df.loc[df['kfold'] == kfold, :'f4'].reset_index(drop=True), df.loc[df['kfold'] == kfold, 'target'].reset_index(drop=True)\n",
        "    y_train, y_test = y_train.values.reshape((1, X_train.shape[0])), y_test.values.reshape((1, X_test.shape[0]))\n",
        "\n",
        "    # Calling the driver function -\n",
        "    alpha, epochs, pred_thresh, hid_nodes = 1e-2, 750, 0.5, 8 # Defining the hyperparameters\n",
        "    tr_hist, best_params, te_hist = NN(X_train.values.T, y_train, X_test.values.T, y_test, alpha, epochs, hid_nodes)\n",
        "\n",
        "    # Printing some metrics -\n",
        "    train_probs = fwd_prop(X_train.values.T, best_params)['a2']\n",
        "    test_probs = fwd_prop(X_test.values.T, best_params)['a2']\n",
        "\n",
        "    train_preds = (train_probs >= pred_thresh).astype('int').flatten()\n",
        "    test_preds = (test_probs >= pred_thresh).astype('int').flatten()\n",
        "\n",
        "    y_train, y_test = y_train.flatten(), y_test.flatten()\n",
        "\n",
        "    train_acc = accuracy_score(y_train, train_preds)\n",
        "    test_acc = accuracy_score(y_test, test_preds)\n",
        "    train_accs.append(train_acc); test_accs.append(test_acc)\n",
        "\n",
        "    train_f1 = f1_score(y_train, train_preds) # f1 score for label 1\n",
        "    test_f1 = f1_score(y_test, test_preds) # f1 score for label 1\n",
        "    train_f1s.append(train_f1); test_f1s.append(test_f1)\n",
        "\n",
        "    print(f\"Train accuracy: {train_acc}, Test accuracy: {test_acc}\")\n",
        "    print(f\"Train f1: {train_f1}, Test f1: {test_f1}\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "# Printing the mean of the metrics across all folds -\n",
        "print(f\"Mean train accuracy: {np.mean(train_accs)}, Mean test accuracy: {np.mean(test_accs)}\")\n",
        "print(f\"Mean train f1: {np.mean(train_f1s)}, Mean test f1: {np.mean(test_f1s)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oN6oCRzwMXgF",
        "outputId": "758e3047-3aaf-4b3f-bc4c-7fa1c9028a4d"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Train loss: 0.17278013307108325, Test loss: 0.18643389574057168\n",
            "\n",
            "\n",
            "Epoch 507 -\n",
            "Updated K params after epoch 507: [-0.18789843 -0.95539854]\n",
            "Train loss: 0.17227499991121767, Test loss: 0.18594869884012416\n",
            "\n",
            "\n",
            "Epoch 508 -\n",
            "Updated K params after epoch 508: [-0.18788262 -0.95558278]\n",
            "Train loss: 0.17177311286146063, Test loss: 0.18546660859337236\n",
            "\n",
            "\n",
            "Epoch 509 -\n",
            "Updated K params after epoch 509: [-0.18786686 -0.95576658]\n",
            "Train loss: 0.17127444230044483, Test loss: 0.1849875961872163\n",
            "\n",
            "\n",
            "Epoch 510 -\n",
            "Updated K params after epoch 510: [-0.18785114 -0.95594993]\n",
            "Train loss: 0.17077895891158001, Test loss: 0.1845116331087768\n",
            "\n",
            "\n",
            "Epoch 511 -\n",
            "Updated K params after epoch 511: [-0.18783546 -0.95613285]\n",
            "Train loss: 0.17028663368038255, Test loss: 0.18403869114278298\n",
            "\n",
            "\n",
            "Epoch 512 -\n",
            "Updated K params after epoch 512: [-0.18781982 -0.95631533]\n",
            "Train loss: 0.16979743789179666, Test loss: 0.18356874236895007\n",
            "\n",
            "\n",
            "Epoch 513 -\n",
            "Updated K params after epoch 513: [-0.18780423 -0.95649737]\n",
            "Train loss: 0.16931134312750984, Test loss: 0.1831017591593488\n",
            "\n",
            "\n",
            "Epoch 514 -\n",
            "Updated K params after epoch 514: [-0.18778868 -0.95667898]\n",
            "Train loss: 0.16882832126326375, Test loss: 0.18263771417576855\n",
            "\n",
            "\n",
            "Epoch 515 -\n",
            "Updated K params after epoch 515: [-0.18777318 -0.95686015]\n",
            "Train loss: 0.16834834446616231, Test loss: 0.18217658036707568\n",
            "\n",
            "\n",
            "Epoch 516 -\n",
            "Updated K params after epoch 516: [-0.18775772 -0.95704089]\n",
            "Train loss: 0.1678713851919783, Test loss: 0.1817183309665684\n",
            "\n",
            "\n",
            "Epoch 517 -\n",
            "Updated K params after epoch 517: [-0.1877423 -0.9572212]\n",
            "Train loss: 0.16739741618246004, Test loss: 0.18126293948932984\n",
            "\n",
            "\n",
            "Epoch 518 -\n",
            "Updated K params after epoch 518: [-0.18772693 -0.95740108]\n",
            "Train loss: 0.16692641046263954, Test loss: 0.18081037972958086\n",
            "\n",
            "\n",
            "Epoch 519 -\n",
            "Updated K params after epoch 519: [-0.1877116  -0.95758053]\n",
            "Train loss: 0.16645834133814313, Test loss: 0.18036062575803333\n",
            "\n",
            "\n",
            "Epoch 520 -\n",
            "Updated K params after epoch 520: [-0.18769631 -0.95775955]\n",
            "Train loss: 0.16599318239250585, Test loss: 0.17991365191924588\n",
            "\n",
            "\n",
            "Epoch 521 -\n",
            "Updated K params after epoch 521: [-0.18768107 -0.95793815]\n",
            "Train loss: 0.16553090748449142, Test loss: 0.17946943282898276\n",
            "\n",
            "\n",
            "Epoch 522 -\n",
            "Updated K params after epoch 522: [-0.18766588 -0.95811632]\n",
            "Train loss: 0.16507149074541727, Test loss: 0.17902794337157724\n",
            "\n",
            "\n",
            "Epoch 523 -\n",
            "Updated K params after epoch 523: [-0.18765072 -0.95829406]\n",
            "Train loss: 0.16461490657648759, Test loss: 0.17858915869730038\n",
            "\n",
            "\n",
            "Epoch 524 -\n",
            "Updated K params after epoch 524: [-0.18763561 -0.95847139]\n",
            "Train loss: 0.16416112964613394, Test loss: 0.1781530542197365\n",
            "\n",
            "\n",
            "Epoch 525 -\n",
            "Updated K params after epoch 525: [-0.18762055 -0.95864829]\n",
            "Train loss: 0.16371013488736497, Test loss: 0.17771960561316605\n",
            "\n",
            "\n",
            "Epoch 526 -\n",
            "Updated K params after epoch 526: [-0.18760553 -0.95882477]\n",
            "Train loss: 0.1632618974951258, Test loss: 0.17728878880995677\n",
            "\n",
            "\n",
            "Epoch 527 -\n",
            "Updated K params after epoch 527: [-0.18759055 -0.95900084]\n",
            "Train loss: 0.16281639292366842, Test loss: 0.17686057999796437\n",
            "\n",
            "\n",
            "Epoch 528 -\n",
            "Updated K params after epoch 528: [-0.18757562 -0.95917648]\n",
            "Train loss: 0.16237359688393307, Test loss: 0.17643495561794312\n",
            "\n",
            "\n",
            "Epoch 529 -\n",
            "Updated K params after epoch 529: [-0.18756074 -0.95935171]\n",
            "Train loss: 0.1619334853409419, Test loss: 0.17601189236096731\n",
            "\n",
            "\n",
            "Epoch 530 -\n",
            "Updated K params after epoch 530: [-0.1875459  -0.95952653]\n",
            "Train loss: 0.16149603451120564, Test loss: 0.1755913671658644\n",
            "\n",
            "\n",
            "Epoch 531 -\n",
            "Updated K params after epoch 531: [-0.1875311  -0.95970094]\n",
            "Train loss: 0.1610612208601434, Test loss: 0.17517335721666036\n",
            "\n",
            "\n",
            "Epoch 532 -\n",
            "Updated K params after epoch 532: [-0.18751635 -0.95987493]\n",
            "Train loss: 0.16062902109951666, Test loss: 0.1747578399400378\n",
            "\n",
            "\n",
            "Epoch 533 -\n",
            "Updated K params after epoch 533: [-0.18750164 -0.96004851]\n",
            "Train loss: 0.16019941218487785, Test loss: 0.17434479300280792\n",
            "\n",
            "\n",
            "Epoch 534 -\n",
            "Updated K params after epoch 534: [-0.18748698 -0.96022169]\n",
            "Train loss: 0.15977237131303404, Test loss: 0.17393419430939647\n",
            "\n",
            "\n",
            "Epoch 535 -\n",
            "Updated K params after epoch 535: [-0.18747236 -0.96039445]\n",
            "Train loss: 0.15934787591952623, Test loss: 0.1735260219993441\n",
            "\n",
            "\n",
            "Epoch 536 -\n",
            "Updated K params after epoch 536: [-0.18745779 -0.96056681]\n",
            "Train loss: 0.15892590367612472, Test loss: 0.1731202544448221\n",
            "\n",
            "\n",
            "Epoch 537 -\n",
            "Updated K params after epoch 537: [-0.18744326 -0.96073877]\n",
            "Train loss: 0.15850643248834095, Test loss: 0.17271687024816354\n",
            "\n",
            "\n",
            "Epoch 538 -\n",
            "Updated K params after epoch 538: [-0.18742878 -0.96091032]\n",
            "Train loss: 0.15808944049295626, Test loss: 0.17231584823941098\n",
            "\n",
            "\n",
            "Epoch 539 -\n",
            "Updated K params after epoch 539: [-0.18741434 -0.96108147]\n",
            "Train loss: 0.1576749060555678, Test loss: 0.1719171674738798\n",
            "\n",
            "\n",
            "Epoch 540 -\n",
            "Updated K params after epoch 540: [-0.18739995 -0.96125223]\n",
            "Train loss: 0.15726280776815224, Test loss: 0.17152080722973892\n",
            "\n",
            "\n",
            "Epoch 541 -\n",
            "Updated K params after epoch 541: [-0.1873856  -0.96142258]\n",
            "Train loss: 0.15685312444664712, Test loss: 0.1711267470056087\n",
            "\n",
            "\n",
            "Epoch 542 -\n",
            "Updated K params after epoch 542: [-0.1873713  -0.96159253]\n",
            "Train loss: 0.1564458351285505, Test loss: 0.17073496651817613\n",
            "\n",
            "\n",
            "Epoch 543 -\n",
            "Updated K params after epoch 543: [-0.18735704 -0.96176209]\n",
            "Train loss: 0.1560409190705392, Test loss: 0.17034544569982787\n",
            "\n",
            "\n",
            "Epoch 544 -\n",
            "Updated K params after epoch 544: [-0.18734283 -0.96193126]\n",
            "Train loss: 0.15563835574610554, Test loss: 0.1699581646963016\n",
            "\n",
            "\n",
            "Epoch 545 -\n",
            "Updated K params after epoch 545: [-0.18732866 -0.96210003]\n",
            "Train loss: 0.1552381248432129, Test loss: 0.16957310386435528\n",
            "\n",
            "\n",
            "Epoch 546 -\n",
            "Updated K params after epoch 546: [-0.18731454 -0.96226841]\n",
            "Train loss: 0.15484020626197068, Test loss: 0.16919024376945557\n",
            "\n",
            "\n",
            "Epoch 547 -\n",
            "Updated K params after epoch 547: [-0.18730046 -0.9624364 ]\n",
            "Train loss: 0.1544445801123284, Test loss: 0.16880956518348447\n",
            "\n",
            "\n",
            "Epoch 548 -\n",
            "Updated K params after epoch 548: [-0.18728643 -0.962604  ]\n",
            "Train loss: 0.15405122671178906, Test loss: 0.16843104908246526\n",
            "\n",
            "\n",
            "Epoch 549 -\n",
            "Updated K params after epoch 549: [-0.18727244 -0.96277121]\n",
            "Train loss: 0.15366012658314251, Test loss: 0.1680546766443075\n",
            "\n",
            "\n",
            "Epoch 550 -\n",
            "Updated K params after epoch 550: [-0.1872585  -0.96293804]\n",
            "Train loss: 0.15327126045221803, Test loss: 0.16768042924657137\n",
            "\n",
            "\n",
            "Epoch 551 -\n",
            "Updated K params after epoch 551: [-0.18724461 -0.96310449]\n",
            "Train loss: 0.1528846092456571, Test loss: 0.1673082884642513\n",
            "\n",
            "\n",
            "Epoch 552 -\n",
            "Updated K params after epoch 552: [-0.18723075 -0.96327055]\n",
            "Train loss: 0.15250015408870574, Test loss: 0.16693823606757954\n",
            "\n",
            "\n",
            "Epoch 553 -\n",
            "Updated K params after epoch 553: [-0.18721695 -0.96343622]\n",
            "Train loss: 0.1521178763030273, Test loss: 0.16657025401984873\n",
            "\n",
            "\n",
            "Epoch 554 -\n",
            "Updated K params after epoch 554: [-0.18720319 -0.96360152]\n",
            "Train loss: 0.1517377574045347, Test loss: 0.16620432447525496\n",
            "\n",
            "\n",
            "Epoch 555 -\n",
            "Updated K params after epoch 555: [-0.18718947 -0.96376644]\n",
            "Train loss: 0.15135977910124326, Test loss: 0.16584042977676008\n",
            "\n",
            "\n",
            "Epoch 556 -\n",
            "Updated K params after epoch 556: [-0.1871758  -0.96393098]\n",
            "Train loss: 0.15098392329114355, Test loss: 0.1654785524539744\n",
            "\n",
            "\n",
            "Epoch 557 -\n",
            "Updated K params after epoch 557: [-0.18716218 -0.96409515]\n",
            "Train loss: 0.1506101720600938, Test loss: 0.16511867522105858\n",
            "\n",
            "\n",
            "Epoch 558 -\n",
            "Updated K params after epoch 558: [-0.1871486  -0.96425894]\n",
            "Train loss: 0.1502385076797338, Test loss: 0.1647607809746464\n",
            "\n",
            "\n",
            "Epoch 559 -\n",
            "Updated K params after epoch 559: [-0.18713506 -0.96442236]\n",
            "Train loss: 0.1498689126054173, Test loss: 0.16440485279178688\n",
            "\n",
            "\n",
            "Epoch 560 -\n",
            "Updated K params after epoch 560: [-0.18712157 -0.96458541]\n",
            "Train loss: 0.14950136947416603, Test loss: 0.16405087392790652\n",
            "\n",
            "\n",
            "Epoch 561 -\n",
            "Updated K params after epoch 561: [-0.18710813 -0.96474809]\n",
            "Train loss: 0.14913586110264293, Test loss: 0.16369882781479167\n",
            "\n",
            "\n",
            "Epoch 562 -\n",
            "Updated K params after epoch 562: [-0.18709473 -0.9649104 ]\n",
            "Train loss: 0.14877237048514577, Test loss: 0.16334869805859095\n",
            "\n",
            "\n",
            "Epoch 563 -\n",
            "Updated K params after epoch 563: [-0.18708137 -0.96507234]\n",
            "Train loss: 0.14841088079162107, Test loss: 0.16300046843783722\n",
            "\n",
            "\n",
            "Epoch 564 -\n",
            "Updated K params after epoch 564: [-0.18706806 -0.96523391]\n",
            "Train loss: 0.1480513753656977, Test loss: 0.16265412290149\n",
            "\n",
            "\n",
            "Epoch 565 -\n",
            "Updated K params after epoch 565: [-0.1870548  -0.96539512]\n",
            "Train loss: 0.14769383772274067, Test loss: 0.1623096455669976\n",
            "\n",
            "\n",
            "Epoch 566 -\n",
            "Updated K params after epoch 566: [-0.18704158 -0.96555597]\n",
            "Train loss: 0.1473382515479247, Test loss: 0.16196702071837887\n",
            "\n",
            "\n",
            "Epoch 567 -\n",
            "Updated K params after epoch 567: [-0.1870284  -0.96571646]\n",
            "Train loss: 0.14698460069432778, Test loss: 0.1616262328043252\n",
            "\n",
            "\n",
            "Epoch 568 -\n",
            "Updated K params after epoch 568: [-0.18701527 -0.96587658]\n",
            "Train loss: 0.14663286918104443, Test loss: 0.16128726643632202\n",
            "\n",
            "\n",
            "Epoch 569 -\n",
            "Updated K params after epoch 569: [-0.18700218 -0.96603635]\n",
            "Train loss: 0.14628304119131857, Test loss: 0.16095010638678972\n",
            "\n",
            "\n",
            "Epoch 570 -\n",
            "Updated K params after epoch 570: [-0.18698914 -0.96619576]\n",
            "Train loss: 0.1459351010706962, Test loss: 0.16061473758724476\n",
            "\n",
            "\n",
            "Epoch 571 -\n",
            "Updated K params after epoch 571: [-0.18697615 -0.96635482]\n",
            "Train loss: 0.14558903332519732, Test loss: 0.1602811451264799\n",
            "\n",
            "\n",
            "Epoch 572 -\n",
            "Updated K params after epoch 572: [-0.1869632  -0.96651352]\n",
            "Train loss: 0.14524482261950764, Test loss: 0.15994931424876374\n",
            "\n",
            "\n",
            "Epoch 573 -\n",
            "Updated K params after epoch 573: [-0.18695029 -0.96667186]\n",
            "Train loss: 0.14490245377518915, Test loss: 0.1596192303520601\n",
            "\n",
            "\n",
            "Epoch 574 -\n",
            "Updated K params after epoch 574: [-0.18693743 -0.96682986]\n",
            "Train loss: 0.14456191176891037, Test loss: 0.15929087898626612\n",
            "\n",
            "\n",
            "Epoch 575 -\n",
            "Updated K params after epoch 575: [-0.18692461 -0.9669875 ]\n",
            "Train loss: 0.14422318173069523, Test loss: 0.15896424585146982\n",
            "\n",
            "\n",
            "Epoch 576 -\n",
            "Updated K params after epoch 576: [-0.18691184 -0.96714479]\n",
            "Train loss: 0.1438862489421914, Test loss: 0.15863931679622656\n",
            "\n",
            "\n",
            "Epoch 577 -\n",
            "Updated K params after epoch 577: [-0.18689911 -0.96730174]\n",
            "Train loss: 0.14355109883495706, Test loss: 0.1583160778158545\n",
            "\n",
            "\n",
            "Epoch 578 -\n",
            "Updated K params after epoch 578: [-0.18688643 -0.96745834]\n",
            "Train loss: 0.14321771698876676, Test loss: 0.15799451505074874\n",
            "\n",
            "\n",
            "Epoch 579 -\n",
            "Updated K params after epoch 579: [-0.18687379 -0.9676146 ]\n",
            "Train loss: 0.14288608912993564, Test loss: 0.15767461478471434\n",
            "\n",
            "\n",
            "Epoch 580 -\n",
            "Updated K params after epoch 580: [-0.18686119 -0.96777051]\n",
            "Train loss: 0.14255620112966216, Test loss: 0.15735636344331766\n",
            "\n",
            "\n",
            "Epoch 581 -\n",
            "Updated K params after epoch 581: [-0.18684864 -0.96792608]\n",
            "Train loss: 0.1422280390023895, Test loss: 0.15703974759225633\n",
            "\n",
            "\n",
            "Epoch 582 -\n",
            "Updated K params after epoch 582: [-0.18683614 -0.96808131]\n",
            "Train loss: 0.14190158890418478, Test loss: 0.15672475393574745\n",
            "\n",
            "\n",
            "Epoch 583 -\n",
            "Updated K params after epoch 583: [-0.18682367 -0.96823621]\n",
            "Train loss: 0.14157683713113647, Test loss: 0.1564113693149338\n",
            "\n",
            "\n",
            "Epoch 584 -\n",
            "Updated K params after epoch 584: [-0.18681126 -0.96839076]\n",
            "Train loss: 0.14125377011776993, Test loss: 0.15609958070630867\n",
            "\n",
            "\n",
            "Epoch 585 -\n",
            "Updated K params after epoch 585: [-0.18679888 -0.96854497]\n",
            "Train loss: 0.1409323744354806, Test loss: 0.15578937522015765\n",
            "\n",
            "\n",
            "Epoch 586 -\n",
            "Updated K params after epoch 586: [-0.18678655 -0.96869886]\n",
            "Train loss: 0.1406126367909847, Test loss: 0.15548074009901905\n",
            "\n",
            "\n",
            "Epoch 587 -\n",
            "Updated K params after epoch 587: [-0.18677427 -0.9688524 ]\n",
            "Train loss: 0.14029454402478797, Test loss: 0.15517366271616156\n",
            "\n",
            "\n",
            "Epoch 588 -\n",
            "Updated K params after epoch 588: [-0.18676203 -0.96900562]\n",
            "Train loss: 0.13997808310967108, Test loss: 0.15486813057407917\n",
            "\n",
            "\n",
            "Epoch 589 -\n",
            "Updated K params after epoch 589: [-0.18674983 -0.9691585 ]\n",
            "Train loss: 0.13966324114919285, Test loss: 0.15456413130300395\n",
            "\n",
            "\n",
            "Epoch 590 -\n",
            "Updated K params after epoch 590: [-0.18673767 -0.96931105]\n",
            "Train loss: 0.13935000537621012, Test loss: 0.15426165265943553\n",
            "\n",
            "\n",
            "Epoch 591 -\n",
            "Updated K params after epoch 591: [-0.18672556 -0.96946328]\n",
            "Train loss: 0.1390383631514149, Test loss: 0.1539606825246879\n",
            "\n",
            "\n",
            "Epoch 592 -\n",
            "Updated K params after epoch 592: [-0.1867135  -0.96961518]\n",
            "Train loss: 0.13872830196188815, Test loss: 0.15366120890345297\n",
            "\n",
            "\n",
            "Epoch 593 -\n",
            "Updated K params after epoch 593: [-0.18670147 -0.96976675]\n",
            "Train loss: 0.13841980941966994, Test loss: 0.1533632199223808\n",
            "\n",
            "\n",
            "Epoch 594 -\n",
            "Updated K params after epoch 594: [-0.1866895  -0.96991799]\n",
            "Train loss: 0.13811287326034644, Test loss: 0.15306670382867668\n",
            "\n",
            "\n",
            "Epoch 595 -\n",
            "Updated K params after epoch 595: [-0.18667756 -0.97006892]\n",
            "Train loss: 0.13780748134165313, Test loss: 0.15277164898871426\n",
            "\n",
            "\n",
            "Epoch 596 -\n",
            "Updated K params after epoch 596: [-0.18666567 -0.97021952]\n",
            "Train loss: 0.13750362164209412, Test loss: 0.1524780438866653\n",
            "\n",
            "\n",
            "Epoch 597 -\n",
            "Updated K params after epoch 597: [-0.18665382 -0.9703698 ]\n",
            "Train loss: 0.13720128225957734, Test loss: 0.1521858771231452\n",
            "\n",
            "\n",
            "Epoch 598 -\n",
            "Updated K params after epoch 598: [-0.18664201 -0.97051976]\n",
            "Train loss: 0.13690045141006593, Test loss: 0.15189513741387498\n",
            "\n",
            "\n",
            "Epoch 599 -\n",
            "Updated K params after epoch 599: [-0.18663025 -0.97066941]\n",
            "Train loss: 0.13660111742624503, Test loss: 0.15160581358835837\n",
            "\n",
            "\n",
            "Epoch 600 -\n",
            "Updated K params after epoch 600: [-0.18661853 -0.97081874]\n",
            "Train loss: 0.1363032687562043, Test loss: 0.15131789458857534\n",
            "\n",
            "\n",
            "Epoch 601 -\n",
            "Updated K params after epoch 601: [-0.18660686 -0.97096775]\n",
            "Train loss: 0.13600689396213558, Test loss: 0.15103136946769052\n",
            "\n",
            "\n",
            "Epoch 602 -\n",
            "Updated K params after epoch 602: [-0.18659522 -0.97111645]\n",
            "Train loss: 0.13571198171904608, Test loss: 0.1507462273887773\n",
            "\n",
            "\n",
            "Epoch 603 -\n",
            "Updated K params after epoch 603: [-0.18658363 -0.97126483]\n",
            "Train loss: 0.1354185208134868, Test loss: 0.15046245762355684\n",
            "\n",
            "\n",
            "Epoch 604 -\n",
            "Updated K params after epoch 604: [-0.18657209 -0.9714129 ]\n",
            "Train loss: 0.13512650014229508, Test loss: 0.15018004955115255\n",
            "\n",
            "\n",
            "Epoch 605 -\n",
            "Updated K params after epoch 605: [-0.18656058 -0.97156067]\n",
            "Train loss: 0.1348359087113529, Test loss: 0.1498989926568589\n",
            "\n",
            "\n",
            "Epoch 606 -\n",
            "Updated K params after epoch 606: [-0.18654912 -0.97170812]\n",
            "Train loss: 0.13454673563435923, Test loss: 0.1496192765309254\n",
            "\n",
            "\n",
            "Epoch 607 -\n",
            "Updated K params after epoch 607: [-0.1865377  -0.97185527]\n",
            "Train loss: 0.13425897013161692, Test loss: 0.14934089086735503\n",
            "\n",
            "\n",
            "Epoch 608 -\n",
            "Updated K params after epoch 608: [-0.18652633 -0.97200211]\n",
            "Train loss: 0.13397260152883406, Test loss: 0.1490638254627168\n",
            "\n",
            "\n",
            "Epoch 609 -\n",
            "Updated K params after epoch 609: [-0.18651499 -0.97214864]\n",
            "Train loss: 0.13368761925593947, Test loss: 0.1487880702149731\n",
            "\n",
            "\n",
            "Epoch 610 -\n",
            "Updated K params after epoch 610: [-0.1865037  -0.97229487]\n",
            "Train loss: 0.13340401284591202, Test loss: 0.14851361512232056\n",
            "\n",
            "\n",
            "Epoch 611 -\n",
            "Updated K params after epoch 611: [-0.18649246 -0.9724408 ]\n",
            "Train loss: 0.13312177193362412, Test loss: 0.14824045028204544\n",
            "\n",
            "\n",
            "Epoch 612 -\n",
            "Updated K params after epoch 612: [-0.18648125 -0.97258642]\n",
            "Train loss: 0.1328408862546985, Test loss: 0.1479685658893924\n",
            "\n",
            "\n",
            "Epoch 613 -\n",
            "Updated K params after epoch 613: [-0.18647009 -0.97273175]\n",
            "Train loss: 0.13256134564437916, Test loss: 0.147697952236447\n",
            "\n",
            "\n",
            "Epoch 614 -\n",
            "Updated K params after epoch 614: [-0.18645896 -0.97287677]\n",
            "Train loss: 0.13228314003641511, Test loss: 0.14742859971103195\n",
            "\n",
            "\n",
            "Epoch 615 -\n",
            "Updated K params after epoch 615: [-0.18644788 -0.9730215 ]\n",
            "Train loss: 0.1320062594619578, Test loss: 0.14716049879561632\n",
            "\n",
            "\n",
            "Epoch 616 -\n",
            "Updated K params after epoch 616: [-0.18643685 -0.97316594]\n",
            "Train loss: 0.13173069404847135, Test loss: 0.14689364006623842\n",
            "\n",
            "\n",
            "Epoch 617 -\n",
            "Updated K params after epoch 617: [-0.18642585 -0.97331007]\n",
            "Train loss: 0.13145643401865617, Test loss: 0.1466280141914412\n",
            "\n",
            "\n",
            "Epoch 618 -\n",
            "Updated K params after epoch 618: [-0.1864149  -0.97345391]\n",
            "Train loss: 0.13118346968938513, Test loss: 0.1463636119312208\n",
            "\n",
            "\n",
            "Epoch 619 -\n",
            "Updated K params after epoch 619: [-0.18640398 -0.97359746]\n",
            "Train loss: 0.1309117914706521, Test loss: 0.1461004241359882\n",
            "\n",
            "\n",
            "Epoch 620 -\n",
            "Updated K params after epoch 620: [-0.18639311 -0.97374072]\n",
            "Train loss: 0.13064138986453402, Test loss: 0.1458384417455427\n",
            "\n",
            "\n",
            "Epoch 621 -\n",
            "Updated K params after epoch 621: [-0.18638229 -0.97388369]\n",
            "Train loss: 0.1303722554641641, Test loss: 0.14557765578805873\n",
            "\n",
            "\n",
            "Epoch 622 -\n",
            "Updated K params after epoch 622: [-0.1863715  -0.97402636]\n",
            "Train loss: 0.13010437895271834, Test loss: 0.14531805737908426\n",
            "\n",
            "\n",
            "Epoch 623 -\n",
            "Updated K params after epoch 623: [-0.18636075 -0.97416875]\n",
            "Train loss: 0.1298377511024136, Test loss: 0.1450596377205519\n",
            "\n",
            "\n",
            "Epoch 624 -\n",
            "Updated K params after epoch 624: [-0.18635005 -0.97431086]\n",
            "Train loss: 0.12957236277351777, Test loss: 0.1448023880998016\n",
            "\n",
            "\n",
            "Epoch 625 -\n",
            "Updated K params after epoch 625: [-0.18633938 -0.97445267]\n",
            "Train loss: 0.12930820491337186, Test loss: 0.14454629988861573\n",
            "\n",
            "\n",
            "Epoch 626 -\n",
            "Updated K params after epoch 626: [-0.18632876 -0.9745942 ]\n",
            "Train loss: 0.1290452685554237, Test loss: 0.14429136454226552\n",
            "\n",
            "\n",
            "Epoch 627 -\n",
            "Updated K params after epoch 627: [-0.18631818 -0.97473545]\n",
            "Train loss: 0.12878354481827328, Test loss: 0.14403757359856928\n",
            "\n",
            "\n",
            "Epoch 628 -\n",
            "Updated K params after epoch 628: [-0.18630764 -0.97487642]\n",
            "Train loss: 0.12852302490472944, Test loss: 0.143784918676962\n",
            "\n",
            "\n",
            "Epoch 629 -\n",
            "Updated K params after epoch 629: [-0.18629714 -0.9750171 ]\n",
            "Train loss: 0.1282637001008779, Test loss: 0.14353339147757646\n",
            "\n",
            "\n",
            "Epoch 630 -\n",
            "Updated K params after epoch 630: [-0.18628668 -0.9751575 ]\n",
            "Train loss: 0.12800556177516054, Test loss: 0.14328298378033555\n",
            "\n",
            "\n",
            "Epoch 631 -\n",
            "Updated K params after epoch 631: [-0.18627627 -0.97529763]\n",
            "Train loss: 0.12774860137746563, Test loss: 0.14303368744405529\n",
            "\n",
            "\n",
            "Epoch 632 -\n",
            "Updated K params after epoch 632: [-0.18626589 -0.97543748]\n",
            "Train loss: 0.12749281043822883, Test loss: 0.1427854944055592\n",
            "\n",
            "\n",
            "Epoch 633 -\n",
            "Updated K params after epoch 633: [-0.18625555 -0.97557705]\n",
            "Train loss: 0.12723818056754524, Test loss: 0.14253839667880358\n",
            "\n",
            "\n",
            "Epoch 634 -\n",
            "Updated K params after epoch 634: [-0.18624526 -0.97571634]\n",
            "Train loss: 0.12698470345429197, Test loss: 0.14229238635401278\n",
            "\n",
            "\n",
            "Epoch 635 -\n",
            "Updated K params after epoch 635: [-0.186235   -0.97585536]\n",
            "Train loss: 0.12673237086526115, Test loss: 0.14204745559682588\n",
            "\n",
            "\n",
            "Epoch 636 -\n",
            "Updated K params after epoch 636: [-0.18622479 -0.97599411]\n",
            "Train loss: 0.12648117464430345, Test loss: 0.14180359664745318\n",
            "\n",
            "\n",
            "Epoch 637 -\n",
            "Updated K params after epoch 637: [-0.18621461 -0.97613258]\n",
            "Train loss: 0.12623110671148177, Test loss: 0.14156080181984307\n",
            "\n",
            "\n",
            "Epoch 638 -\n",
            "Updated K params after epoch 638: [-0.18620448 -0.97627079]\n",
            "Train loss: 0.12598215906223525, Test loss: 0.14131906350085954\n",
            "\n",
            "\n",
            "Epoch 639 -\n",
            "Updated K params after epoch 639: [-0.18619438 -0.97640872]\n",
            "Train loss: 0.1257343237665531, Test loss: 0.14107837414946908\n",
            "\n",
            "\n",
            "Epoch 640 -\n",
            "Updated K params after epoch 640: [-0.18618433 -0.97654639]\n",
            "Train loss: 0.12548759296815842, Test loss: 0.14083872629593774\n",
            "\n",
            "\n",
            "Epoch 641 -\n",
            "Updated K params after epoch 641: [-0.18617431 -0.97668378]\n",
            "Train loss: 0.1252419588837018, Test loss: 0.14060011254103846\n",
            "\n",
            "\n",
            "Epoch 642 -\n",
            "Updated K params after epoch 642: [-0.18616434 -0.97682091]\n",
            "Train loss: 0.12499741380196466, Test loss: 0.1403625255552672\n",
            "\n",
            "\n",
            "Epoch 643 -\n",
            "Updated K params after epoch 643: [-0.1861544  -0.97695778]\n",
            "Train loss: 0.12475395008307205, Test loss: 0.1401259580780693\n",
            "\n",
            "\n",
            "Epoch 644 -\n",
            "Updated K params after epoch 644: [-0.18614451 -0.97709438]\n",
            "Train loss: 0.12451156015771486, Test loss: 0.13989040291707505\n",
            "\n",
            "\n",
            "Epoch 645 -\n",
            "Updated K params after epoch 645: [-0.18613465 -0.97723071]\n",
            "Train loss: 0.12427023652638139, Test loss: 0.13965585294734456\n",
            "\n",
            "\n",
            "Epoch 646 -\n",
            "Updated K params after epoch 646: [-0.18612484 -0.97736679]\n",
            "Train loss: 0.12402997175859819, Test loss: 0.13942230111062198\n",
            "\n",
            "\n",
            "Epoch 647 -\n",
            "Updated K params after epoch 647: [-0.18611506 -0.9775026 ]\n",
            "Train loss: 0.12379075849217991, Test loss: 0.1391897404145985\n",
            "\n",
            "\n",
            "Epoch 648 -\n",
            "Updated K params after epoch 648: [-0.18610532 -0.97763815]\n",
            "Train loss: 0.12355258943248801, Test loss: 0.13895816393218477\n",
            "\n",
            "\n",
            "Epoch 649 -\n",
            "Updated K params after epoch 649: [-0.18609562 -0.97777345]\n",
            "Train loss: 0.12331545735169877, Test loss: 0.13872756480079185\n",
            "\n",
            "\n",
            "Epoch 650 -\n",
            "Updated K params after epoch 650: [-0.18608596 -0.97790848]\n",
            "Train loss: 0.12307935508807942, Test loss: 0.1384979362216209\n",
            "\n",
            "\n",
            "Epoch 651 -\n",
            "Updated K params after epoch 651: [-0.18607634 -0.97804326]\n",
            "Train loss: 0.1228442755452735, Test loss: 0.1382692714589619\n",
            "\n",
            "\n",
            "Epoch 652 -\n",
            "Updated K params after epoch 652: [-0.18606676 -0.97817778]\n",
            "Train loss: 0.12261021169159451, Test loss: 0.13804156383950028\n",
            "\n",
            "\n",
            "Epoch 653 -\n",
            "Updated K params after epoch 653: [-0.18605721 -0.97831205]\n",
            "Train loss: 0.12237715655932803, Test loss: 0.13781480675163274\n",
            "\n",
            "\n",
            "Epoch 654 -\n",
            "Updated K params after epoch 654: [-0.18604771 -0.97844606]\n",
            "Train loss: 0.12214510324404201, Test loss: 0.13758899364479066\n",
            "\n",
            "\n",
            "Epoch 655 -\n",
            "Updated K params after epoch 655: [-0.18603824 -0.97857982]\n",
            "Train loss: 0.12191404490390571, Test loss: 0.1373641180287723\n",
            "\n",
            "\n",
            "Epoch 656 -\n",
            "Updated K params after epoch 656: [-0.18602882 -0.97871333]\n",
            "Train loss: 0.12168397475901645, Test loss: 0.1371401734730827\n",
            "\n",
            "\n",
            "Epoch 657 -\n",
            "Updated K params after epoch 657: [-0.18601943 -0.97884658]\n",
            "Train loss: 0.12145488609073428, Test loss: 0.1369171536062818\n",
            "\n",
            "\n",
            "Epoch 658 -\n",
            "Updated K params after epoch 658: [-0.18601008 -0.97897959]\n",
            "Train loss: 0.12122677224102509, Test loss: 0.13669505211534055\n",
            "\n",
            "\n",
            "Epoch 659 -\n",
            "Updated K params after epoch 659: [-0.18600076 -0.97911235]\n",
            "Train loss: 0.12099962661181098, Test loss: 0.13647386274500475\n",
            "\n",
            "\n",
            "Epoch 660 -\n",
            "Updated K params after epoch 660: [-0.18599149 -0.97924486]\n",
            "Train loss: 0.12077344266432868, Test loss: 0.13625357929716625\n",
            "\n",
            "\n",
            "Epoch 661 -\n",
            "Updated K params after epoch 661: [-0.18598225 -0.97937712]\n",
            "Train loss: 0.12054821391849567, Test loss: 0.13603419563024247\n",
            "\n",
            "\n",
            "Epoch 662 -\n",
            "Updated K params after epoch 662: [-0.18597305 -0.97950914]\n",
            "Train loss: 0.12032393395228358, Test loss: 0.13581570565856294\n",
            "\n",
            "\n",
            "Epoch 663 -\n",
            "Updated K params after epoch 663: [-0.18596389 -0.97964091]\n",
            "Train loss: 0.12010059640109917, Test loss: 0.13559810335176328\n",
            "\n",
            "\n",
            "Epoch 664 -\n",
            "Updated K params after epoch 664: [-0.18595477 -0.97977244]\n",
            "Train loss: 0.11987819495717297, Test loss: 0.1353813827341867\n",
            "\n",
            "\n",
            "Epoch 665 -\n",
            "Updated K params after epoch 665: [-0.18594569 -0.97990372]\n",
            "Train loss: 0.11965672336895444, Test loss: 0.13516553788429267\n",
            "\n",
            "\n",
            "Epoch 666 -\n",
            "Updated K params after epoch 666: [-0.18593664 -0.98003476]\n",
            "Train loss: 0.11943617544051519, Test loss: 0.13495056293407254\n",
            "\n",
            "\n",
            "Epoch 667 -\n",
            "Updated K params after epoch 667: [-0.18592763 -0.98016556]\n",
            "Train loss: 0.11921654503095869, Test loss: 0.13473645206847273\n",
            "\n",
            "\n",
            "Epoch 668 -\n",
            "Updated K params after epoch 668: [-0.18591865 -0.98029613]\n",
            "Train loss: 0.11899782605383707, Test loss: 0.13452319952482406\n",
            "\n",
            "\n",
            "Epoch 669 -\n",
            "Updated K params after epoch 669: [-0.18590972 -0.98042645]\n",
            "Train loss: 0.11878001247657509, Test loss: 0.13431079959227885\n",
            "\n",
            "\n",
            "Epoch 670 -\n",
            "Updated K params after epoch 670: [-0.18590082 -0.98055653]\n",
            "Train loss: 0.11856309831990068, Test loss: 0.13409924661125414\n",
            "\n",
            "\n",
            "Epoch 671 -\n",
            "Updated K params after epoch 671: [-0.18589196 -0.98068638]\n",
            "Train loss: 0.11834707765728228, Test loss: 0.13388853497288197\n",
            "\n",
            "\n",
            "Epoch 672 -\n",
            "Updated K params after epoch 672: [-0.18588314 -0.98081599]\n",
            "Train loss: 0.11813194461437287, Test loss: 0.13367865911846605\n",
            "\n",
            "\n",
            "Epoch 673 -\n",
            "Updated K params after epoch 673: [-0.18587435 -0.98094536]\n",
            "Train loss: 0.11791769336846056, Test loss: 0.1334696135389452\n",
            "\n",
            "\n",
            "Epoch 674 -\n",
            "Updated K params after epoch 674: [-0.1858656 -0.9810745]\n",
            "Train loss: 0.11770431814792573, Test loss: 0.13326139277436297\n",
            "\n",
            "\n",
            "Epoch 675 -\n",
            "Updated K params after epoch 675: [-0.18585688 -0.98120341]\n",
            "Train loss: 0.11749181323170427, Test loss: 0.13305399141334387\n",
            "\n",
            "\n",
            "Epoch 676 -\n",
            "Updated K params after epoch 676: [-0.18584821 -0.98133208]\n",
            "Train loss: 0.11728017294875775, Test loss: 0.13284740409257564\n",
            "\n",
            "\n",
            "Epoch 677 -\n",
            "Updated K params after epoch 677: [-0.18583957 -0.98146053]\n",
            "Train loss: 0.11706939167754922, Test loss: 0.13264162549629793\n",
            "\n",
            "\n",
            "Epoch 678 -\n",
            "Updated K params after epoch 678: [-0.18583096 -0.98158874]\n",
            "Train loss: 0.11685946384552542, Test loss: 0.13243665035579721\n",
            "\n",
            "\n",
            "Epoch 679 -\n",
            "Updated K params after epoch 679: [-0.18582239 -0.98171672]\n",
            "Train loss: 0.11665038392860538, Test loss: 0.1322324734489071\n",
            "\n",
            "\n",
            "Epoch 680 -\n",
            "Updated K params after epoch 680: [-0.18581386 -0.98184447]\n",
            "Train loss: 0.11644214645067441, Test loss: 0.13202908959951545\n",
            "\n",
            "\n",
            "Epoch 681 -\n",
            "Updated K params after epoch 681: [-0.18580537 -0.981972  ]\n",
            "Train loss: 0.1162347459830845, Test loss: 0.13182649367707694\n",
            "\n",
            "\n",
            "Epoch 682 -\n",
            "Updated K params after epoch 682: [-0.18579691 -0.9820993 ]\n",
            "Train loss: 0.11602817714416058, Test loss: 0.1316246805961313\n",
            "\n",
            "\n",
            "Epoch 683 -\n",
            "Updated K params after epoch 683: [-0.18578848 -0.98222637]\n",
            "Train loss: 0.11582243459871232, Test loss: 0.13142364531582787\n",
            "\n",
            "\n",
            "Epoch 684 -\n",
            "Updated K params after epoch 684: [-0.1857801  -0.98235322]\n",
            "Train loss: 0.11561751305755177, Test loss: 0.13122338283945492\n",
            "\n",
            "\n",
            "Epoch 685 -\n",
            "Updated K params after epoch 685: [-0.18577174 -0.98247984]\n",
            "Train loss: 0.1154134072770168, Test loss: 0.1310238882139757\n",
            "\n",
            "\n",
            "Epoch 686 -\n",
            "Updated K params after epoch 686: [-0.18576343 -0.98260624]\n",
            "Train loss: 0.11521011205849979, Test loss: 0.13082515652956897\n",
            "\n",
            "\n",
            "Epoch 687 -\n",
            "Updated K params after epoch 687: [-0.18575515 -0.98273241]\n",
            "Train loss: 0.11500762224798214, Test loss: 0.13062718291917558\n",
            "\n",
            "\n",
            "Epoch 688 -\n",
            "Updated K params after epoch 688: [-0.1857469  -0.98285836]\n",
            "Train loss: 0.11480593273557409, Test loss: 0.13042996255805023\n",
            "\n",
            "\n",
            "Epoch 689 -\n",
            "Updated K params after epoch 689: [-0.18573869 -0.9829841 ]\n",
            "Train loss: 0.11460503845505983, Test loss: 0.13023349066331866\n",
            "\n",
            "\n",
            "Epoch 690 -\n",
            "Updated K params after epoch 690: [-0.18573052 -0.98310961]\n",
            "Train loss: 0.11440493438344804, Test loss: 0.13003776249353996\n",
            "\n",
            "\n",
            "Epoch 691 -\n",
            "Updated K params after epoch 691: [-0.18572238 -0.9832349 ]\n",
            "Train loss: 0.11420561554052763, Test loss: 0.12984277334827418\n",
            "\n",
            "\n",
            "Epoch 692 -\n",
            "Updated K params after epoch 692: [-0.18571427 -0.98335998]\n",
            "Train loss: 0.11400707698842862, Test loss: 0.12964851856765489\n",
            "\n",
            "\n",
            "Epoch 693 -\n",
            "Updated K params after epoch 693: [-0.18570621 -0.98348484]\n",
            "Train loss: 0.11380931383118817, Test loss: 0.1294549935319669\n",
            "\n",
            "\n",
            "Epoch 694 -\n",
            "Updated K params after epoch 694: [-0.18569817 -0.98360948]\n",
            "Train loss: 0.11361232121432151, Test loss: 0.12926219366122907\n",
            "\n",
            "\n",
            "Epoch 695 -\n",
            "Updated K params after epoch 695: [-0.18569017 -0.9837339 ]\n",
            "Train loss: 0.11341609432439817, Test loss: 0.12907011441478175\n",
            "\n",
            "\n",
            "Epoch 696 -\n",
            "Updated K params after epoch 696: [-0.18568221 -0.98385811]\n",
            "Train loss: 0.11322062838862265, Test loss: 0.12887875129087945\n",
            "\n",
            "\n",
            "Epoch 697 -\n",
            "Updated K params after epoch 697: [-0.18567428 -0.98398211]\n",
            "Train loss: 0.11302591867442047, Test loss: 0.12868809982628776\n",
            "\n",
            "\n",
            "Epoch 698 -\n",
            "Updated K params after epoch 698: [-0.18566638 -0.98410589]\n",
            "Train loss: 0.11283196048902853, Test loss: 0.12849815559588576\n",
            "\n",
            "\n",
            "Epoch 699 -\n",
            "Updated K params after epoch 699: [-0.18565852 -0.98422946]\n",
            "Train loss: 0.11263874917909067, Test loss: 0.12830891421227242\n",
            "\n",
            "\n",
            "Epoch 700 -\n",
            "Updated K params after epoch 700: [-0.1856507  -0.98435282]\n",
            "Train loss: 0.11244628013025734, Test loss: 0.12812037132537796\n",
            "\n",
            "\n",
            "Epoch 701 -\n",
            "Updated K params after epoch 701: [-0.1856429  -0.98447597]\n",
            "Train loss: 0.1122545487667905, Test loss: 0.12793252262207955\n",
            "\n",
            "\n",
            "Epoch 702 -\n",
            "Updated K params after epoch 702: [-0.18563514 -0.9845989 ]\n",
            "Train loss: 0.11206355055117258, Test loss: 0.12774536382582197\n",
            "\n",
            "\n",
            "Epoch 703 -\n",
            "Updated K params after epoch 703: [-0.18562742 -0.98472163]\n",
            "Train loss: 0.11187328098372025, Test loss: 0.127558890696242\n",
            "\n",
            "\n",
            "Epoch 704 -\n",
            "Updated K params after epoch 704: [-0.18561973 -0.98484415]\n",
            "Train loss: 0.1116837356022025, Test loss: 0.12737309902879773\n",
            "\n",
            "\n",
            "Epoch 705 -\n",
            "Updated K params after epoch 705: [-0.18561207 -0.98496646]\n",
            "Train loss: 0.11149490998146308, Test loss: 0.12718798465440218\n",
            "\n",
            "\n",
            "Epoch 706 -\n",
            "Updated K params after epoch 706: [-0.18560445 -0.98508857]\n",
            "Train loss: 0.11130679973304741, Test loss: 0.12700354343906076\n",
            "\n",
            "\n",
            "Epoch 707 -\n",
            "Updated K params after epoch 707: [-0.18559686 -0.98521047]\n",
            "Train loss: 0.11111940050483363, Test loss: 0.12681977128351352\n",
            "\n",
            "\n",
            "Epoch 708 -\n",
            "Updated K params after epoch 708: [-0.18558931 -0.98533216]\n",
            "Train loss: 0.11093270798066786, Test loss: 0.1266366641228811\n",
            "\n",
            "\n",
            "Epoch 709 -\n",
            "Updated K params after epoch 709: [-0.18558179 -0.98545365]\n",
            "Train loss: 0.11074671788000386, Test loss: 0.126454217926315\n",
            "\n",
            "\n",
            "Epoch 710 -\n",
            "Updated K params after epoch 710: [-0.1855743  -0.98557494]\n",
            "Train loss: 0.11056142595754655, Test loss: 0.12627242869665192\n",
            "\n",
            "\n",
            "Epoch 711 -\n",
            "Updated K params after epoch 711: [-0.18556684 -0.98569602]\n",
            "Train loss: 0.11037682800289976, Test loss: 0.1260912924700721\n",
            "\n",
            "\n",
            "Epoch 712 -\n",
            "Updated K params after epoch 712: [-0.18555942 -0.9858169 ]\n",
            "Train loss: 0.11019291984021784, Test loss: 0.12591080531576151\n",
            "\n",
            "\n",
            "Epoch 713 -\n",
            "Updated K params after epoch 713: [-0.18555203 -0.98593758]\n",
            "Train loss: 0.11000969732786142, Test loss: 0.125730963335578\n",
            "\n",
            "\n",
            "Epoch 714 -\n",
            "Updated K params after epoch 714: [-0.18554468 -0.98605806]\n",
            "Train loss: 0.10982715635805705, Test loss: 0.12555176266372137\n",
            "\n",
            "\n",
            "Epoch 715 -\n",
            "Updated K params after epoch 715: [-0.18553736 -0.98617834]\n",
            "Train loss: 0.10964529285656056, Test loss: 0.12537319946640732\n",
            "\n",
            "\n",
            "Epoch 716 -\n",
            "Updated K params after epoch 716: [-0.18553007 -0.98629842]\n",
            "Train loss: 0.10946410278232448, Test loss: 0.12519526994154476\n",
            "\n",
            "\n",
            "Epoch 717 -\n",
            "Updated K params after epoch 717: [-0.18552281 -0.9864183 ]\n",
            "Train loss: 0.10928358212716889, Test loss: 0.12501797031841747\n",
            "\n",
            "\n",
            "Epoch 718 -\n",
            "Updated K params after epoch 718: [-0.18551559 -0.98653798]\n",
            "Train loss: 0.10910372691545651, Test loss: 0.12484129685736868\n",
            "\n",
            "\n",
            "Epoch 719 -\n",
            "Updated K params after epoch 719: [-0.1855084  -0.98665747]\n",
            "Train loss: 0.10892453320377096, Test loss: 0.12466524584949007\n",
            "\n",
            "\n",
            "Epoch 720 -\n",
            "Updated K params after epoch 720: [-0.18550124 -0.98677676]\n",
            "Train loss: 0.10874599708059891, Test loss: 0.12448981361631356\n",
            "\n",
            "\n",
            "Epoch 721 -\n",
            "Updated K params after epoch 721: [-0.18549411 -0.98689586]\n",
            "Train loss: 0.10856811466601585, Test loss: 0.12431499650950724\n",
            "\n",
            "\n",
            "Epoch 722 -\n",
            "Updated K params after epoch 722: [-0.18548702 -0.98701476]\n",
            "Train loss: 0.1083908821113753, Test loss: 0.12414079091057445\n",
            "\n",
            "\n",
            "Epoch 723 -\n",
            "Updated K params after epoch 723: [-0.18547996 -0.98713347]\n",
            "Train loss: 0.10821429559900152, Test loss: 0.1239671932305564\n",
            "\n",
            "\n",
            "Epoch 724 -\n",
            "Updated K params after epoch 724: [-0.18547293 -0.98725198]\n",
            "Train loss: 0.10803835134188576, Test loss: 0.12379419990973821\n",
            "\n",
            "\n",
            "Epoch 725 -\n",
            "Updated K params after epoch 725: [-0.18546593 -0.9873703 ]\n",
            "Train loss: 0.1078630455833859, Test loss: 0.1236218074173582\n",
            "\n",
            "\n",
            "Epoch 726 -\n",
            "Updated K params after epoch 726: [-0.18545896 -0.98748843]\n",
            "Train loss: 0.10768837459692944, Test loss: 0.12345001225132073\n",
            "\n",
            "\n",
            "Epoch 727 -\n",
            "Updated K params after epoch 727: [-0.18545203 -0.98760637]\n",
            "Train loss: 0.1075143346857198, Test loss: 0.12327881093791185\n",
            "\n",
            "\n",
            "Epoch 728 -\n",
            "Updated K params after epoch 728: [-0.18544513 -0.98772412]\n",
            "Train loss: 0.10734092218244597, Test loss: 0.12310820003151877\n",
            "\n",
            "\n",
            "Epoch 729 -\n",
            "Updated K params after epoch 729: [-0.18543826 -0.98784168]\n",
            "Train loss: 0.10716813344899535, Test loss: 0.1229381761143521\n",
            "\n",
            "\n",
            "Epoch 730 -\n",
            "Updated K params after epoch 730: [-0.18543142 -0.98795905]\n",
            "Train loss: 0.1069959648761699, Test loss: 0.12276873579617126\n",
            "\n",
            "\n",
            "Epoch 731 -\n",
            "Updated K params after epoch 731: [-0.18542461 -0.98807624]\n",
            "Train loss: 0.10682441288340534, Test loss: 0.12259987571401332\n",
            "\n",
            "\n",
            "Epoch 732 -\n",
            "Updated K params after epoch 732: [-0.18541783 -0.98819323]\n",
            "Train loss: 0.10665347391849349, Test loss: 0.12243159253192444\n",
            "\n",
            "\n",
            "Epoch 733 -\n",
            "Updated K params after epoch 733: [-0.18541109 -0.98831004]\n",
            "Train loss: 0.10648314445730792, Test loss: 0.12226388294069479\n",
            "\n",
            "\n",
            "Epoch 734 -\n",
            "Updated K params after epoch 734: [-0.18540438 -0.98842666]\n",
            "Train loss: 0.10631342100353224, Test loss: 0.12209674365759614\n",
            "\n",
            "\n",
            "Epoch 735 -\n",
            "Updated K params after epoch 735: [-0.18539769 -0.9885431 ]\n",
            "Train loss: 0.10614430008839183, Test loss: 0.12193017142612256\n",
            "\n",
            "\n",
            "Epoch 736 -\n",
            "Updated K params after epoch 736: [-0.18539104 -0.98865935]\n",
            "Train loss: 0.10597577827038836, Test loss: 0.12176416301573402\n",
            "\n",
            "\n",
            "Epoch 737 -\n",
            "Updated K params after epoch 737: [-0.18538442 -0.98877542]\n",
            "Train loss: 0.10580785213503711, Test loss: 0.12159871522160287\n",
            "\n",
            "\n",
            "Epoch 738 -\n",
            "Updated K params after epoch 738: [-0.18537783 -0.98889131]\n",
            "Train loss: 0.10564051829460762, Test loss: 0.12143382486436309\n",
            "\n",
            "\n",
            "Epoch 739 -\n",
            "Updated K params after epoch 739: [-0.18537128 -0.98900701]\n",
            "Train loss: 0.1054737733878666, Test loss: 0.12126948878986256\n",
            "\n",
            "\n",
            "Epoch 740 -\n",
            "Updated K params after epoch 740: [-0.18536475 -0.98912253]\n",
            "Train loss: 0.10530761407982438, Test loss: 0.12110570386891772\n",
            "\n",
            "\n",
            "Epoch 741 -\n",
            "Updated K params after epoch 741: [-0.18535825 -0.98923787]\n",
            "Train loss: 0.10514203706148349, Test loss: 0.12094246699707151\n",
            "\n",
            "\n",
            "Epoch 742 -\n",
            "Updated K params after epoch 742: [-0.18535179 -0.98935303]\n",
            "Train loss: 0.10497703904959056, Test loss: 0.12077977509435356\n",
            "\n",
            "\n",
            "Epoch 743 -\n",
            "Updated K params after epoch 743: [-0.18534535 -0.98946801]\n",
            "Train loss: 0.10481261678639056, Test loss: 0.12061762510504315\n",
            "\n",
            "\n",
            "Epoch 744 -\n",
            "Updated K params after epoch 744: [-0.18533894 -0.98958281]\n",
            "Train loss: 0.10464876703938401, Test loss: 0.1204560139974351\n",
            "\n",
            "\n",
            "Epoch 745 -\n",
            "Updated K params after epoch 745: [-0.18533257 -0.98969743]\n",
            "Train loss: 0.1044854866010866, Test loss: 0.12029493876360779\n",
            "\n",
            "\n",
            "Epoch 746 -\n",
            "Updated K params after epoch 746: [-0.18532622 -0.98981187]\n",
            "Train loss: 0.10432277228879176, Test loss: 0.12013439641919425\n",
            "\n",
            "\n",
            "Epoch 747 -\n",
            "Updated K params after epoch 747: [-0.18531991 -0.98992614]\n",
            "Train loss: 0.10416062094433552, Test loss: 0.11997438400315542\n",
            "\n",
            "\n",
            "Epoch 748 -\n",
            "Updated K params after epoch 748: [-0.18531362 -0.99004023]\n",
            "Train loss: 0.10399902943386417, Test loss: 0.11981489857755614\n",
            "\n",
            "\n",
            "Epoch 749 -\n",
            "Updated K params after epoch 749: [-0.18530737 -0.99015414]\n",
            "Train loss: 0.10383799464760433, Test loss: 0.11965593722734347\n",
            "\n",
            "\n",
            "Epoch 750 -\n",
            "Updated K params after epoch 750: [-0.18530114 -0.99026788]\n",
            "Train loss: 0.10367751349963565, Test loss: 0.11949749706012755\n",
            "\n",
            "\n",
            "Train accuracy: 0.9744990892531876, Test accuracy: 0.9525547445255474\n",
            "Train f1: 0.9708939708939709, Test f1: 0.9451476793248946\n",
            "\n",
            "\n",
            "Fold 5 -\n",
            "Initial params: {'W1': array([[ 0.01624345, -0.00611756, -0.00528172, -0.01072969],\n",
            "       [ 0.00865408, -0.02301539,  0.01744812, -0.00761207],\n",
            "       [ 0.00319039, -0.0024937 ,  0.01462108, -0.02060141],\n",
            "       [-0.00322417, -0.00384054,  0.01133769, -0.01099891],\n",
            "       [-0.00172428, -0.00877858,  0.00042214,  0.00582815],\n",
            "       [-0.01100619,  0.01144724,  0.00901591,  0.00502494],\n",
            "       [ 0.00900856, -0.00683728, -0.0012289 , -0.00935769],\n",
            "       [-0.00267888,  0.00530355, -0.00691661, -0.00396754]]), 'b1': array([[0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.]]), 'W2': array([[-0.00687173, -0.00845206, -0.00671246, -0.00012665, -0.0111731 ,\n",
            "         0.00234416,  0.01659802,  0.00742044]]), 'b2': array([[0.]]), 'K': array([-0.19183555, -0.88762896])}\n",
            "Epoch 1 -\n",
            "Updated K params after epoch 1: [-0.19183507 -0.88762949]\n",
            "Train loss: 0.6928467459691038, Test loss: 0.6928604212390697\n",
            "\n",
            "\n",
            "Epoch 2 -\n",
            "Updated K params after epoch 2: [-0.19183462 -0.88763005]\n",
            "Train loss: 0.6927869086771565, Test loss: 0.6928042768598082\n",
            "\n",
            "\n",
            "Epoch 3 -\n",
            "Updated K params after epoch 3: [-0.1918342  -0.88763064]\n",
            "Train loss: 0.6927269826201079, Test loss: 0.6927480607095835\n",
            "\n",
            "\n",
            "Epoch 4 -\n",
            "Updated K params after epoch 4: [-0.19183383 -0.88763127]\n",
            "Train loss: 0.6926669366403014, Test loss: 0.6926917448649678\n",
            "\n",
            "\n",
            "Epoch 5 -\n",
            "Updated K params after epoch 5: [-0.19183349 -0.88763193]\n",
            "Train loss: 0.692606739386448, Test loss: 0.6926353012119674\n",
            "\n",
            "\n",
            "Epoch 6 -\n",
            "Updated K params after epoch 6: [-0.19183319 -0.88763262]\n",
            "Train loss: 0.6925463592831042, Test loss: 0.6925787014189174\n",
            "\n",
            "\n",
            "Epoch 7 -\n",
            "Updated K params after epoch 7: [-0.19183292 -0.88763335]\n",
            "Train loss: 0.6924857645001042, Test loss: 0.6925219169093121\n",
            "\n",
            "\n",
            "Epoch 8 -\n",
            "Updated K params after epoch 8: [-0.19183269 -0.88763412]\n",
            "Train loss: 0.6924249229219125, Test loss: 0.6924649188345475\n",
            "\n",
            "\n",
            "Epoch 9 -\n",
            "Updated K params after epoch 9: [-0.19183249 -0.88763492]\n",
            "Train loss: 0.6923638021168731, Test loss: 0.6924076780465521\n",
            "\n",
            "\n",
            "Epoch 10 -\n",
            "Updated K params after epoch 10: [-0.19183232 -0.88763576]\n",
            "Train loss: 0.692302369306328, Test loss: 0.692350165070282\n",
            "\n",
            "\n",
            "Epoch 11 -\n",
            "Updated K params after epoch 11: [-0.19183219 -0.88763664]\n",
            "Train loss: 0.6922405913335786, Test loss: 0.6922923500760595\n",
            "\n",
            "\n",
            "Epoch 12 -\n",
            "Updated K params after epoch 12: [-0.1918321  -0.88763755]\n",
            "Train loss: 0.6921784346326618, Test loss: 0.6922342028517285\n",
            "\n",
            "\n",
            "Epoch 13 -\n",
            "Updated K params after epoch 13: [-0.19183203 -0.88763851]\n",
            "Train loss: 0.6921158651969203, Test loss: 0.6921756927746093\n",
            "\n",
            "\n",
            "Epoch 14 -\n",
            "Updated K params after epoch 14: [-0.191832   -0.88763951]\n",
            "Train loss: 0.6920528485473355, Test loss: 0.6921167887832278\n",
            "\n",
            "\n",
            "Epoch 15 -\n",
            "Updated K params after epoch 15: [-0.191832   -0.88764054]\n",
            "Train loss: 0.6919893497006049, Test loss: 0.6920574593487971\n",
            "\n",
            "\n",
            "Epoch 16 -\n",
            "Updated K params after epoch 16: [-0.19183203 -0.88764163]\n",
            "Train loss: 0.6919253331369343, Test loss: 0.6919976724464344\n",
            "\n",
            "\n",
            "Epoch 17 -\n",
            "Updated K params after epoch 17: [-0.19183209 -0.88764275]\n",
            "Train loss: 0.6918607627675234, Test loss: 0.6919373955260867\n",
            "\n",
            "\n",
            "Epoch 18 -\n",
            "Updated K params after epoch 18: [-0.19183218 -0.88764392]\n",
            "Train loss: 0.6917956019017224, Test loss: 0.6918765954831492\n",
            "\n",
            "\n",
            "Epoch 19 -\n",
            "Updated K params after epoch 19: [-0.1918323  -0.88764514]\n",
            "Train loss: 0.6917298132138326, Test loss: 0.6918152386287557\n",
            "\n",
            "\n",
            "Epoch 20 -\n",
            "Updated K params after epoch 20: [-0.19183246 -0.8876464 ]\n",
            "Train loss: 0.691663358709533, Test loss: 0.6917532906597188\n",
            "\n",
            "\n",
            "Epoch 21 -\n",
            "Updated K params after epoch 21: [-0.19183264 -0.88764771]\n",
            "Train loss: 0.691596199691907, Test loss: 0.6916907166281032\n",
            "\n",
            "\n",
            "Epoch 22 -\n",
            "Updated K params after epoch 22: [-0.19183285 -0.88764908]\n",
            "Train loss: 0.6915282967270499, Test loss: 0.6916274809104137\n",
            "\n",
            "\n",
            "Epoch 23 -\n",
            "Updated K params after epoch 23: [-0.19183309 -0.88765049]\n",
            "Train loss: 0.6914596096092333, Test loss: 0.6915635471763755\n",
            "\n",
            "\n",
            "Epoch 24 -\n",
            "Updated K params after epoch 24: [-0.19183337 -0.88765196]\n",
            "Train loss: 0.6913900973256087, Test loss: 0.6914988783572947\n",
            "\n",
            "\n",
            "Epoch 25 -\n",
            "Updated K params after epoch 25: [-0.19183366 -0.88765348]\n",
            "Train loss: 0.6913197180204278, Test loss: 0.6914334366139767\n",
            "\n",
            "\n",
            "Epoch 26 -\n",
            "Updated K params after epoch 26: [-0.19183399 -0.88765505]\n",
            "Train loss: 0.6912484289587586, Test loss: 0.6913671833041866\n",
            "\n",
            "\n",
            "Epoch 27 -\n",
            "Updated K params after epoch 27: [-0.19183435 -0.88765669]\n",
            "Train loss: 0.6911761864896826, Test loss: 0.691300078949638\n",
            "\n",
            "\n",
            "Epoch 28 -\n",
            "Updated K params after epoch 28: [-0.19183473 -0.88765838]\n",
            "Train loss: 0.6911029460089492, Test loss: 0.6912320832024905\n",
            "\n",
            "\n",
            "Epoch 29 -\n",
            "Updated K params after epoch 29: [-0.19183514 -0.88766013]\n",
            "Train loss: 0.691028661921073, Test loss: 0.6911631548113435\n",
            "\n",
            "\n",
            "Epoch 30 -\n",
            "Updated K params after epoch 30: [-0.19183558 -0.88766195]\n",
            "Train loss: 0.6909532876008542, Test loss: 0.6910932515867095\n",
            "\n",
            "\n",
            "Epoch 31 -\n",
            "Updated K params after epoch 31: [-0.19183605 -0.88766383]\n",
            "Train loss: 0.6908767753543084, Test loss: 0.6910223303659546\n",
            "\n",
            "\n",
            "Epoch 32 -\n",
            "Updated K params after epoch 32: [-0.19183654 -0.88766578]\n",
            "Train loss: 0.6907990763789865, Test loss: 0.6909503469776935\n",
            "\n",
            "\n",
            "Epoch 33 -\n",
            "Updated K params after epoch 33: [-0.19183706 -0.88766779]\n",
            "Train loss: 0.6907201407236736, Test loss: 0.6908772562056236\n",
            "\n",
            "\n",
            "Epoch 34 -\n",
            "Updated K params after epoch 34: [-0.19183761 -0.88766987]\n",
            "Train loss: 0.6906399172474484, Test loss: 0.6908030117517914\n",
            "\n",
            "\n",
            "Epoch 35 -\n",
            "Updated K params after epoch 35: [-0.19183818 -0.88767203]\n",
            "Train loss: 0.6905583535780957, Test loss: 0.6907275661992764\n",
            "\n",
            "\n",
            "Epoch 36 -\n",
            "Updated K params after epoch 36: [-0.19183878 -0.88767426]\n",
            "Train loss: 0.690475396069853, Test loss: 0.6906508709742848\n",
            "\n",
            "\n",
            "Epoch 37 -\n",
            "Updated K params after epoch 37: [-0.1918394  -0.88767657]\n",
            "Train loss: 0.6903909897604875, Test loss: 0.6905728763076451\n",
            "\n",
            "\n",
            "Epoch 38 -\n",
            "Updated K params after epoch 38: [-0.19184005 -0.88767896]\n",
            "Train loss: 0.6903050783276886, Test loss: 0.6904935311956971\n",
            "\n",
            "\n",
            "Epoch 39 -\n",
            "Updated K params after epoch 39: [-0.19184073 -0.88768143]\n",
            "Train loss: 0.6902176040447691, Test loss: 0.6904127833605684\n",
            "\n",
            "\n",
            "Epoch 40 -\n",
            "Updated K params after epoch 40: [-0.19184143 -0.88768398]\n",
            "Train loss: 0.6901285077356677, Test loss: 0.6903305792098337\n",
            "\n",
            "\n",
            "Epoch 41 -\n",
            "Updated K params after epoch 41: [-0.19184215 -0.88768662]\n",
            "Train loss: 0.69003772872925, Test loss: 0.6902468637955536\n",
            "\n",
            "\n",
            "Epoch 42 -\n",
            "Updated K params after epoch 42: [-0.19184291 -0.88768934]\n",
            "Train loss: 0.689945204812898, Test loss: 0.6901615807726915\n",
            "\n",
            "\n",
            "Epoch 43 -\n",
            "Updated K params after epoch 43: [-0.19184368 -0.88769216]\n",
            "Train loss: 0.6898508721853925, Test loss: 0.6900746723569069\n",
            "\n",
            "\n",
            "Epoch 44 -\n",
            "Updated K params after epoch 44: [-0.19184448 -0.88769508]\n",
            "Train loss: 0.6897546654090826, Test loss: 0.6899860792817271\n",
            "\n",
            "\n",
            "Epoch 45 -\n",
            "Updated K params after epoch 45: [-0.19184531 -0.88769809]\n",
            "Train loss: 0.6896565173613468, Test loss: 0.6898957407551021\n",
            "\n",
            "\n",
            "Epoch 46 -\n",
            "Updated K params after epoch 46: [-0.19184616 -0.8877012 ]\n",
            "Train loss: 0.6895563591853493, Test loss: 0.6898035944153453\n",
            "\n",
            "\n",
            "Epoch 47 -\n",
            "Updated K params after epoch 47: [-0.19184704 -0.88770442]\n",
            "Train loss: 0.6894541202400933, Test loss: 0.6897095762864706\n",
            "\n",
            "\n",
            "Epoch 48 -\n",
            "Updated K params after epoch 48: [-0.19184794 -0.88770774]\n",
            "Train loss: 0.6893497280497869, Test loss: 0.6896136207329309\n",
            "\n",
            "\n",
            "Epoch 49 -\n",
            "Updated K params after epoch 49: [-0.19184886 -0.88771117]\n",
            "Train loss: 0.6892431082525234, Test loss: 0.6895156604137768\n",
            "\n",
            "\n",
            "Epoch 50 -\n",
            "Updated K params after epoch 50: [-0.19184981 -0.88771472]\n",
            "Train loss: 0.6891341845482978, Test loss: 0.689415626236242\n",
            "\n",
            "\n",
            "Epoch 51 -\n",
            "Updated K params after epoch 51: [-0.19185078 -0.88771838]\n",
            "Train loss: 0.6890228786463686, Test loss: 0.6893134473087811\n",
            "\n",
            "\n",
            "Epoch 52 -\n",
            "Updated K params after epoch 52: [-0.19185177 -0.88772217]\n",
            "Train loss: 0.6889091102119906, Test loss: 0.6892090508935735\n",
            "\n",
            "\n",
            "Epoch 53 -\n",
            "Updated K params after epoch 53: [-0.19185279 -0.88772607]\n",
            "Train loss: 0.6887927968125375, Test loss: 0.6891023623585203\n",
            "\n",
            "\n",
            "Epoch 54 -\n",
            "Updated K params after epoch 54: [-0.19185384 -0.88773011]\n",
            "Train loss: 0.6886738538630398, Test loss: 0.6889933051287601\n",
            "\n",
            "\n",
            "Epoch 55 -\n",
            "Updated K params after epoch 55: [-0.19185491 -0.88773428]\n",
            "Train loss: 0.6885521945711736, Test loss: 0.6888818006377334\n",
            "\n",
            "\n",
            "Epoch 56 -\n",
            "Updated K params after epoch 56: [-0.191856   -0.88773858]\n",
            "Train loss: 0.6884277298817263, Test loss: 0.6887677682778315\n",
            "\n",
            "\n",
            "Epoch 57 -\n",
            "Updated K params after epoch 57: [-0.19185711 -0.88774302]\n",
            "Train loss: 0.6883003684205855, Test loss: 0.6886511253506649\n",
            "\n",
            "\n",
            "Epoch 58 -\n",
            "Updated K params after epoch 58: [-0.19185825 -0.88774761]\n",
            "Train loss: 0.6881700164382862, Test loss: 0.6885317870169948\n",
            "\n",
            "\n",
            "Epoch 59 -\n",
            "Updated K params after epoch 59: [-0.19185941 -0.88775235]\n",
            "Train loss: 0.6880365777531691, Test loss: 0.6884096662463769\n",
            "\n",
            "\n",
            "Epoch 60 -\n",
            "Updated K params after epoch 60: [-0.1918606  -0.88775724]\n",
            "Train loss: 0.6878999536942009, Test loss: 0.6882846737665623\n",
            "\n",
            "\n",
            "Epoch 61 -\n",
            "Updated K params after epoch 61: [-0.1918618  -0.88776229]\n",
            "Train loss: 0.6877600430435122, Test loss: 0.6881567180127189\n",
            "\n",
            "\n",
            "Epoch 62 -\n",
            "Updated K params after epoch 62: [-0.19186303 -0.8877675 ]\n",
            "Train loss: 0.687616741978719, Test loss: 0.68802570507653\n",
            "\n",
            "\n",
            "Epoch 63 -\n",
            "Updated K params after epoch 63: [-0.19186429 -0.88777288]\n",
            "Train loss: 0.6874699440150965, Test loss: 0.6878915386552386\n",
            "\n",
            "\n",
            "Epoch 64 -\n",
            "Updated K params after epoch 64: [-0.19186557 -0.88777843]\n",
            "Train loss: 0.6873195399476787, Test loss: 0.6877541200007102\n",
            "\n",
            "\n",
            "Epoch 65 -\n",
            "Updated K params after epoch 65: [-0.19186687 -0.88778417]\n",
            "Train loss: 0.6871654177933679, Test loss: 0.687613347868594\n",
            "\n",
            "\n",
            "Epoch 66 -\n",
            "Updated K params after epoch 66: [-0.19186819 -0.88779008]\n",
            "Train loss: 0.6870074627331434, Test loss: 0.6874691184676649\n",
            "\n",
            "\n",
            "Epoch 67 -\n",
            "Updated K params after epoch 67: [-0.19186954 -0.88779618]\n",
            "Train loss: 0.6868455570544625, Test loss: 0.687321325409442\n",
            "\n",
            "\n",
            "Epoch 68 -\n",
            "Updated K params after epoch 68: [-0.19187091 -0.88780248]\n",
            "Train loss: 0.6866795800939625, Test loss: 0.687169859658181\n",
            "\n",
            "\n",
            "Epoch 69 -\n",
            "Updated K params after epoch 69: [-0.1918723  -0.88780898]\n",
            "Train loss: 0.6865094081805705, Test loss: 0.6870146094813464\n",
            "\n",
            "\n",
            "Epoch 70 -\n",
            "Updated K params after epoch 70: [-0.19187372 -0.88781569]\n",
            "Train loss: 0.6863349145791465, Test loss: 0.6868554604006807\n",
            "\n",
            "\n",
            "Epoch 71 -\n",
            "Updated K params after epoch 71: [-0.19187515 -0.88782261]\n",
            "Train loss: 0.6861559694347845, Test loss: 0.6866922951439894\n",
            "\n",
            "\n",
            "Epoch 72 -\n",
            "Updated K params after epoch 72: [-0.19187662 -0.88782975]\n",
            "Train loss: 0.685972439717913, Test loss: 0.6865249935977774\n",
            "\n",
            "\n",
            "Epoch 73 -\n",
            "Updated K params after epoch 73: [-0.1918781  -0.88783711]\n",
            "Train loss: 0.6857841891703415, Test loss: 0.6863534327608718\n",
            "\n",
            "\n",
            "Epoch 74 -\n",
            "Updated K params after epoch 74: [-0.19187961 -0.88784471]\n",
            "Train loss: 0.6855910782524134, Test loss: 0.6861774866991855\n",
            "\n",
            "\n",
            "Epoch 75 -\n",
            "Updated K params after epoch 75: [-0.19188114 -0.88785255]\n",
            "Train loss: 0.6853929640914297, Test loss: 0.685997026501777\n",
            "\n",
            "\n",
            "Epoch 76 -\n",
            "Updated K params after epoch 76: [-0.19188269 -0.88786063]\n",
            "Train loss: 0.6851897004315284, Test loss: 0.685811920238375\n",
            "\n",
            "\n",
            "Epoch 77 -\n",
            "Updated K params after epoch 77: [-0.19188426 -0.88786897]\n",
            "Train loss: 0.6849811375852057, Test loss: 0.6856220329185506\n",
            "\n",
            "\n",
            "Epoch 78 -\n",
            "Updated K params after epoch 78: [-0.19188586 -0.88787757]\n",
            "Train loss: 0.6847671223866859, Test loss: 0.6854272264527234\n",
            "\n",
            "\n",
            "Epoch 79 -\n",
            "Updated K params after epoch 79: [-0.19188748 -0.88788644]\n",
            "Train loss: 0.6845474981473508, Test loss: 0.685227359615203\n",
            "\n",
            "\n",
            "Epoch 80 -\n",
            "Updated K params after epoch 80: [-0.19188912 -0.88789559]\n",
            "Train loss: 0.6843221046134599, Test loss: 0.6850222880094818\n",
            "\n",
            "\n",
            "Epoch 81 -\n",
            "Updated K params after epoch 81: [-0.19189079 -0.88790502]\n",
            "Train loss: 0.6840907779263995, Test loss: 0.6848118640359977\n",
            "\n",
            "\n",
            "Epoch 82 -\n",
            "Updated K params after epoch 82: [-0.19189248 -0.88791474]\n",
            "Train loss: 0.6838533505857146, Test loss: 0.6845959368626078\n",
            "\n",
            "\n",
            "Epoch 83 -\n",
            "Updated K params after epoch 83: [-0.19189419 -0.88792477]\n",
            "Train loss: 0.6836096514151936, Test loss: 0.6843743523980205\n",
            "\n",
            "\n",
            "Epoch 84 -\n",
            "Updated K params after epoch 84: [-0.19189592 -0.8879351 ]\n",
            "Train loss: 0.6833595055322841, Test loss: 0.6841469532684447\n",
            "\n",
            "\n",
            "Epoch 85 -\n",
            "Updated K params after epoch 85: [-0.19189767 -0.88794576]\n",
            "Train loss: 0.6831027343211395, Test loss: 0.6839135787977355\n",
            "\n",
            "\n",
            "Epoch 86 -\n",
            "Updated K params after epoch 86: [-0.19189945 -0.88795675]\n",
            "Train loss: 0.6828391554096052, Test loss: 0.683674064991319\n",
            "\n",
            "\n",
            "Epoch 87 -\n",
            "Updated K params after epoch 87: [-0.19190125 -0.88796807]\n",
            "Train loss: 0.6825685826504703, Test loss: 0.6834282445241996\n",
            "\n",
            "\n",
            "Epoch 88 -\n",
            "Updated K params after epoch 88: [-0.19190307 -0.88797974]\n",
            "Train loss: 0.6822908261073287, Test loss: 0.6831759467333642\n",
            "\n",
            "\n",
            "Epoch 89 -\n",
            "Updated K params after epoch 89: [-0.19190491 -0.88799177]\n",
            "Train loss: 0.6820056920454037, Test loss: 0.6829169976149099\n",
            "\n",
            "\n",
            "Epoch 90 -\n",
            "Updated K params after epoch 90: [-0.19190678 -0.88800417]\n",
            "Train loss: 0.6817129829277095, Test loss: 0.682651219826237\n",
            "\n",
            "\n",
            "Epoch 91 -\n",
            "Updated K params after epoch 91: [-0.19190867 -0.88801695]\n",
            "Train loss: 0.6814124974169365, Test loss: 0.682378432693664\n",
            "\n",
            "\n",
            "Epoch 92 -\n",
            "Updated K params after epoch 92: [-0.19191057 -0.88803011]\n",
            "Train loss: 0.6811040303834671, Test loss: 0.682098452225831\n",
            "\n",
            "\n",
            "Epoch 93 -\n",
            "Updated K params after epoch 93: [-0.19191251 -0.88804367]\n",
            "Train loss: 0.680787372919937, Test loss: 0.6818110911332744\n",
            "\n",
            "\n",
            "Epoch 94 -\n",
            "Updated K params after epoch 94: [-0.19191446 -0.88805765]\n",
            "Train loss: 0.6804623123627798, Test loss: 0.6815161588545684\n",
            "\n",
            "\n",
            "Epoch 95 -\n",
            "Updated K params after epoch 95: [-0.19191643 -0.88807205]\n",
            "Train loss: 0.6801286323212022, Test loss: 0.6812134615894402\n",
            "\n",
            "\n",
            "Epoch 96 -\n",
            "Updated K params after epoch 96: [-0.19191843 -0.88808688]\n",
            "Train loss: 0.6797861127140569, Test loss: 0.68090280233928\n",
            "\n",
            "\n",
            "Epoch 97 -\n",
            "Updated K params after epoch 97: [-0.19192045 -0.88810215]\n",
            "Train loss: 0.679434529815089, Test loss: 0.6805839809554763\n",
            "\n",
            "\n",
            "Epoch 98 -\n",
            "Updated K params after epoch 98: [-0.19192248 -0.88811789]\n",
            "Train loss: 0.6790736563070512, Test loss: 0.6802567941960206\n",
            "\n",
            "\n",
            "Epoch 99 -\n",
            "Updated K params after epoch 99: [-0.19192454 -0.88813409]\n",
            "Train loss: 0.6787032613451905, Test loss: 0.6799210357908334\n",
            "\n",
            "\n",
            "Epoch 100 -\n",
            "Updated K params after epoch 100: [-0.19192662 -0.88815078]\n",
            "Train loss: 0.6783231106306269, Test loss: 0.6795764965162759\n",
            "\n",
            "\n",
            "Epoch 101 -\n",
            "Updated K params after epoch 101: [-0.19192873 -0.88816796]\n",
            "Train loss: 0.6779329664941491, Test loss: 0.679222964279315\n",
            "\n",
            "\n",
            "Epoch 102 -\n",
            "Updated K params after epoch 102: [-0.19193085 -0.88818565]\n",
            "Train loss: 0.6775325879909704, Test loss: 0.6788602242118214\n",
            "\n",
            "\n",
            "Epoch 103 -\n",
            "Updated K params after epoch 103: [-0.19193299 -0.88820387]\n",
            "Train loss: 0.6771217310069846, Test loss: 0.6784880587754829\n",
            "\n",
            "\n",
            "Epoch 104 -\n",
            "Updated K params after epoch 104: [-0.19193515 -0.88822261]\n",
            "Train loss: 0.676700148377084, Test loss: 0.6781062478778186\n",
            "\n",
            "\n",
            "Epoch 105 -\n",
            "Updated K params after epoch 105: [-0.19193734 -0.88824191]\n",
            "Train loss: 0.6762675900160904, Test loss: 0.6777145689997837\n",
            "\n",
            "\n",
            "Epoch 106 -\n",
            "Updated K params after epoch 106: [-0.19193954 -0.88826178]\n",
            "Train loss: 0.6758238030628645, Test loss: 0.6773127973354515\n",
            "\n",
            "\n",
            "Epoch 107 -\n",
            "Updated K params after epoch 107: [-0.19194176 -0.88828222]\n",
            "Train loss: 0.6753685320381556, Test loss: 0.6769007059442617\n",
            "\n",
            "\n",
            "Epoch 108 -\n",
            "Updated K params after epoch 108: [-0.191944   -0.88830325]\n",
            "Train loss: 0.6749015190167503, Test loss: 0.6764780659163097\n",
            "\n",
            "\n",
            "Epoch 109 -\n",
            "Updated K params after epoch 109: [-0.19194627 -0.8883249 ]\n",
            "Train loss: 0.674422503814477, Test loss: 0.6760446465511571\n",
            "\n",
            "\n",
            "Epoch 110 -\n",
            "Updated K params after epoch 110: [-0.19194855 -0.88834717]\n",
            "Train loss: 0.6739312241906124, Test loss: 0.6756002155506196\n",
            "\n",
            "\n",
            "Epoch 111 -\n",
            "Updated K params after epoch 111: [-0.19195085 -0.88837007]\n",
            "Train loss: 0.6734274160662272, Test loss: 0.6751445392259827\n",
            "\n",
            "\n",
            "Epoch 112 -\n",
            "Updated K params after epoch 112: [-0.19195316 -0.88839363]\n",
            "Train loss: 0.6729108137589942, Test loss: 0.6746773827200754\n",
            "\n",
            "\n",
            "Epoch 113 -\n",
            "Updated K params after epoch 113: [-0.1919555  -0.88841787]\n",
            "Train loss: 0.6723811502349561, Test loss: 0.6741985102446105\n",
            "\n",
            "\n",
            "Epoch 114 -\n",
            "Updated K params after epoch 114: [-0.19195785 -0.88844278]\n",
            "Train loss: 0.671838157377739, Test loss: 0.6737076853331724\n",
            "\n",
            "\n",
            "Epoch 115 -\n",
            "Updated K params after epoch 115: [-0.19196023 -0.88846841]\n",
            "Train loss: 0.6712815662756576, Test loss: 0.6732046711102075\n",
            "\n",
            "\n",
            "Epoch 116 -\n",
            "Updated K params after epoch 116: [-0.19196261 -0.88849475]\n",
            "Train loss: 0.6707111075271357, Test loss: 0.6726892305763328\n",
            "\n",
            "\n",
            "Epoch 117 -\n",
            "Updated K params after epoch 117: [-0.19196502 -0.88852183]\n",
            "Train loss: 0.6701265115648168, Test loss: 0.6721611269102411\n",
            "\n",
            "\n",
            "Epoch 118 -\n",
            "Updated K params after epoch 118: [-0.19196744 -0.88854967]\n",
            "Train loss: 0.6695275089987085, Test loss: 0.6716201237874373\n",
            "\n",
            "\n",
            "Epoch 119 -\n",
            "Updated K params after epoch 119: [-0.19196988 -0.88857828]\n",
            "Train loss: 0.6689138309786407, Test loss: 0.6710659857159819\n",
            "\n",
            "\n",
            "Epoch 120 -\n",
            "Updated K params after epoch 120: [-0.19197233 -0.88860767]\n",
            "Train loss: 0.6682852095762765, Test loss: 0.6704984783893735\n",
            "\n",
            "\n",
            "Epoch 121 -\n",
            "Updated K params after epoch 121: [-0.1919748  -0.88863788]\n",
            "Train loss: 0.6676413781868383, Test loss: 0.6699173690566282\n",
            "\n",
            "\n",
            "Epoch 122 -\n",
            "Updated K params after epoch 122: [-0.19197728 -0.88866891]\n",
            "Train loss: 0.6669820719506523, Test loss: 0.6693224269095511\n",
            "\n",
            "\n",
            "Epoch 123 -\n",
            "Updated K params after epoch 123: [-0.19197978 -0.88870078]\n",
            "Train loss: 0.6663070281945342, Test loss: 0.6687134234871182\n",
            "\n",
            "\n",
            "Epoch 124 -\n",
            "Updated K params after epoch 124: [-0.19198229 -0.88873352]\n",
            "Train loss: 0.6656159868929558, Test loss: 0.6680901330968034\n",
            "\n",
            "\n",
            "Epoch 125 -\n",
            "Updated K params after epoch 125: [-0.19198481 -0.88876713]\n",
            "Train loss: 0.6649086911488401, Test loss: 0.6674523332526023\n",
            "\n",
            "\n",
            "Epoch 126 -\n",
            "Updated K params after epoch 126: [-0.19198734 -0.88880164]\n",
            "Train loss: 0.6641848876937374, Test loss: 0.6667998051293993\n",
            "\n",
            "\n",
            "Epoch 127 -\n",
            "Updated K params after epoch 127: [-0.19198989 -0.88883707]\n",
            "Train loss: 0.6634443274070245, Test loss: 0.6661323340332329\n",
            "\n",
            "\n",
            "Epoch 128 -\n",
            "Updated K params after epoch 128: [-0.19199245 -0.88887344]\n",
            "Train loss: 0.662686765853655, Test loss: 0.6654497098868984\n",
            "\n",
            "\n",
            "Epoch 129 -\n",
            "Updated K params after epoch 129: [-0.19199502 -0.88891076]\n",
            "Train loss: 0.6619119638398728, Test loss: 0.6647517277302137\n",
            "\n",
            "\n",
            "Epoch 130 -\n",
            "Updated K params after epoch 130: [-0.19199759 -0.88894906]\n",
            "Train loss: 0.6611196879861634, Test loss: 0.6640381882341516\n",
            "\n",
            "\n",
            "Epoch 131 -\n",
            "Updated K params after epoch 131: [-0.19200018 -0.88898834]\n",
            "Train loss: 0.6603097113165842, Test loss: 0.6633088982279154\n",
            "\n",
            "\n",
            "Epoch 132 -\n",
            "Updated K params after epoch 132: [-0.19200277 -0.88902864]\n",
            "Train loss: 0.6594818138634765, Test loss: 0.6625636712378986\n",
            "\n",
            "\n",
            "Epoch 133 -\n",
            "Updated K params after epoch 133: [-0.19200538 -0.88906997]\n",
            "Train loss: 0.6586357832863986, Test loss: 0.6618023280373307\n",
            "\n",
            "\n",
            "Epoch 134 -\n",
            "Updated K params after epoch 134: [-0.19200798 -0.88911235]\n",
            "Train loss: 0.657771415503977, Test loss: 0.6610246972052702\n",
            "\n",
            "\n",
            "Epoch 135 -\n",
            "Updated K params after epoch 135: [-0.1920106 -0.8891558]\n",
            "Train loss: 0.6568885153371976, Test loss: 0.6602306156934511\n",
            "\n",
            "\n",
            "Epoch 136 -\n",
            "Updated K params after epoch 136: [-0.19201322 -0.88920033]\n",
            "Train loss: 0.655986897162501, Test loss: 0.6594199293993467\n",
            "\n",
            "\n",
            "Epoch 137 -\n",
            "Updated K params after epoch 137: [-0.19201584 -0.88924597]\n",
            "Train loss: 0.6550663855728674, Test loss: 0.6585924937436544\n",
            "\n",
            "\n",
            "Epoch 138 -\n",
            "Updated K params after epoch 138: [-0.19201846 -0.88929273]\n",
            "Train loss: 0.6541268160449071, Test loss: 0.6577481742502516\n",
            "\n",
            "\n",
            "Epoch 139 -\n",
            "Updated K params after epoch 139: [-0.19202109 -0.88934063]\n",
            "Train loss: 0.6531680356097911, Test loss: 0.6568868471265197\n",
            "\n",
            "\n",
            "Epoch 140 -\n",
            "Updated K params after epoch 140: [-0.19202372 -0.8893897 ]\n",
            "Train loss: 0.6521899035256865, Test loss: 0.6560083998417781\n",
            "\n",
            "\n",
            "Epoch 141 -\n",
            "Updated K params after epoch 141: [-0.19202635 -0.88943994]\n",
            "Train loss: 0.6511922919491752, Test loss: 0.6551127317014183\n",
            "\n",
            "\n",
            "Epoch 142 -\n",
            "Updated K params after epoch 142: [-0.19202897 -0.88949139]\n",
            "Train loss: 0.6501750866029696, Test loss: 0.654199754414183\n",
            "\n",
            "\n",
            "Epoch 143 -\n",
            "Updated K params after epoch 143: [-0.1920316  -0.88954404]\n",
            "Train loss: 0.6491381874370656, Test loss: 0.6532693926498904\n",
            "\n",
            "\n",
            "Epoch 144 -\n",
            "Updated K params after epoch 144: [-0.19203422 -0.88959792]\n",
            "Train loss: 0.648081509280305, Test loss: 0.6523215845847758\n",
            "\n",
            "\n",
            "Epoch 145 -\n",
            "Updated K params after epoch 145: [-0.19203683 -0.88965306]\n",
            "Train loss: 0.6470049824791733, Test loss: 0.651356282431494\n",
            "\n",
            "\n",
            "Epoch 146 -\n",
            "Updated K params after epoch 146: [-0.19203944 -0.88970946]\n",
            "Train loss: 0.6459085535205034, Test loss: 0.6503734529507145\n",
            "\n",
            "\n",
            "Epoch 147 -\n",
            "Updated K params after epoch 147: [-0.19204204 -0.88976713]\n",
            "Train loss: 0.6447921856346267, Test loss: 0.6493730779411448\n",
            "\n",
            "\n",
            "Epoch 148 -\n",
            "Updated K params after epoch 148: [-0.19204463 -0.88982611]\n",
            "Train loss: 0.6436558593753956, Test loss: 0.6483551547047335\n",
            "\n",
            "\n",
            "Epoch 149 -\n",
            "Updated K params after epoch 149: [-0.19204721 -0.88988639]\n",
            "Train loss: 0.6424995731733943, Test loss: 0.6473196964837358\n",
            "\n",
            "\n",
            "Epoch 150 -\n",
            "Updated K params after epoch 150: [-0.19204978 -0.88994801]\n",
            "Train loss: 0.6413233438585769, Test loss: 0.6462667328662861\n",
            "\n",
            "\n",
            "Epoch 151 -\n",
            "Updated K params after epoch 151: [-0.19205234 -0.89001096]\n",
            "Train loss: 0.64012720714851, Test loss: 0.6451963101570931\n",
            "\n",
            "\n",
            "Epoch 152 -\n",
            "Updated K params after epoch 152: [-0.19205488 -0.89007526]\n",
            "Train loss: 0.6389112180983557, Test loss: 0.6441084917098749\n",
            "\n",
            "\n",
            "Epoch 153 -\n",
            "Updated K params after epoch 153: [-0.19205741 -0.89014093]\n",
            "Train loss: 0.6376754515087295, Test loss: 0.6430033582181852\n",
            "\n",
            "\n",
            "Epoch 154 -\n",
            "Updated K params after epoch 154: [-0.19205992 -0.89020798]\n",
            "Train loss: 0.6364200022875758, Test loss: 0.6418810079613265\n",
            "\n",
            "\n",
            "Epoch 155 -\n",
            "Updated K params after epoch 155: [-0.1920624  -0.89027642]\n",
            "Train loss: 0.635144985762264, Test loss: 0.6407415570021414\n",
            "\n",
            "\n",
            "Epoch 156 -\n",
            "Updated K params after epoch 156: [-0.19206487 -0.89034627]\n",
            "Train loss: 0.6338505379381807, Test loss: 0.6395851393335819\n",
            "\n",
            "\n",
            "Epoch 157 -\n",
            "Updated K params after epoch 157: [-0.19206732 -0.89041752]\n",
            "Train loss: 0.6325368157002171, Test loss: 0.638411906971109\n",
            "\n",
            "\n",
            "Epoch 158 -\n",
            "Updated K params after epoch 158: [-0.19206974 -0.8904902 ]\n",
            "Train loss: 0.6312039969537021, Test loss: 0.6372220299881565\n",
            "\n",
            "\n",
            "Epoch 159 -\n",
            "Updated K params after epoch 159: [-0.19207213 -0.8905643 ]\n",
            "Train loss: 0.6298522807015188, Test loss: 0.6360156964921017\n",
            "\n",
            "\n",
            "Epoch 160 -\n",
            "Updated K params after epoch 160: [-0.1920745  -0.89063985]\n",
            "Train loss: 0.6284818870543747, Test loss: 0.6347931125384471\n",
            "\n",
            "\n",
            "Epoch 161 -\n",
            "Updated K params after epoch 161: [-0.19207683 -0.89071683]\n",
            "Train loss: 0.6270930571714585, Test loss: 0.6335545019811851\n",
            "\n",
            "\n",
            "Epoch 162 -\n",
            "Updated K params after epoch 162: [-0.19207914 -0.89079527]\n",
            "Train loss: 0.6256860531290196, Test loss: 0.632300106257646\n",
            "\n",
            "\n",
            "Epoch 163 -\n",
            "Updated K params after epoch 163: [-0.19208141 -0.89087517]\n",
            "Train loss: 0.6242611577147539, Test loss: 0.6310301841064655\n",
            "\n",
            "\n",
            "Epoch 164 -\n",
            "Updated K params after epoch 164: [-0.19208364 -0.89095652]\n",
            "Train loss: 0.6228186741462494, Test loss: 0.6297450112176954\n",
            "\n",
            "\n",
            "Epoch 165 -\n",
            "Updated K params after epoch 165: [-0.19208584 -0.89103934]\n",
            "Train loss: 0.621358925712164, Test loss: 0.628444879814477\n",
            "\n",
            "\n",
            "Epoch 166 -\n",
            "Updated K params after epoch 166: [-0.192088   -0.89112363]\n",
            "Train loss: 0.6198822553352482, Test loss: 0.6271300981661324\n",
            "\n",
            "\n",
            "Epoch 167 -\n",
            "Updated K params after epoch 167: [-0.19209012 -0.89120938]\n",
            "Train loss: 0.6183890250568039, Test loss: 0.6258009900329757\n",
            "\n",
            "\n",
            "Epoch 168 -\n",
            "Updated K params after epoch 168: [-0.19209219 -0.8912966 ]\n",
            "Train loss: 0.6168796154426653, Test loss: 0.6244578940436231\n",
            "\n",
            "\n",
            "Epoch 169 -\n",
            "Updated K params after epoch 169: [-0.19209422 -0.89138529]\n",
            "Train loss: 0.6153544249113178, Test loss: 0.6231011630060538\n",
            "\n",
            "\n",
            "Epoch 170 -\n",
            "Updated K params after epoch 170: [-0.1920962  -0.89147544]\n",
            "Train loss: 0.613813868985303, Test loss: 0.6217311631541763\n",
            "\n",
            "\n",
            "Epoch 171 -\n",
            "Updated K params after epoch 171: [-0.19209813 -0.89156706]\n",
            "Train loss: 0.6122583794676189, Test loss: 0.620348273332148\n",
            "\n",
            "\n",
            "Epoch 172 -\n",
            "Updated K params after epoch 172: [-0.1921     -0.89166014]\n",
            "Train loss: 0.6106884035453762, Test loss: 0.6189528841191906\n",
            "\n",
            "\n",
            "Epoch 173 -\n",
            "Updated K params after epoch 173: [-0.19210183 -0.89175467]\n",
            "Train loss: 0.6091044028235415, Test loss: 0.6175453968981345\n",
            "\n",
            "\n",
            "Epoch 174 -\n",
            "Updated K params after epoch 174: [-0.19210359 -0.89185066]\n",
            "Train loss: 0.6075068522921478, Test loss: 0.6161262228714063\n",
            "\n",
            "\n",
            "Epoch 175 -\n",
            "Updated K params after epoch 175: [-0.1921053  -0.89194809]\n",
            "Train loss: 0.6058962392309036, Test loss: 0.6146957820286245\n",
            "\n",
            "\n",
            "Epoch 176 -\n",
            "Updated K params after epoch 176: [-0.19210695 -0.89204695]\n",
            "Train loss: 0.6042730620556572, Test loss: 0.6132545020704132\n",
            "\n",
            "\n",
            "Epoch 177 -\n",
            "Updated K params after epoch 177: [-0.19210854 -0.89214724]\n",
            "Train loss: 0.6026378291116785, Test loss: 0.6118028172934398\n",
            "\n",
            "\n",
            "Epoch 178 -\n",
            "Updated K params after epoch 178: [-0.19211006 -0.89224895]\n",
            "Train loss: 0.6009910574191929, Test loss: 0.6103411674420586\n",
            "\n",
            "\n",
            "Epoch 179 -\n",
            "Updated K params after epoch 179: [-0.19211151 -0.89235206]\n",
            "Train loss: 0.5993332713770405, Test loss: 0.6088699965322705\n",
            "\n",
            "\n",
            "Epoch 180 -\n",
            "Updated K params after epoch 180: [-0.1921129  -0.89245658]\n",
            "Train loss: 0.5976650014307253, Test loss: 0.6073897516539926\n",
            "\n",
            "\n",
            "Epoch 181 -\n",
            "Updated K params after epoch 181: [-0.19211421 -0.89256247]\n",
            "Train loss: 0.5959867827114662, Test loss: 0.6059008817578646\n",
            "\n",
            "\n",
            "Epoch 182 -\n",
            "Updated K params after epoch 182: [-0.19211545 -0.89266974]\n",
            "Train loss: 0.5942991536531559, Test loss: 0.6044038364330045\n",
            "\n",
            "\n",
            "Epoch 183 -\n",
            "Updated K params after epoch 183: [-0.19211661 -0.89277837]\n",
            "Train loss: 0.5926026545943633, Test loss: 0.6028990646822486\n",
            "\n",
            "\n",
            "Epoch 184 -\n",
            "Updated K params after epoch 184: [-0.1921177  -0.89288834]\n",
            "Train loss: 0.5908978263726943, Test loss: 0.6013870137014771\n",
            "\n",
            "\n",
            "Epoch 185 -\n",
            "Updated K params after epoch 185: [-0.1921187  -0.89299963]\n",
            "Train loss: 0.5891852089189273, Test loss: 0.5998681276696334\n",
            "\n",
            "\n",
            "Epoch 186 -\n",
            "Updated K params after epoch 186: [-0.19211963 -0.89311224]\n",
            "Train loss: 0.587465339858393, Test loss: 0.5983428465559949\n",
            "\n",
            "\n",
            "Epoch 187 -\n",
            "Updated K params after epoch 187: [-0.19212047 -0.89322614]\n",
            "Train loss: 0.5857387531270276, Test loss: 0.5968116049511325\n",
            "\n",
            "\n",
            "Epoch 188 -\n",
            "Updated K params after epoch 188: [-0.19212122 -0.89334132]\n",
            "Train loss: 0.5840059776094514, Test loss: 0.5952748309278246\n",
            "\n",
            "\n",
            "Epoch 189 -\n",
            "Updated K params after epoch 189: [-0.19212188 -0.89345776]\n",
            "Train loss: 0.5822675358062527, Test loss: 0.5937329449379629\n",
            "\n",
            "\n",
            "Epoch 190 -\n",
            "Updated K params after epoch 190: [-0.19212245 -0.89357544]\n",
            "Train loss: 0.5805239425374398, Test loss: 0.5921863587511966\n",
            "\n",
            "\n",
            "Epoch 191 -\n",
            "Updated K params after epoch 191: [-0.19212293 -0.89369434]\n",
            "Train loss: 0.5787757036887301, Test loss: 0.5906354744407258\n",
            "\n",
            "\n",
            "Epoch 192 -\n",
            "Updated K params after epoch 192: [-0.19212332 -0.89381444]\n",
            "Train loss: 0.5770233150070014, Test loss: 0.589080683421272\n",
            "\n",
            "\n",
            "Epoch 193 -\n",
            "Updated K params after epoch 193: [-0.1921236  -0.89393573]\n",
            "Train loss: 0.5752672609508311, Test loss: 0.587522365543824\n",
            "\n",
            "\n",
            "Epoch 194 -\n",
            "Updated K params after epoch 194: [-0.19212379 -0.89405818]\n",
            "Train loss: 0.5735080136015894, Test loss: 0.5859608882512986\n",
            "\n",
            "\n",
            "Epoch 195 -\n",
            "Updated K params after epoch 195: [-0.19212388 -0.89418177]\n",
            "Train loss: 0.5717460316400682, Test loss: 0.5843966057987567\n",
            "\n",
            "\n",
            "Epoch 196 -\n",
            "Updated K params after epoch 196: [-0.19212386 -0.89430648]\n",
            "Train loss: 0.5699817593930853, Test loss: 0.5828298585413006\n",
            "\n",
            "\n",
            "Epoch 197 -\n",
            "Updated K params after epoch 197: [-0.19212375 -0.89443229]\n",
            "Train loss: 0.5682156259539417, Test loss: 0.5812609722922386\n",
            "\n",
            "\n",
            "Epoch 198 -\n",
            "Updated K params after epoch 198: [-0.19212352 -0.89455919]\n",
            "Train loss: 0.5664480443800286, Test loss: 0.5796902577535585\n",
            "\n",
            "\n",
            "Epoch 199 -\n",
            "Updated K params after epoch 199: [-0.19212319 -0.89468714]\n",
            "Train loss: 0.5646794109702622, Test loss: 0.5781180100201933\n",
            "\n",
            "\n",
            "Epoch 200 -\n",
            "Updated K params after epoch 200: [-0.19212274 -0.89481613]\n",
            "Train loss: 0.5629101046244245, Test loss: 0.5765445081590146\n",
            "\n",
            "\n",
            "Epoch 201 -\n",
            "Updated K params after epoch 201: [-0.19212219 -0.89494614]\n",
            "Train loss: 0.5611404862858623, Test loss: 0.5749700148629412\n",
            "\n",
            "\n",
            "Epoch 202 -\n",
            "Updated K params after epoch 202: [-0.19212152 -0.89507714]\n",
            "Train loss: 0.5593708984683867, Test loss: 0.5733947761800234\n",
            "\n",
            "\n",
            "Epoch 203 -\n",
            "Updated K params after epoch 203: [-0.19212074 -0.89520912]\n",
            "Train loss: 0.5576016648676186, Test loss: 0.5718190213168467\n",
            "\n",
            "\n",
            "Epoch 204 -\n",
            "Updated K params after epoch 204: [-0.19211984 -0.89534206]\n",
            "Train loss: 0.5558330900564344, Test loss: 0.570242962515114\n",
            "\n",
            "\n",
            "Epoch 205 -\n",
            "Updated K params after epoch 205: [-0.19211882 -0.89547593]\n",
            "Train loss: 0.554065459263618, Test loss: 0.5686667949998058\n",
            "\n",
            "\n",
            "Epoch 206 -\n",
            "Updated K params after epoch 206: [-0.19211768 -0.89561071]\n",
            "Train loss: 0.5522990382342823, Test loss: 0.5670906969968877\n",
            "\n",
            "\n",
            "Epoch 207 -\n",
            "Updated K params after epoch 207: [-0.19211643 -0.89574639]\n",
            "Train loss: 0.5505340731701347, Test loss: 0.5655148298181472\n",
            "\n",
            "\n",
            "Epoch 208 -\n",
            "Updated K params after epoch 208: [-0.19211505 -0.89588295]\n",
            "Train loss: 0.5487707907471978, Test loss: 0.563939338010383\n",
            "\n",
            "\n",
            "Epoch 209 -\n",
            "Updated K params after epoch 209: [-0.19211355 -0.89602036]\n",
            "Train loss: 0.5470093982081777, Test loss: 0.5623643495658645\n",
            "\n",
            "\n",
            "Epoch 210 -\n",
            "Updated K params after epoch 210: [-0.19211192 -0.89615861]\n",
            "Train loss: 0.5452500835262915, Test loss: 0.5607899761906999\n",
            "\n",
            "\n",
            "Epoch 211 -\n",
            "Updated K params after epoch 211: [-0.19211017 -0.89629768]\n",
            "Train loss: 0.5434930156370461, Test loss: 0.5592163136275251\n",
            "\n",
            "\n",
            "Epoch 212 -\n",
            "Updated K params after epoch 212: [-0.19210829 -0.89643756]\n",
            "Train loss: 0.5417383447341596, Test loss: 0.5576434420287435\n",
            "\n",
            "\n",
            "Epoch 213 -\n",
            "Updated K params after epoch 213: [-0.19210628 -0.89657822]\n",
            "Train loss: 0.5399862026255907, Test loss: 0.5560714263763903\n",
            "\n",
            "\n",
            "Epoch 214 -\n",
            "Updated K params after epoch 214: [-0.19210414 -0.89671965]\n",
            "Train loss: 0.5382367031454459, Test loss: 0.554500316944605\n",
            "\n",
            "\n",
            "Epoch 215 -\n",
            "Updated K params after epoch 215: [-0.19210187 -0.89686183]\n",
            "Train loss: 0.5364899426173815, Test loss: 0.552930149800618\n",
            "\n",
            "\n",
            "Epoch 216 -\n",
            "Updated K params after epoch 216: [-0.19209948 -0.89700476]\n",
            "Train loss: 0.5347460003650292, Test loss: 0.5513609473401381\n",
            "\n",
            "\n",
            "Epoch 217 -\n",
            "Updated K params after epoch 217: [-0.19209695 -0.8971484 ]\n",
            "Train loss: 0.5330049392649004, Test loss: 0.5497927188530343\n",
            "\n",
            "\n",
            "Epoch 218 -\n",
            "Updated K params after epoch 218: [-0.19209428 -0.89729276]\n",
            "Train loss: 0.5312668063372171, Test loss: 0.5482254611152444\n",
            "\n",
            "\n",
            "Epoch 219 -\n",
            "Updated K params after epoch 219: [-0.19209149 -0.89743781]\n",
            "Train loss: 0.5295316333701288, Test loss: 0.5466591590029171\n",
            "\n",
            "\n",
            "Epoch 220 -\n",
            "Updated K params after epoch 220: [-0.19208856 -0.89758355]\n",
            "Train loss: 0.5277994375728347, Test loss: 0.5450937861248892\n",
            "\n",
            "\n",
            "Epoch 221 -\n",
            "Updated K params after epoch 221: [-0.19208549 -0.89772996]\n",
            "Train loss: 0.5260702222532135, Test loss: 0.5435293054697278\n",
            "\n",
            "\n",
            "Epoch 222 -\n",
            "Updated K params after epoch 222: [-0.19208229 -0.89787702]\n",
            "Train loss: 0.5243439775156813, Test loss: 0.541965670063705\n",
            "\n",
            "\n",
            "Epoch 223 -\n",
            "Updated K params after epoch 223: [-0.19207895 -0.89802474]\n",
            "Train loss: 0.5226206809751333, Test loss: 0.5404028236362414\n",
            "\n",
            "\n",
            "Epoch 224 -\n",
            "Updated K params after epoch 224: [-0.19207548 -0.8981731 ]\n",
            "Train loss: 0.5209002984829973, Test loss: 0.5388407012895261\n",
            "\n",
            "\n",
            "Epoch 225 -\n",
            "Updated K params after epoch 225: [-0.19207187 -0.89832209]\n",
            "Train loss: 0.5191827848616021, Test loss: 0.5372792301692162\n",
            "\n",
            "\n",
            "Epoch 226 -\n",
            "Updated K params after epoch 226: [-0.19206812 -0.8984717 ]\n",
            "Train loss: 0.5174680846432592, Test loss: 0.5357183301333124\n",
            "\n",
            "\n",
            "Epoch 227 -\n",
            "Updated K params after epoch 227: [-0.19206423 -0.89862193]\n",
            "Train loss: 0.5157561328106773, Test loss: 0.5341579144165187\n",
            "\n",
            "\n",
            "Epoch 228 -\n",
            "Updated K params after epoch 228: [-0.1920602  -0.89877276]\n",
            "Train loss: 0.5140468555355363, Test loss: 0.5325978902875969\n",
            "\n",
            "\n",
            "Epoch 229 -\n",
            "Updated K params after epoch 229: [-0.19205604 -0.89892419]\n",
            "Train loss: 0.5123401709122783, Test loss: 0.5310381596974465\n",
            "\n",
            "\n",
            "Epoch 230 -\n",
            "Updated K params after epoch 230: [-0.19205173 -0.89907622]\n",
            "Train loss: 0.5106359896844093, Test loss: 0.5294786199158446\n",
            "\n",
            "\n",
            "Epoch 231 -\n",
            "Updated K params after epoch 231: [-0.19204729 -0.89922883]\n",
            "Train loss: 0.5089342159608218, Test loss: 0.5279191641549935\n",
            "\n",
            "\n",
            "Epoch 232 -\n",
            "Updated K params after epoch 232: [-0.19204271 -0.89938203]\n",
            "Train loss: 0.5072347479198936, Test loss: 0.5263596821782285\n",
            "\n",
            "\n",
            "Epoch 233 -\n",
            "Updated K params after epoch 233: [-0.19203799 -0.89953582]\n",
            "Train loss: 0.505537478499339, Test loss: 0.5248000608924339\n",
            "\n",
            "\n",
            "Epoch 234 -\n",
            "Updated K params after epoch 234: [-0.19203312 -0.89969018]\n",
            "Train loss: 0.5038422960700066, Test loss: 0.5232401849229156\n",
            "\n",
            "\n",
            "Epoch 235 -\n",
            "Updated K params after epoch 235: [-0.19202812 -0.89984511]\n",
            "Train loss: 0.5021490850920455, Test loss: 0.5216799371696542\n",
            "\n",
            "\n",
            "Epoch 236 -\n",
            "Updated K params after epoch 236: [-0.19202298 -0.90000062]\n",
            "Train loss: 0.5004577267520591, Test loss: 0.5201191993440468\n",
            "\n",
            "\n",
            "Epoch 237 -\n",
            "Updated K params after epoch 237: [-0.1920177 -0.9001567]\n",
            "Train loss: 0.498768099580076, Test loss: 0.5185578524854048\n",
            "\n",
            "\n",
            "Epoch 238 -\n",
            "Updated K params after epoch 238: [-0.19201227 -0.90031334]\n",
            "Train loss: 0.49708008004534554, Test loss: 0.5169957774566368\n",
            "\n",
            "\n",
            "Epoch 239 -\n",
            "Updated K params after epoch 239: [-0.19200671 -0.90047056]\n",
            "Train loss: 0.49539354313015926, Test loss: 0.5154328554186907\n",
            "\n",
            "\n",
            "Epoch 240 -\n",
            "Updated K params after epoch 240: [-0.19200101 -0.90062835]\n",
            "Train loss: 0.493708362881057, Test loss: 0.513868968283459\n",
            "\n",
            "\n",
            "Epoch 241 -\n",
            "Updated K params after epoch 241: [-0.19199517 -0.9007867 ]\n",
            "Train loss: 0.49202441293694, Test loss: 0.5123039991449845\n",
            "\n",
            "\n",
            "Epoch 242 -\n",
            "Updated K params after epoch 242: [-0.19198919 -0.90094563]\n",
            "Train loss: 0.490341567033758, Test loss: 0.5107378326889095\n",
            "\n",
            "\n",
            "Epoch 243 -\n",
            "Updated K params after epoch 243: [-0.19198307 -0.90110513]\n",
            "Train loss: 0.48865969948557186, Test loss: 0.5091703555802174\n",
            "\n",
            "\n",
            "Epoch 244 -\n",
            "Updated K params after epoch 244: [-0.19197681 -0.90126521]\n",
            "Train loss: 0.48697868564191366, Test loss: 0.5076014568294108\n",
            "\n",
            "\n",
            "Epoch 245 -\n",
            "Updated K params after epoch 245: [-0.19197041 -0.90142586]\n",
            "Train loss: 0.48529840232148275, Test loss: 0.5060310281373482\n",
            "\n",
            "\n",
            "Epoch 246 -\n",
            "Updated K params after epoch 246: [-0.19196387 -0.90158709]\n",
            "Train loss: 0.48361872822231206, Test loss: 0.5044589642190362\n",
            "\n",
            "\n",
            "Epoch 247 -\n",
            "Updated K params after epoch 247: [-0.1919572 -0.9017489]\n",
            "Train loss: 0.48193954430862984, Test loss: 0.5028851631067396\n",
            "\n",
            "\n",
            "Epoch 248 -\n",
            "Updated K params after epoch 248: [-0.19195039 -0.9019113 ]\n",
            "Train loss: 0.4802607341747216, Test loss: 0.5013095264328213\n",
            "\n",
            "\n",
            "Epoch 249 -\n",
            "Updated K params after epoch 249: [-0.19194343 -0.90207429]\n",
            "Train loss: 0.47858218438616584, Test loss: 0.4997319596927766\n",
            "\n",
            "\n",
            "Epoch 250 -\n",
            "Updated K params after epoch 250: [-0.19193635 -0.90223787]\n",
            "Train loss: 0.4769037847988776, Test loss: 0.49815237248895816\n",
            "\n",
            "\n",
            "Epoch 251 -\n",
            "Updated K params after epoch 251: [-0.19192912 -0.90240205]\n",
            "Train loss: 0.4752254288564425, Test loss: 0.49657067875552696\n",
            "\n",
            "\n",
            "Epoch 252 -\n",
            "Updated K params after epoch 252: [-0.19192176 -0.90256683]\n",
            "Train loss: 0.4735470138662689, Test loss: 0.4949867969651789\n",
            "\n",
            "\n",
            "Epoch 253 -\n",
            "Updated K params after epoch 253: [-0.19191426 -0.90273221]\n",
            "Train loss: 0.47186844125511995, Test loss: 0.49340065031822594\n",
            "\n",
            "\n",
            "Epoch 254 -\n",
            "Updated K params after epoch 254: [-0.19190663 -0.90289821]\n",
            "Train loss: 0.47018961680461235, Test loss: 0.4918121669146143\n",
            "\n",
            "\n",
            "Epoch 255 -\n",
            "Updated K params after epoch 255: [-0.19189886 -0.90306482]\n",
            "Train loss: 0.46851045086729576, Test loss: 0.4902212799094762\n",
            "\n",
            "\n",
            "Epoch 256 -\n",
            "Updated K params after epoch 256: [-0.19189095 -0.90323206]\n",
            "Train loss: 0.4668308585639324, Test loss: 0.48862792765280855\n",
            "\n",
            "\n",
            "Epoch 257 -\n",
            "Updated K params after epoch 257: [-0.19188291 -0.90339992]\n",
            "Train loss: 0.46515075996261684, Test loss: 0.48703205381387726\n",
            "\n",
            "\n",
            "Epoch 258 -\n",
            "Updated K params after epoch 258: [-0.19187474 -0.90356841]\n",
            "Train loss: 0.4634700802403694, Test loss: 0.48543360749093506\n",
            "\n",
            "\n",
            "Epoch 259 -\n",
            "Updated K params after epoch 259: [-0.19186643 -0.90373754]\n",
            "Train loss: 0.46178874982784746, Test loss: 0.4838325433068377\n",
            "\n",
            "\n",
            "Epoch 260 -\n",
            "Updated K params after epoch 260: [-0.19185799 -0.90390732]\n",
            "Train loss: 0.4601067045378068, Test loss: 0.4822288214911292\n",
            "\n",
            "\n",
            "Epoch 261 -\n",
            "Updated K params after epoch 261: [-0.19184942 -0.90407774]\n",
            "Train loss: 0.4584238856779428, Test loss: 0.48062240794915484\n",
            "\n",
            "\n",
            "Epoch 262 -\n",
            "Updated K params after epoch 262: [-0.19184072 -0.90424882]\n",
            "Train loss: 0.45674024014873005, Test loss: 0.4790132743187478\n",
            "\n",
            "\n",
            "Epoch 263 -\n",
            "Updated K params after epoch 263: [-0.19183188 -0.90442056]\n",
            "Train loss: 0.45505572052686777, Test loss: 0.4774013980150168\n",
            "\n",
            "\n",
            "Epoch 264 -\n",
            "Updated K params after epoch 264: [-0.19182292 -0.90459297]\n",
            "Train loss: 0.45337028513492045, Test loss: 0.4757867622637429\n",
            "\n",
            "\n",
            "Epoch 265 -\n",
            "Updated K params after epoch 265: [-0.19181382 -0.90476605]\n",
            "Train loss: 0.45168389809773163, Test loss: 0.47416935612388056\n",
            "\n",
            "\n",
            "Epoch 266 -\n",
            "Updated K params after epoch 266: [-0.1918046  -0.90493981]\n",
            "Train loss: 0.4499965293861696, Test loss: 0.47254917449963246\n",
            "\n",
            "\n",
            "Epoch 267 -\n",
            "Updated K params after epoch 267: [-0.19179525 -0.90511425]\n",
            "Train loss: 0.4483081548487408, Test loss: 0.47092621814255353\n",
            "\n",
            "\n",
            "Epoch 268 -\n",
            "Updated K params after epoch 268: [-0.19178576 -0.90528938]\n",
            "Train loss: 0.4466187562315977, Test loss: 0.46930049364411536\n",
            "\n",
            "\n",
            "Epoch 269 -\n",
            "Updated K params after epoch 269: [-0.19177615 -0.9054652 ]\n",
            "Train loss: 0.44492832118743586, Test loss: 0.46767201341914666\n",
            "\n",
            "\n",
            "Epoch 270 -\n",
            "Updated K params after epoch 270: [-0.19176642 -0.90564173]\n",
            "Train loss: 0.443236843273766, Test loss: 0.46604079568054285\n",
            "\n",
            "\n",
            "Epoch 271 -\n",
            "Updated K params after epoch 271: [-0.19175656 -0.90581896]\n",
            "Train loss: 0.441544321941019, Test loss: 0.4644068644056201\n",
            "\n",
            "\n",
            "Epoch 272 -\n",
            "Updated K params after epoch 272: [-0.19174657 -0.9059969 ]\n",
            "Train loss: 0.4398507625109274, Test loss: 0.4627702492944731\n",
            "\n",
            "\n",
            "Epoch 273 -\n",
            "Updated K params after epoch 273: [-0.19173646 -0.90617555]\n",
            "Train loss: 0.4381561761456047, Test loss: 0.4611309857206718\n",
            "\n",
            "\n",
            "Epoch 274 -\n",
            "Updated K params after epoch 274: [-0.19172622 -0.90635493]\n",
            "Train loss: 0.4364605798077259, Test loss: 0.459489114674625\n",
            "\n",
            "\n",
            "Epoch 275 -\n",
            "Updated K params after epoch 275: [-0.19171586 -0.90653502]\n",
            "Train loss: 0.4347639962121949, Test loss: 0.4578446826999119\n",
            "\n",
            "\n",
            "Epoch 276 -\n",
            "Updated K params after epoch 276: [-0.19170538 -0.90671585]\n",
            "Train loss: 0.4330664537696674, Test loss: 0.45619774182287665\n",
            "\n",
            "\n",
            "Epoch 277 -\n",
            "Updated K params after epoch 277: [-0.19169477 -0.90689742]\n",
            "Train loss: 0.4313679865222795, Test loss: 0.4545483494757626\n",
            "\n",
            "\n",
            "Epoch 278 -\n",
            "Updated K params after epoch 278: [-0.19168404 -0.90707971]\n",
            "Train loss: 0.42966863407191974, Test loss: 0.45289656841364867\n",
            "\n",
            "\n",
            "Epoch 279 -\n",
            "Updated K params after epoch 279: [-0.1916732  -0.90726275]\n",
            "Train loss: 0.4279684415013647, Test loss: 0.4512424666254433\n",
            "\n",
            "\n",
            "Epoch 280 -\n",
            "Updated K params after epoch 280: [-0.19166223 -0.90744654]\n",
            "Train loss: 0.4262674592885871, Test loss: 0.44958611723917485\n",
            "\n",
            "\n",
            "Epoch 281 -\n",
            "Updated K params after epoch 281: [-0.19165114 -0.90763107]\n",
            "Train loss: 0.42456574321453266, Test loss: 0.4479275984218128\n",
            "\n",
            "\n",
            "Epoch 282 -\n",
            "Updated K params after epoch 282: [-0.19163994 -0.90781636]\n",
            "Train loss: 0.422863354264648, Test loss: 0.4462669932738408\n",
            "\n",
            "\n",
            "Epoch 283 -\n",
            "Updated K params after epoch 283: [-0.19162862 -0.90800239]\n",
            "Train loss: 0.4211603585244362, Test loss: 0.4446043897187998\n",
            "\n",
            "\n",
            "Epoch 284 -\n",
            "Updated K params after epoch 284: [-0.19161718 -0.90818919]\n",
            "Train loss: 0.41945682706930393, Test loss: 0.4429398803880085\n",
            "\n",
            "\n",
            "Epoch 285 -\n",
            "Updated K params after epoch 285: [-0.19160562 -0.90837674]\n",
            "Train loss: 0.4177528358489571, Test loss: 0.4412735625006674\n",
            "\n",
            "\n",
            "Epoch 286 -\n",
            "Updated K params after epoch 286: [-0.19159395 -0.90856505]\n",
            "Train loss: 0.41604846556659564, Test loss: 0.4396055377395436\n",
            "\n",
            "\n",
            "Epoch 287 -\n",
            "Updated K params after epoch 287: [-0.19158217 -0.90875413]\n",
            "Train loss: 0.4143438015531507, Test loss: 0.437935912122436\n",
            "\n",
            "\n",
            "Epoch 288 -\n",
            "Updated K params after epoch 288: [-0.19157027 -0.90894396]\n",
            "Train loss: 0.4126389336368057, Test loss: 0.43626479586961114\n",
            "\n",
            "\n",
            "Epoch 289 -\n",
            "Updated K params after epoch 289: [-0.19155826 -0.90913457]\n",
            "Train loss: 0.4109339560080331, Test loss: 0.43459230326740567\n",
            "\n",
            "\n",
            "Epoch 290 -\n",
            "Updated K params after epoch 290: [-0.19154614 -0.90932593]\n",
            "Train loss: 0.4092289670803829, Test loss: 0.43291855252818323\n",
            "\n",
            "\n",
            "Epoch 291 -\n",
            "Updated K params after epoch 291: [-0.19153391 -0.90951806]\n",
            "Train loss: 0.40752406934724966, Test loss: 0.43124366564684025\n",
            "\n",
            "\n",
            "Epoch 292 -\n",
            "Updated K params after epoch 292: [-0.19152156 -0.90971096]\n",
            "Train loss: 0.40581936923484807, Test loss: 0.4295677682540513\n",
            "\n",
            "\n",
            "Epoch 293 -\n",
            "Updated K params after epoch 293: [-0.19150911 -0.90990463]\n",
            "Train loss: 0.4041149769516252, Test loss: 0.4278909894664474\n",
            "\n",
            "\n",
            "Epoch 294 -\n",
            "Updated K params after epoch 294: [-0.19149655 -0.91009905]\n",
            "Train loss: 0.4024110063343359, Test loss: 0.4262134617339219\n",
            "\n",
            "\n",
            "Epoch 295 -\n",
            "Updated K params after epoch 295: [-0.19148388 -0.91029424]\n",
            "Train loss: 0.40070757469101004, Test loss: 0.42453532068426386\n",
            "\n",
            "\n",
            "Epoch 296 -\n",
            "Updated K params after epoch 296: [-0.1914711 -0.9104902]\n",
            "Train loss: 0.39900480264104177, Test loss: 0.42285670496531447\n",
            "\n",
            "\n",
            "Epoch 297 -\n",
            "Updated K params after epoch 297: [-0.19145822 -0.91068691]\n",
            "Train loss: 0.3973028139526287, Test loss: 0.42117775608485414\n",
            "\n",
            "\n",
            "Epoch 298 -\n",
            "Updated K params after epoch 298: [-0.19144523 -0.91088438]\n",
            "Train loss: 0.39560173537779675, Test loss: 0.4194986182484231\n",
            "\n",
            "\n",
            "Epoch 299 -\n",
            "Updated K params after epoch 299: [-0.19143214 -0.91108261]\n",
            "Train loss: 0.3939016964852423, Test loss: 0.4178194381952874\n",
            "\n",
            "\n",
            "Epoch 300 -\n",
            "Updated K params after epoch 300: [-0.19141894 -0.91128159]\n",
            "Train loss: 0.3922028294912297, Test loss: 0.41614036503276175\n",
            "\n",
            "\n",
            "Epoch 301 -\n",
            "Updated K params after epoch 301: [-0.19140564 -0.91148133]\n",
            "Train loss: 0.3905052690887822, Test loss: 0.41446155006910673\n",
            "\n",
            "\n",
            "Epoch 302 -\n",
            "Updated K params after epoch 302: [-0.19139224 -0.91168181]\n",
            "Train loss: 0.38880915227540935, Test loss: 0.41278314664522053\n",
            "\n",
            "\n",
            "Epoch 303 -\n",
            "Updated K params after epoch 303: [-0.19137874 -0.91188303]\n",
            "Train loss: 0.3871146181796129, Test loss: 0.41110530996534833\n",
            "\n",
            "\n",
            "Epoch 304 -\n",
            "Updated K params after epoch 304: [-0.19136514 -0.91208499]\n",
            "Train loss: 0.3854218078864182, Test loss: 0.40942819692703636\n",
            "\n",
            "\n",
            "Epoch 305 -\n",
            "Updated K params after epoch 305: [-0.19135144 -0.91228769]\n",
            "Train loss: 0.38373086426217984, Test loss: 0.40775196595055935\n",
            "\n",
            "\n",
            "Epoch 306 -\n",
            "Updated K params after epoch 306: [-0.19133764 -0.91249112]\n",
            "Train loss: 0.38204193177891127, Test loss: 0.4060767768080553\n",
            "\n",
            "\n",
            "Epoch 307 -\n",
            "Updated K params after epoch 307: [-0.19132375 -0.91269527]\n",
            "Train loss: 0.38035515633839, Test loss: 0.4044027904526006\n",
            "\n",
            "\n",
            "Epoch 308 -\n",
            "Updated K params after epoch 308: [-0.19130976 -0.91290015]\n",
            "Train loss: 0.37867068509629237, Test loss: 0.4027301688474622\n",
            "\n",
            "\n",
            "Epoch 309 -\n",
            "Updated K params after epoch 309: [-0.19129567 -0.91310573]\n",
            "Train loss: 0.37698866628661154, Test loss: 0.40105907479576536\n",
            "\n",
            "\n",
            "Epoch 310 -\n",
            "Updated K params after epoch 310: [-0.19128149 -0.91331203]\n",
            "Train loss: 0.3753092490466129, Test loss: 0.3993896717708146\n",
            "\n",
            "\n",
            "Epoch 311 -\n",
            "Updated K params after epoch 311: [-0.19126722 -0.91351903]\n",
            "Train loss: 0.3736325832425816, Test loss: 0.3977221237473076\n",
            "\n",
            "\n",
            "Epoch 312 -\n",
            "Updated K params after epoch 312: [-0.19125286 -0.91372672]\n",
            "Train loss: 0.3719588192966167, Test loss: 0.39605659503368024\n",
            "\n",
            "\n",
            "Epoch 313 -\n",
            "Updated K params after epoch 313: [-0.1912384 -0.9139351]\n",
            "Train loss: 0.37028810801472256, Test loss: 0.3943932501058215\n",
            "\n",
            "\n",
            "Epoch 314 -\n",
            "Updated K params after epoch 314: [-0.19122385 -0.91414417]\n",
            "Train loss: 0.3686206004164501, Test loss: 0.39273225344239293\n",
            "\n",
            "\n",
            "Epoch 315 -\n",
            "Updated K params after epoch 315: [-0.19120922 -0.9143539 ]\n",
            "Train loss: 0.366956447566336, Test loss: 0.39107376936198907\n",
            "\n",
            "\n",
            "Epoch 316 -\n",
            "Updated K params after epoch 316: [-0.19119449 -0.9145643 ]\n",
            "Train loss: 0.36529580040738285, Test loss: 0.3894179618623678\n",
            "\n",
            "\n",
            "Epoch 317 -\n",
            "Updated K params after epoch 317: [-0.19117968 -0.91477536]\n",
            "Train loss: 0.363638809596825, Test loss: 0.38776499446198043\n",
            "\n",
            "\n",
            "Epoch 318 -\n",
            "Updated K params after epoch 318: [-0.19116478 -0.91498707]\n",
            "Train loss: 0.36198562534441386, Test loss: 0.3861150300440232\n",
            "\n",
            "\n",
            "Epoch 319 -\n",
            "Updated K params after epoch 319: [-0.19114979 -0.91519941]\n",
            "Train loss: 0.36033639725345656, Test loss: 0.38446823070323005\n",
            "\n",
            "\n",
            "Epoch 320 -\n",
            "Updated K params after epoch 320: [-0.19113472 -0.91541239]\n",
            "Train loss: 0.3586912741648314, Test loss: 0.38282475759561935\n",
            "\n",
            "\n",
            "Epoch 321 -\n",
            "Updated K params after epoch 321: [-0.19111957 -0.91562599]\n",
            "Train loss: 0.3570504040042017, Test loss: 0.3811847707914003\n",
            "\n",
            "\n",
            "Epoch 322 -\n",
            "Updated K params after epoch 322: [-0.19110434 -0.9158402 ]\n",
            "Train loss: 0.35541393363263873, Test loss: 0.3795484291312398\n",
            "\n",
            "\n",
            "Epoch 323 -\n",
            "Updated K params after epoch 323: [-0.19108902 -0.91605501]\n",
            "Train loss: 0.35378200870085746, Test loss: 0.37791589008608173\n",
            "\n",
            "\n",
            "Epoch 324 -\n",
            "Updated K params after epoch 324: [-0.19107362 -0.91627042]\n",
            "Train loss: 0.3521547735072617, Test loss: 0.3762873096207011\n",
            "\n",
            "\n",
            "Epoch 325 -\n",
            "Updated K params after epoch 325: [-0.19105814 -0.91648641]\n",
            "Train loss: 0.35053237085998423, Test loss: 0.3746628420611708\n",
            "\n",
            "\n",
            "Epoch 326 -\n",
            "Updated K params after epoch 326: [-0.19104259 -0.91670296]\n",
            "Train loss: 0.3489149419430991, Test loss: 0.37304263996640347\n",
            "\n",
            "\n",
            "Epoch 327 -\n",
            "Updated K params after epoch 327: [-0.19102696 -0.91692008]\n",
            "Train loss: 0.34730262618717234, Test loss: 0.37142685400392705\n",
            "\n",
            "\n",
            "Epoch 328 -\n",
            "Updated K params after epoch 328: [-0.19101125 -0.91713775]\n",
            "Train loss: 0.3456955611443074, Test loss: 0.36981563283003815\n",
            "\n",
            "\n",
            "Epoch 329 -\n",
            "Updated K params after epoch 329: [-0.19099546 -0.91735596]\n",
            "Train loss: 0.3440938823678283, Test loss: 0.36820912297446834\n",
            "\n",
            "\n",
            "Epoch 330 -\n",
            "Updated K params after epoch 330: [-0.1909796 -0.9175747]\n",
            "Train loss: 0.34249772329673606, Test loss: 0.3666074687296872\n",
            "\n",
            "\n",
            "Epoch 331 -\n",
            "Updated K params after epoch 331: [-0.19096367 -0.91779395]\n",
            "Train loss: 0.34090721514505673, Test loss: 0.3650108120449538\n",
            "\n",
            "\n",
            "Epoch 332 -\n",
            "Updated K params after epoch 332: [-0.19094766 -0.91801371]\n",
            "Train loss: 0.33932248679619126, Test loss: 0.3634192924252179\n",
            "\n",
            "\n",
            "Epoch 333 -\n",
            "Updated K params after epoch 333: [-0.19093159 -0.91823396]\n",
            "Train loss: 0.3377436647023636, Test loss: 0.3618330468349588\n",
            "\n",
            "\n",
            "Epoch 334 -\n",
            "Updated K params after epoch 334: [-0.19091544 -0.91845469]\n",
            "Train loss: 0.33617087278924923, Test loss: 0.36025220960703797\n",
            "\n",
            "\n",
            "Epoch 335 -\n",
            "Updated K params after epoch 335: [-0.19089922 -0.91867589]\n",
            "Train loss: 0.3346042323658558, Test loss: 0.35867691235662963\n",
            "\n",
            "\n",
            "Epoch 336 -\n",
            "Updated K params after epoch 336: [-0.19088294 -0.91889754]\n",
            "Train loss: 0.33304386203971287, Test loss: 0.357107283900282\n",
            "\n",
            "\n",
            "Epoch 337 -\n",
            "Updated K params after epoch 337: [-0.19086659 -0.91911964]\n",
            "Train loss: 0.3314898776374164, Test loss: 0.3555434501801468\n",
            "\n",
            "\n",
            "Epoch 338 -\n",
            "Updated K params after epoch 338: [-0.19085017 -0.91934217]\n",
            "Train loss: 0.3299423921305575, Test loss: 0.3539855341934047\n",
            "\n",
            "\n",
            "Epoch 339 -\n",
            "Updated K params after epoch 339: [-0.19083369 -0.91956512]\n",
            "Train loss: 0.32840151556705616, Test loss: 0.3524336559269029\n",
            "\n",
            "\n",
            "Epoch 340 -\n",
            "Updated K params after epoch 340: [-0.19081714 -0.91978848]\n",
            "Train loss: 0.32686735500790537, Test loss: 0.35088793229700427\n",
            "\n",
            "\n",
            "Epoch 341 -\n",
            "Updated K params after epoch 341: [-0.19080053 -0.92001223]\n",
            "Train loss: 0.32534001446931754, Test loss: 0.3493484770946427\n",
            "\n",
            "\n",
            "Epoch 342 -\n",
            "Updated K params after epoch 342: [-0.19078386 -0.92023636]\n",
            "Train loss: 0.3238195948702563, Test loss: 0.34781540093556035\n",
            "\n",
            "\n",
            "Epoch 343 -\n",
            "Updated K params after epoch 343: [-0.19076713 -0.92046086]\n",
            "Train loss: 0.32230619398532034, Test loss: 0.3462888112156963\n",
            "\n",
            "\n",
            "Epoch 344 -\n",
            "Updated K params after epoch 344: [-0.19075034 -0.92068572]\n",
            "Train loss: 0.3207999064029379, Test loss: 0.3447688120716829\n",
            "\n",
            "\n",
            "Epoch 345 -\n",
            "Updated K params after epoch 345: [-0.19073349 -0.92091091]\n",
            "Train loss: 0.31930082348881644, Test loss: 0.343255504346395\n",
            "\n",
            "\n",
            "Epoch 346 -\n",
            "Updated K params after epoch 346: [-0.19071658 -0.92113644]\n",
            "Train loss: 0.31780903335458227, Test loss: 0.3417489855594881\n",
            "\n",
            "\n",
            "Epoch 347 -\n",
            "Updated K params after epoch 347: [-0.19069961 -0.92136228]\n",
            "Train loss: 0.31632462083153484, Test loss: 0.3402493498828515\n",
            "\n",
            "\n",
            "Epoch 348 -\n",
            "Updated K params after epoch 348: [-0.19068259 -0.92158843]\n",
            "Train loss: 0.31484766744942844, Test loss: 0.33875668812089116\n",
            "\n",
            "\n",
            "Epoch 349 -\n",
            "Updated K params after epoch 349: [-0.19066552 -0.92181486]\n",
            "Train loss: 0.3133782514201862, Test loss: 0.3372710876955509\n",
            "\n",
            "\n",
            "Epoch 350 -\n",
            "Updated K params after epoch 350: [-0.19064839 -0.92204157]\n",
            "Train loss: 0.3119164476264419, Test loss: 0.3357926326359709\n",
            "\n",
            "\n",
            "Epoch 351 -\n",
            "Updated K params after epoch 351: [-0.19063121 -0.92226855]\n",
            "Train loss: 0.3104623276147961, Test loss: 0.33432140357267254\n",
            "\n",
            "\n",
            "Epoch 352 -\n",
            "Updated K params after epoch 352: [-0.19061397 -0.92249577]\n",
            "Train loss: 0.30901595959366507, Test loss: 0.33285747773615526\n",
            "\n",
            "\n",
            "Epoch 353 -\n",
            "Updated K params after epoch 353: [-0.19059669 -0.92272323]\n",
            "Train loss: 0.30757740843559517, Test loss: 0.33140092895978174\n",
            "\n",
            "\n",
            "Epoch 354 -\n",
            "Updated K params after epoch 354: [-0.19057936 -0.92295092]\n",
            "Train loss: 0.30614673568390643, Test loss: 0.3299518276868202\n",
            "\n",
            "\n",
            "Epoch 355 -\n",
            "Updated K params after epoch 355: [-0.19056198 -0.92317882]\n",
            "Train loss: 0.3047239995635242, Test loss: 0.3285102409815118\n",
            "\n",
            "\n",
            "Epoch 356 -\n",
            "Updated K params after epoch 356: [-0.19054455 -0.92340691]\n",
            "Train loss: 0.3033092549958534, Test loss: 0.32707623254402113\n",
            "\n",
            "\n",
            "Epoch 357 -\n",
            "Updated K params after epoch 357: [-0.19052708 -0.92363519]\n",
            "Train loss: 0.30190255361754237, Test loss: 0.3256498627291245\n",
            "\n",
            "\n",
            "Epoch 358 -\n",
            "Updated K params after epoch 358: [-0.19050956 -0.92386365]\n",
            "Train loss: 0.3005039438029814, Test loss: 0.3242311885684902\n",
            "\n",
            "\n",
            "Epoch 359 -\n",
            "Updated K params after epoch 359: [-0.190492   -0.92409226]\n",
            "Train loss: 0.299113470690376, Test loss: 0.3228202637963968\n",
            "\n",
            "\n",
            "Epoch 360 -\n",
            "Updated K params after epoch 360: [-0.19047439 -0.92432102]\n",
            "Train loss: 0.2977311762112338, Test loss: 0.3214171388787362\n",
            "\n",
            "\n",
            "Epoch 361 -\n",
            "Updated K params after epoch 361: [-0.19045674 -0.92454991]\n",
            "Train loss: 0.29635709912309804, Test loss: 0.3200218610451443\n",
            "\n",
            "\n",
            "Epoch 362 -\n",
            "Updated K params after epoch 362: [-0.19043905 -0.92477893]\n",
            "Train loss: 0.2949912750453646, Test loss: 0.31863447432410175\n",
            "\n",
            "\n",
            "Epoch 363 -\n",
            "Updated K params after epoch 363: [-0.19042133 -0.92500805]\n",
            "Train loss: 0.29363373649801083, Test loss: 0.3172550195808444\n",
            "\n",
            "\n",
            "Epoch 364 -\n",
            "Updated K params after epoch 364: [-0.19040356 -0.92523727]\n",
            "Train loss: 0.29228451294307145, Test loss: 0.31588353455792534\n",
            "\n",
            "\n",
            "Epoch 365 -\n",
            "Updated K params after epoch 365: [-0.19038575 -0.92546658]\n",
            "Train loss: 0.2909436308286897, Test loss: 0.3145200539182665\n",
            "\n",
            "\n",
            "Epoch 366 -\n",
            "Updated K params after epoch 366: [-0.19036791 -0.92569595]\n",
            "Train loss: 0.2896111136355787, Test loss: 0.31316460929054263\n",
            "\n",
            "\n",
            "Epoch 367 -\n",
            "Updated K params after epoch 367: [-0.19035004 -0.92592539]\n",
            "Train loss: 0.2882869819257233, Test loss: 0.31181722931673705\n",
            "\n",
            "\n",
            "Epoch 368 -\n",
            "Updated K params after epoch 368: [-0.19033212 -0.92615487]\n",
            "Train loss: 0.28697125339315716, Test loss: 0.3104779397017128\n",
            "\n",
            "\n",
            "Epoch 369 -\n",
            "Updated K params after epoch 369: [-0.19031418 -0.92638439]\n",
            "Train loss: 0.2856639429166519, Test loss: 0.30914676326464335\n",
            "\n",
            "\n",
            "Epoch 370 -\n",
            "Updated K params after epoch 370: [-0.1902962  -0.92661393]\n",
            "Train loss: 0.2843650626141552, Test loss: 0.3078237199921502\n",
            "\n",
            "\n",
            "Epoch 371 -\n",
            "Updated K params after epoch 371: [-0.19027819 -0.92684349]\n",
            "Train loss: 0.28307462189881805, Test loss: 0.30650882709299376\n",
            "\n",
            "\n",
            "Epoch 372 -\n",
            "Updated K params after epoch 372: [-0.19026015 -0.92707304]\n",
            "Train loss: 0.2817926275364569, Test loss: 0.3052020990541724\n",
            "\n",
            "\n",
            "Epoch 373 -\n",
            "Updated K params after epoch 373: [-0.19024208 -0.92730259]\n",
            "Train loss: 0.2805190837042947, Test loss: 0.30390354769828204\n",
            "\n",
            "\n",
            "Epoch 374 -\n",
            "Updated K params after epoch 374: [-0.19022398 -0.92753211]\n",
            "Train loss: 0.2792539920508337, Test loss: 0.3026131822419955\n",
            "\n",
            "\n",
            "Epoch 375 -\n",
            "Updated K params after epoch 375: [-0.19020586 -0.9277616 ]\n",
            "Train loss: 0.27799735175671264, Test loss: 0.3013310093555224\n",
            "\n",
            "\n",
            "Epoch 376 -\n",
            "Updated K params after epoch 376: [-0.1901877  -0.92799104]\n",
            "Train loss: 0.2767491595964074, Test loss: 0.3000570332229169\n",
            "\n",
            "\n",
            "Epoch 377 -\n",
            "Updated K params after epoch 377: [-0.19016952 -0.92822043]\n",
            "Train loss: 0.2755094100006384, Test loss: 0.2987912556031003\n",
            "\n",
            "\n",
            "Epoch 378 -\n",
            "Updated K params after epoch 378: [-0.19015132 -0.92844975]\n",
            "Train loss: 0.2742780951193505, Test loss: 0.29753367589147495\n",
            "\n",
            "\n",
            "Epoch 379 -\n",
            "Updated K params after epoch 379: [-0.19013309 -0.928679  ]\n",
            "Train loss: 0.2730552048851389, Test loss: 0.2962842911820055\n",
            "\n",
            "\n",
            "Epoch 380 -\n",
            "Updated K params after epoch 380: [-0.19011484 -0.92890816]\n",
            "Train loss: 0.2718407270769969, Test loss: 0.29504309632965176\n",
            "\n",
            "\n",
            "Epoch 381 -\n",
            "Updated K params after epoch 381: [-0.19009657 -0.92913722]\n",
            "Train loss: 0.2706346473842674, Test loss: 0.29381008401303926\n",
            "\n",
            "\n",
            "Epoch 382 -\n",
            "Updated K params after epoch 382: [-0.19007828 -0.92936617]\n",
            "Train loss: 0.26943694947068575, Test loss: 0.2925852447972606\n",
            "\n",
            "\n",
            "Epoch 383 -\n",
            "Updated K params after epoch 383: [-0.19005997 -0.92959501]\n",
            "Train loss: 0.2682476150384038, Test loss: 0.291368567196703\n",
            "\n",
            "\n",
            "Epoch 384 -\n",
            "Updated K params after epoch 384: [-0.19004164 -0.92982372]\n",
            "Train loss: 0.26706662389189495, Test loss: 0.29016003773780547\n",
            "\n",
            "\n",
            "Epoch 385 -\n",
            "Updated K params after epoch 385: [-0.19002329 -0.93005229]\n",
            "Train loss: 0.26589395400164034, Test loss: 0.28895964102164917\n",
            "\n",
            "\n",
            "Epoch 386 -\n",
            "Updated K params after epoch 386: [-0.19000493 -0.93028072]\n",
            "Train loss: 0.2647295815675037, Test loss: 0.28776735978629575\n",
            "\n",
            "\n",
            "Epoch 387 -\n",
            "Updated K params after epoch 387: [-0.18998654 -0.93050899]\n",
            "Train loss: 0.26357348108170936, Test loss: 0.2865831749687864\n",
            "\n",
            "\n",
            "Epoch 388 -\n",
            "Updated K params after epoch 388: [-0.18996815 -0.9307371 ]\n",
            "Train loss: 0.2624256253913386, Test loss: 0.28540706576672376\n",
            "\n",
            "\n",
            "Epoch 389 -\n",
            "Updated K params after epoch 389: [-0.18994974 -0.93096503]\n",
            "Train loss: 0.26128598576026923, Test loss: 0.2842390096993635\n",
            "\n",
            "\n",
            "Epoch 390 -\n",
            "Updated K params after epoch 390: [-0.18993131 -0.93119277]\n",
            "Train loss: 0.26015453193048443, Test loss: 0.28307898266814285\n",
            "\n",
            "\n",
            "Epoch 391 -\n",
            "Updated K params after epoch 391: [-0.18991288 -0.93142033]\n",
            "Train loss: 0.2590312321826864, Test loss: 0.28192695901658366\n",
            "\n",
            "\n",
            "Epoch 392 -\n",
            "Updated K params after epoch 392: [-0.18989443 -0.93164769]\n",
            "Train loss: 0.25791605339614915, Test loss: 0.28078291158950813\n",
            "\n",
            "\n",
            "Epoch 393 -\n",
            "Updated K params after epoch 393: [-0.18987597 -0.93187484]\n",
            "Train loss: 0.25680896110775486, Test loss: 0.2796468117915113\n",
            "\n",
            "\n",
            "Epoch 394 -\n",
            "Updated K params after epoch 394: [-0.18985751 -0.93210177]\n",
            "Train loss: 0.2557099195701623, Test loss: 0.2785186296446381\n",
            "\n",
            "\n",
            "Epoch 395 -\n",
            "Updated K params after epoch 395: [-0.18983903 -0.93232847]\n",
            "Train loss: 0.25461889180905484, Test loss: 0.2773983338452181\n",
            "\n",
            "\n",
            "Epoch 396 -\n",
            "Updated K params after epoch 396: [-0.18982055 -0.93255495]\n",
            "Train loss: 0.2535358396794288, Test loss: 0.27628589181981444\n",
            "\n",
            "\n",
            "Epoch 397 -\n",
            "Updated K params after epoch 397: [-0.18980206 -0.93278118]\n",
            "Train loss: 0.252460723920879, Test loss: 0.27518126978024604\n",
            "\n",
            "\n",
            "Epoch 398 -\n",
            "Updated K params after epoch 398: [-0.18978356 -0.93300717]\n",
            "Train loss: 0.2513935042118484, Test loss: 0.27408443277765043\n",
            "\n",
            "\n",
            "Epoch 399 -\n",
            "Updated K params after epoch 399: [-0.18976506 -0.9332329 ]\n",
            "Train loss: 0.2503341392228084, Test loss: 0.27299534475555226\n",
            "\n",
            "\n",
            "Epoch 400 -\n",
            "Updated K params after epoch 400: [-0.18974655 -0.93345836]\n",
            "Train loss: 0.24928258666834346, Test loss: 0.2719139686019113\n",
            "\n",
            "\n",
            "Epoch 401 -\n",
            "Updated K params after epoch 401: [-0.18972804 -0.93368356]\n",
            "Train loss: 0.24823880335811593, Test loss: 0.2708402662001246\n",
            "\n",
            "\n",
            "Epoch 402 -\n",
            "Updated K params after epoch 402: [-0.18970953 -0.93390848]\n",
            "Train loss: 0.2472027452466895, Test loss: 0.26977419847896067\n",
            "\n",
            "\n",
            "Epoch 403 -\n",
            "Updated K params after epoch 403: [-0.18969102 -0.93413312]\n",
            "Train loss: 0.2461743674821961, Test loss: 0.26871572546140915\n",
            "\n",
            "\n",
            "Epoch 404 -\n",
            "Updated K params after epoch 404: [-0.1896725  -0.93435747]\n",
            "Train loss: 0.24515362445383002, Test loss: 0.26766480631242867\n",
            "\n",
            "\n",
            "Epoch 405 -\n",
            "Updated K params after epoch 405: [-0.18965398 -0.93458152]\n",
            "Train loss: 0.24414046983816093, Test loss: 0.26662139938558216\n",
            "\n",
            "\n",
            "Epoch 406 -\n",
            "Updated K params after epoch 406: [-0.18963547 -0.93480527]\n",
            "Train loss: 0.243134856644256, Test loss: 0.2655854622685495\n",
            "\n",
            "\n",
            "Epoch 407 -\n",
            "Updated K params after epoch 407: [-0.18961696 -0.93502871]\n",
            "Train loss: 0.24213673725760637, Test loss: 0.2645569518275098\n",
            "\n",
            "\n",
            "Epoch 408 -\n",
            "Updated K params after epoch 408: [-0.18959845 -0.93525183]\n",
            "Train loss: 0.2411460634828567, Test loss: 0.26353582425039035\n",
            "\n",
            "\n",
            "Epoch 409 -\n",
            "Updated K params after epoch 409: [-0.18957994 -0.93547464]\n",
            "Train loss: 0.24016278658533488, Test loss: 0.26252203508897887\n",
            "\n",
            "\n",
            "Epoch 410 -\n",
            "Updated K params after epoch 410: [-0.18956143 -0.93569712]\n",
            "Train loss: 0.23918685733138784, Test loss: 0.26151553929989907\n",
            "\n",
            "\n",
            "Epoch 411 -\n",
            "Updated K params after epoch 411: [-0.18954293 -0.93591926]\n",
            "Train loss: 0.23821822602752324, Test loss: 0.26051629128445253\n",
            "\n",
            "\n",
            "Epoch 412 -\n",
            "Updated K params after epoch 412: [-0.18952444 -0.93614107]\n",
            "Train loss: 0.23725684255836793, Test loss: 0.2595242449273298\n",
            "\n",
            "\n",
            "Epoch 413 -\n",
            "Updated K params after epoch 413: [-0.18950595 -0.93636254]\n",
            "Train loss: 0.23630265642344714, Test loss: 0.25853935363419656\n",
            "\n",
            "\n",
            "Epoch 414 -\n",
            "Updated K params after epoch 414: [-0.18948747 -0.93658366]\n",
            "Train loss: 0.23535561677279743, Test loss: 0.2575615703681638\n",
            "\n",
            "\n",
            "Epoch 415 -\n",
            "Updated K params after epoch 415: [-0.189469   -0.93680443]\n",
            "Train loss: 0.2344156724414224, Test loss: 0.25659084768514856\n",
            "\n",
            "\n",
            "Epoch 416 -\n",
            "Updated K params after epoch 416: [-0.18945053 -0.93702483]\n",
            "Train loss: 0.23348277198260636, Test loss: 0.2556271377681372\n",
            "\n",
            "\n",
            "Epoch 417 -\n",
            "Updated K params after epoch 417: [-0.18943208 -0.93724488]\n",
            "Train loss: 0.23255686370009748, Test loss: 0.2546703924603634\n",
            "\n",
            "\n",
            "Epoch 418 -\n",
            "Updated K params after epoch 418: [-0.18941363 -0.93746456]\n",
            "Train loss: 0.23163789567917806, Test loss: 0.25372056329741155\n",
            "\n",
            "\n",
            "Epoch 419 -\n",
            "Updated K params after epoch 419: [-0.18939519 -0.93768387]\n",
            "Train loss: 0.23072581581663765, Test loss: 0.2527776015382621\n",
            "\n",
            "\n",
            "Epoch 420 -\n",
            "Updated K params after epoch 420: [-0.18937677 -0.9379028 ]\n",
            "Train loss: 0.22982057184966576, Test loss: 0.2518414581952925\n",
            "\n",
            "\n",
            "Epoch 421 -\n",
            "Updated K params after epoch 421: [-0.18935836 -0.93812136]\n",
            "Train loss: 0.22892211138368393, Test loss: 0.2509120840632491\n",
            "\n",
            "\n",
            "Epoch 422 -\n",
            "Updated K params after epoch 422: [-0.18933996 -0.93833953]\n",
            "Train loss: 0.22803038191913516, Test loss: 0.24998942974720842\n",
            "\n",
            "\n",
            "Epoch 423 -\n",
            "Updated K params after epoch 423: [-0.18932157 -0.93855731]\n",
            "Train loss: 0.22714533087725114, Test loss: 0.2490734456895436\n",
            "\n",
            "\n",
            "Epoch 424 -\n",
            "Updated K params after epoch 424: [-0.1893032  -0.93877471]\n",
            "Train loss: 0.22626690562481724, Test loss: 0.24816408219591501\n",
            "\n",
            "\n",
            "Epoch 425 -\n",
            "Updated K params after epoch 425: [-0.18928484 -0.93899171]\n",
            "Train loss: 0.2253950534979571, Test loss: 0.24726128946030296\n",
            "\n",
            "\n",
            "Epoch 426 -\n",
            "Updated K params after epoch 426: [-0.1892665 -0.9392083]\n",
            "Train loss: 0.2245297218249569, Test loss: 0.24636501758910243\n",
            "\n",
            "\n",
            "Epoch 427 -\n",
            "Updated K params after epoch 427: [-0.18924817 -0.9394245 ]\n",
            "Train loss: 0.22367085794815234, Test loss: 0.24547521662429816\n",
            "\n",
            "\n",
            "Epoch 428 -\n",
            "Updated K params after epoch 428: [-0.18922986 -0.9396403 ]\n",
            "Train loss: 0.2228184092448996, Test loss: 0.244591836565741\n",
            "\n",
            "\n",
            "Epoch 429 -\n",
            "Updated K params after epoch 429: [-0.18921157 -0.93985568]\n",
            "Train loss: 0.2219723231476522, Test loss: 0.24371482739254477\n",
            "\n",
            "\n",
            "Epoch 430 -\n",
            "Updated K params after epoch 430: [-0.18919329 -0.94007065]\n",
            "Train loss: 0.2211325471631675, Test loss: 0.2428441390836247\n",
            "\n",
            "\n",
            "Epoch 431 -\n",
            "Updated K params after epoch 431: [-0.18917504 -0.94028521]\n",
            "Train loss: 0.22029902889086367, Test loss: 0.2419797216373967\n",
            "\n",
            "\n",
            "Epoch 432 -\n",
            "Updated K params after epoch 432: [-0.1891568  -0.94049936]\n",
            "Train loss: 0.21947171604035084, Test loss: 0.24112152509065957\n",
            "\n",
            "\n",
            "Epoch 433 -\n",
            "Updated K params after epoch 433: [-0.18913858 -0.94071308]\n",
            "Train loss: 0.21865055644815823, Test loss: 0.24026949953667956\n",
            "\n",
            "\n",
            "Epoch 434 -\n",
            "Updated K params after epoch 434: [-0.18912038 -0.94092638]\n",
            "Train loss: 0.21783549809368027, Test loss: 0.23942359514249803\n",
            "\n",
            "\n",
            "Epoch 435 -\n",
            "Updated K params after epoch 435: [-0.18910221 -0.94113926]\n",
            "Train loss: 0.21702648911436348, Test loss: 0.2385837621654833\n",
            "\n",
            "\n",
            "Epoch 436 -\n",
            "Updated K params after epoch 436: [-0.18908405 -0.9413517 ]\n",
            "Train loss: 0.2162234778201567, Test loss: 0.23774995096914686\n",
            "\n",
            "\n",
            "Epoch 437 -\n",
            "Updated K params after epoch 437: [-0.18906592 -0.94156372]\n",
            "Train loss: 0.21542641270724666, Test loss: 0.23692211203824437\n",
            "\n",
            "\n",
            "Epoch 438 -\n",
            "Updated K params after epoch 438: [-0.18904781 -0.94177531]\n",
            "Train loss: 0.21463524247110047, Test loss: 0.23610019599318138\n",
            "\n",
            "\n",
            "Epoch 439 -\n",
            "Updated K params after epoch 439: [-0.18902972 -0.94198646]\n",
            "Train loss: 0.21384991601883652, Test loss: 0.2352841536037443\n",
            "\n",
            "\n",
            "Epoch 440 -\n",
            "Updated K params after epoch 440: [-0.18901165 -0.94219718]\n",
            "Train loss: 0.21307038248094573, Test loss: 0.23447393580217696\n",
            "\n",
            "\n",
            "Epoch 441 -\n",
            "Updated K params after epoch 441: [-0.18899361 -0.94240746]\n",
            "Train loss: 0.2122965912223833, Test loss: 0.2336694936956202\n",
            "\n",
            "\n",
            "Epoch 442 -\n",
            "Updated K params after epoch 442: [-0.1889756 -0.9426173]\n",
            "Train loss: 0.211528491853052, Test loss: 0.2328707785779374\n",
            "\n",
            "\n",
            "Epoch 443 -\n",
            "Updated K params after epoch 443: [-0.18895761 -0.9428267 ]\n",
            "Train loss: 0.21076603423769755, Test loss: 0.23207774194094138\n",
            "\n",
            "\n",
            "Epoch 444 -\n",
            "Updated K params after epoch 444: [-0.18893964 -0.94303566]\n",
            "Train loss: 0.21000916850523546, Test loss: 0.23129033548504452\n",
            "\n",
            "\n",
            "Epoch 445 -\n",
            "Updated K params after epoch 445: [-0.18892171 -0.94324417]\n",
            "Train loss: 0.2092578450575297, Test loss: 0.2305085111293485\n",
            "\n",
            "\n",
            "Epoch 446 -\n",
            "Updated K params after epoch 446: [-0.18890379 -0.94345223]\n",
            "Train loss: 0.2085120145776417, Test loss: 0.22973222102119326\n",
            "\n",
            "\n",
            "Epoch 447 -\n",
            "Updated K params after epoch 447: [-0.18888591 -0.94365985]\n",
            "Train loss: 0.20777162803756896, Test loss: 0.22896141754518237\n",
            "\n",
            "\n",
            "Epoch 448 -\n",
            "Updated K params after epoch 448: [-0.18886805 -0.94386702]\n",
            "Train loss: 0.20703663670549136, Test loss: 0.22819605333170248\n",
            "\n",
            "\n",
            "Epoch 449 -\n",
            "Updated K params after epoch 449: [-0.18885023 -0.94407374]\n",
            "Train loss: 0.2063069921525435, Test loss: 0.22743608126495413\n",
            "\n",
            "\n",
            "Epoch 450 -\n",
            "Updated K params after epoch 450: [-0.18883243 -0.94428001]\n",
            "Train loss: 0.20558264625913047, Test loss: 0.22668145449051072\n",
            "\n",
            "\n",
            "Epoch 451 -\n",
            "Updated K params after epoch 451: [-0.18881466 -0.94448582]\n",
            "Train loss: 0.204863551220804, Test loss: 0.225932126422422\n",
            "\n",
            "\n",
            "Epoch 452 -\n",
            "Updated K params after epoch 452: [-0.18879692 -0.94469119]\n",
            "Train loss: 0.20414965955371622, Test loss: 0.2251880507498786\n",
            "\n",
            "\n",
            "Epoch 453 -\n",
            "Updated K params after epoch 453: [-0.18877921 -0.94489609]\n",
            "Train loss: 0.20344092409966683, Test loss: 0.22444918144345277\n",
            "\n",
            "\n",
            "Epoch 454 -\n",
            "Updated K params after epoch 454: [-0.18876153 -0.94510055]\n",
            "Train loss: 0.20273729803076, Test loss: 0.22371547276093068\n",
            "\n",
            "\n",
            "Epoch 455 -\n",
            "Updated K params after epoch 455: [-0.18874388 -0.94530454]\n",
            "Train loss: 0.20203873485368576, Test loss: 0.2229868792527522\n",
            "\n",
            "\n",
            "Epoch 456 -\n",
            "Updated K params after epoch 456: [-0.18872626 -0.94550808]\n",
            "Train loss: 0.20134518841364205, Test loss: 0.2222633557670714\n",
            "\n",
            "\n",
            "Epoch 457 -\n",
            "Updated K params after epoch 457: [-0.18870867 -0.94571117]\n",
            "Train loss: 0.2006566128979106, Test loss: 0.22154485745445313\n",
            "\n",
            "\n",
            "Epoch 458 -\n",
            "Updated K params after epoch 458: [-0.18869112 -0.94591379]\n",
            "Train loss: 0.19997296283910188, Test loss: 0.22083133977221867\n",
            "\n",
            "\n",
            "Epoch 459 -\n",
            "Updated K params after epoch 459: [-0.1886736  -0.94611596]\n",
            "Train loss: 0.1992941931180825, Test loss: 0.22012275848845467\n",
            "\n",
            "\n",
            "Epoch 460 -\n",
            "Updated K params after epoch 460: [-0.18865611 -0.94631767]\n",
            "Train loss: 0.1986202589665986, Test loss: 0.21941906968569794\n",
            "\n",
            "\n",
            "Epoch 461 -\n",
            "Updated K params after epoch 461: [-0.18863865 -0.94651891]\n",
            "Train loss: 0.19795111596960718, Test loss: 0.2187202297643091\n",
            "\n",
            "\n",
            "Epoch 462 -\n",
            "Updated K params after epoch 462: [-0.18862123 -0.9467197 ]\n",
            "Train loss: 0.1972867200673297, Test loss: 0.21802619544554708\n",
            "\n",
            "\n",
            "Epoch 463 -\n",
            "Updated K params after epoch 463: [-0.18860385 -0.94692003]\n",
            "Train loss: 0.19662702755703843, Test loss: 0.21733692377435732\n",
            "\n",
            "\n",
            "Epoch 464 -\n",
            "Updated K params after epoch 464: [-0.18858649 -0.9471199 ]\n",
            "Train loss: 0.1959719950945878, Test loss: 0.2166523721218842\n",
            "\n",
            "\n",
            "Epoch 465 -\n",
            "Updated K params after epoch 465: [-0.18856918 -0.94731931]\n",
            "Train loss: 0.19532157969570282, Test loss: 0.21597249818772\n",
            "\n",
            "\n",
            "Epoch 466 -\n",
            "Updated K params after epoch 466: [-0.18855189 -0.94751825]\n",
            "Train loss: 0.1946757387370343, Test loss: 0.21529726000190055\n",
            "\n",
            "\n",
            "Epoch 467 -\n",
            "Updated K params after epoch 467: [-0.18853465 -0.94771674]\n",
            "Train loss: 0.1940344299569923, Test loss: 0.21462661592665896\n",
            "\n",
            "\n",
            "Epoch 468 -\n",
            "Updated K params after epoch 468: [-0.18851744 -0.94791477]\n",
            "Train loss: 0.1933976114563679, Test loss: 0.21396052465794685\n",
            "\n",
            "\n",
            "Epoch 469 -\n",
            "Updated K params after epoch 469: [-0.18850026 -0.94811233]\n",
            "Train loss: 0.1927652416987533, Test loss: 0.21329894522673387\n",
            "\n",
            "\n",
            "Epoch 470 -\n",
            "Updated K params after epoch 470: [-0.18848312 -0.94830943]\n",
            "Train loss: 0.19213727951076912, Test loss: 0.21264183700009456\n",
            "\n",
            "\n",
            "Epoch 471 -\n",
            "Updated K params after epoch 471: [-0.18846602 -0.94850608]\n",
            "Train loss: 0.1915136840821091, Test loss: 0.2119891596820923\n",
            "\n",
            "\n",
            "Epoch 472 -\n",
            "Updated K params after epoch 472: [-0.18844896 -0.94870226]\n",
            "Train loss: 0.19089441496541032, Test loss: 0.21134087331446855\n",
            "\n",
            "\n",
            "Epoch 473 -\n",
            "Updated K params after epoch 473: [-0.18843193 -0.94889798]\n",
            "Train loss: 0.19027943207595782, Test loss: 0.21069693827714753\n",
            "\n",
            "\n",
            "Epoch 474 -\n",
            "Updated K params after epoch 474: [-0.18841494 -0.94909324]\n",
            "Train loss: 0.18966869569123182, Test loss: 0.21005731528856317\n",
            "\n",
            "\n",
            "Epoch 475 -\n",
            "Updated K params after epoch 475: [-0.18839799 -0.94928805]\n",
            "Train loss: 0.18906216645030524, Test loss: 0.20942196540581756\n",
            "\n",
            "\n",
            "Epoch 476 -\n",
            "Updated K params after epoch 476: [-0.18838108 -0.94948239]\n",
            "Train loss: 0.18845980535309997, Test loss: 0.20879085002467848\n",
            "\n",
            "\n",
            "Epoch 477 -\n",
            "Updated K params after epoch 477: [-0.18836421 -0.94967627]\n",
            "Train loss: 0.18786157375950763, Test loss: 0.2081639308794229\n",
            "\n",
            "\n",
            "Epoch 478 -\n",
            "Updated K params after epoch 478: [-0.18834737 -0.9498697 ]\n",
            "Train loss: 0.1872674333883841, Test loss: 0.20754117004253506\n",
            "\n",
            "\n",
            "Epoch 479 -\n",
            "Updated K params after epoch 479: [-0.18833058 -0.95006267]\n",
            "Train loss: 0.18667734631642255, Test loss: 0.2069225299242651\n",
            "\n",
            "\n",
            "Epoch 480 -\n",
            "Updated K params after epoch 480: [-0.18831382 -0.95025517]\n",
            "Train loss: 0.18609127497691247, Test loss: 0.20630797327205527\n",
            "\n",
            "\n",
            "Epoch 481 -\n",
            "Updated K params after epoch 481: [-0.18829711 -0.95044723]\n",
            "Train loss: 0.18550918215839166, Test loss: 0.2056974631698408\n",
            "\n",
            "\n",
            "Epoch 482 -\n",
            "Updated K params after epoch 482: [-0.18828043 -0.95063882]\n",
            "Train loss: 0.18493103100319547, Test loss: 0.20509096303723104\n",
            "\n",
            "\n",
            "Epoch 483 -\n",
            "Updated K params after epoch 483: [-0.1882638  -0.95082996]\n",
            "Train loss: 0.18435678500591077, Test loss: 0.20448843662857727\n",
            "\n",
            "\n",
            "Epoch 484 -\n",
            "Updated K params after epoch 484: [-0.1882472  -0.95102064]\n",
            "Train loss: 0.18378640801173915, Test loss: 0.2038898480319332\n",
            "\n",
            "\n",
            "Epoch 485 -\n",
            "Updated K params after epoch 485: [-0.18823065 -0.95121087]\n",
            "Train loss: 0.1832198642147752, Test loss: 0.20329516166791342\n",
            "\n",
            "\n",
            "Epoch 486 -\n",
            "Updated K params after epoch 486: [-0.18821414 -0.95140064]\n",
            "Train loss: 0.18265711815620517, Test loss: 0.20270434228845588\n",
            "\n",
            "\n",
            "Epoch 487 -\n",
            "Updated K params after epoch 487: [-0.18819767 -0.95158997]\n",
            "Train loss: 0.1820981347224299, Test loss: 0.20211735497549238\n",
            "\n",
            "\n",
            "Epoch 488 -\n",
            "Updated K params after epoch 488: [-0.18818124 -0.95177883]\n",
            "Train loss: 0.18154287914311798, Test loss: 0.2015341651395336\n",
            "\n",
            "\n",
            "Epoch 489 -\n",
            "Updated K params after epoch 489: [-0.18816485 -0.95196725]\n",
            "Train loss: 0.18099131698919327, Test loss: 0.2009547385181725\n",
            "\n",
            "\n",
            "Epoch 490 -\n",
            "Updated K params after epoch 490: [-0.1881485  -0.95215521]\n",
            "Train loss: 0.18044341417076018, Test loss: 0.20037904117451105\n",
            "\n",
            "\n",
            "Epoch 491 -\n",
            "Updated K params after epoch 491: [-0.1881322  -0.95234273]\n",
            "Train loss: 0.17989913693497264, Test loss: 0.19980703949551484\n",
            "\n",
            "\n",
            "Epoch 492 -\n",
            "Updated K params after epoch 492: [-0.18811594 -0.95252979]\n",
            "Train loss: 0.17935845186384886, Test loss: 0.1992387001902993\n",
            "\n",
            "\n",
            "Epoch 493 -\n",
            "Updated K params after epoch 493: [-0.18809972 -0.9527164 ]\n",
            "Train loss: 0.17882132587203706, Test loss: 0.19867399028835267\n",
            "\n",
            "\n",
            "Epoch 494 -\n",
            "Updated K params after epoch 494: [-0.18808354 -0.95290257]\n",
            "Train loss: 0.1782877262045348, Test loss: 0.19811287713769873\n",
            "\n",
            "\n",
            "Epoch 495 -\n",
            "Updated K params after epoch 495: [-0.18806741 -0.95308829]\n",
            "Train loss: 0.17775762043436674, Test loss: 0.19755532840300347\n",
            "\n",
            "\n",
            "Epoch 496 -\n",
            "Updated K params after epoch 496: [-0.18805132 -0.95327357]\n",
            "Train loss: 0.17723097646022196, Test loss: 0.19700131206362945\n",
            "\n",
            "\n",
            "Epoch 497 -\n",
            "Updated K params after epoch 497: [-0.18803527 -0.95345839]\n",
            "Train loss: 0.1767077625040566, Test loss: 0.19645079641164093\n",
            "\n",
            "\n",
            "Epoch 498 -\n",
            "Updated K params after epoch 498: [-0.18801927 -0.95364278]\n",
            "Train loss: 0.1761879471086628, Test loss: 0.1959037500497637\n",
            "\n",
            "\n",
            "Epoch 499 -\n",
            "Updated K params after epoch 499: [-0.18800331 -0.95382672]\n",
            "Train loss: 0.17567149913520796, Test loss: 0.19536014188930237\n",
            "\n",
            "\n",
            "Epoch 500 -\n",
            "Updated K params after epoch 500: [-0.18798739 -0.95401022]\n",
            "Train loss: 0.1751583877607466, Test loss: 0.19481994114801812\n",
            "\n",
            "\n",
            "Epoch 501 -\n",
            "Updated K params after epoch 501: [-0.18797152 -0.95419328]\n",
            "Train loss: 0.1746485824757076, Test loss: 0.19428311734797063\n",
            "\n",
            "\n",
            "Epoch 502 -\n",
            "Updated K params after epoch 502: [-0.18795569 -0.95437589]\n",
            "Train loss: 0.1741420530813597, Test loss: 0.1937496403133256\n",
            "\n",
            "\n",
            "Epoch 503 -\n",
            "Updated K params after epoch 503: [-0.18793991 -0.95455807]\n",
            "Train loss: 0.1736387696872567, Test loss: 0.19321948016813253\n",
            "\n",
            "\n",
            "Epoch 504 -\n",
            "Updated K params after epoch 504: [-0.18792417 -0.95473981]\n",
            "Train loss: 0.17313870270866585, Test loss: 0.1926926073340731\n",
            "\n",
            "\n",
            "Epoch 505 -\n",
            "Updated K params after epoch 505: [-0.18790848 -0.95492112]\n",
            "Train loss: 0.17264182286398086, Test loss: 0.19216899252818462\n",
            "\n",
            "\n",
            "Epoch 506 -\n",
            "Updated K params after epoch 506: [-0.18789283 -0.95510199]\n",
            "Train loss: 0.1721481011721216, Test loss: 0.19164860676055953\n",
            "\n",
            "\n",
            "Epoch 507 -\n",
            "Updated K params after epoch 507: [-0.18787722 -0.95528242]\n",
            "Train loss: 0.17165750894992282, Test loss: 0.19113142133202418\n",
            "\n",
            "\n",
            "Epoch 508 -\n",
            "Updated K params after epoch 508: [-0.18786166 -0.95546242]\n",
            "Train loss: 0.17117001780951382, Test loss: 0.1906174078317984\n",
            "\n",
            "\n",
            "Epoch 509 -\n",
            "Updated K params after epoch 509: [-0.18784615 -0.95564199]\n",
            "Train loss: 0.17068559965569027, Test loss: 0.19010653813513784\n",
            "\n",
            "\n",
            "Epoch 510 -\n",
            "Updated K params after epoch 510: [-0.18783068 -0.95582112]\n",
            "Train loss: 0.17020422668328053, Test loss: 0.1895987844009621\n",
            "\n",
            "\n",
            "Epoch 511 -\n",
            "Updated K params after epoch 511: [-0.18781525 -0.95599983]\n",
            "Train loss: 0.16972587137450793, Test loss: 0.18909411906946916\n",
            "\n",
            "\n",
            "Epoch 512 -\n",
            "Updated K params after epoch 512: [-0.18779988 -0.95617811]\n",
            "Train loss: 0.16925050649635026, Test loss: 0.18859251485973916\n",
            "\n",
            "\n",
            "Epoch 513 -\n",
            "Updated K params after epoch 513: [-0.18778454 -0.95635596]\n",
            "Train loss: 0.16877810509789823, Test loss: 0.18809394476732774\n",
            "\n",
            "\n",
            "Epoch 514 -\n",
            "Updated K params after epoch 514: [-0.18776926 -0.95653338]\n",
            "Train loss: 0.16830864050771435, Test loss: 0.18759838206185267\n",
            "\n",
            "\n",
            "Epoch 515 -\n",
            "Updated K params after epoch 515: [-0.18775402 -0.95671038]\n",
            "Train loss: 0.16784208633119307, Test loss: 0.18710580028457302\n",
            "\n",
            "\n",
            "Epoch 516 -\n",
            "Updated K params after epoch 516: [-0.18773882 -0.95688695]\n",
            "Train loss: 0.1673784164479242, Test loss: 0.18661617324596452\n",
            "\n",
            "\n",
            "Epoch 517 -\n",
            "Updated K params after epoch 517: [-0.18772367 -0.95706311]\n",
            "Train loss: 0.16691760500906017, Test loss: 0.1861294750232907\n",
            "\n",
            "\n",
            "Epoch 518 -\n",
            "Updated K params after epoch 518: [-0.18770857 -0.95723884]\n",
            "Train loss: 0.16645962643468812, Test loss: 0.18564567995817216\n",
            "\n",
            "\n",
            "Epoch 519 -\n",
            "Updated K params after epoch 519: [-0.18769351 -0.95741415]\n",
            "Train loss: 0.16600445541120884, Test loss: 0.18516476265415557\n",
            "\n",
            "\n",
            "Epoch 520 -\n",
            "Updated K params after epoch 520: [-0.1876785  -0.95758904]\n",
            "Train loss: 0.16555206688872245, Test loss: 0.18468669797428228\n",
            "\n",
            "\n",
            "Epoch 521 -\n",
            "Updated K params after epoch 521: [-0.18766354 -0.95776351]\n",
            "Train loss: 0.16510243607842248, Test loss: 0.1842114610386593\n",
            "\n",
            "\n",
            "Epoch 522 -\n",
            "Updated K params after epoch 522: [-0.18764862 -0.95793757]\n",
            "Train loss: 0.16465553844999872, Test loss: 0.18373902722203245\n",
            "\n",
            "\n",
            "Epoch 523 -\n",
            "Updated K params after epoch 523: [-0.18763375 -0.95811122]\n",
            "Train loss: 0.16421134972905055, Test loss: 0.18326937215136338\n",
            "\n",
            "\n",
            "Epoch 524 -\n",
            "Updated K params after epoch 524: [-0.18761892 -0.95828445]\n",
            "Train loss: 0.16376984589451024, Test loss: 0.18280247170341163\n",
            "\n",
            "\n",
            "Epoch 525 -\n",
            "Updated K params after epoch 525: [-0.18760414 -0.95845726]\n",
            "Train loss: 0.16333100317607824, Test loss: 0.18233830200232187\n",
            "\n",
            "\n",
            "Epoch 526 -\n",
            "Updated K params after epoch 526: [-0.18758941 -0.95862967]\n",
            "Train loss: 0.16289479805167045, Test loss: 0.18187683941721833\n",
            "\n",
            "\n",
            "Epoch 527 -\n",
            "Updated K params after epoch 527: [-0.18757473 -0.95880167]\n",
            "Train loss: 0.16246120724487836, Test loss: 0.18141806055980594\n",
            "\n",
            "\n",
            "Epoch 528 -\n",
            "Updated K params after epoch 528: [-0.18756009 -0.95897326]\n",
            "Train loss: 0.16203020772244242, Test loss: 0.18096194228198034\n",
            "\n",
            "\n",
            "Epoch 529 -\n",
            "Updated K params after epoch 529: [-0.1875455  -0.95914444]\n",
            "Train loss: 0.16160177669173967, Test loss: 0.1805084616734464\n",
            "\n",
            "\n",
            "Epoch 530 -\n",
            "Updated K params after epoch 530: [-0.18753096 -0.95931521]\n",
            "Train loss: 0.16117589159828585, Test loss: 0.1800575960593468\n",
            "\n",
            "\n",
            "Epoch 531 -\n",
            "Updated K params after epoch 531: [-0.18751646 -0.95948559]\n",
            "Train loss: 0.16075253012325258, Test loss: 0.17960932299790044\n",
            "\n",
            "\n",
            "Epoch 532 -\n",
            "Updated K params after epoch 532: [-0.18750201 -0.95965556]\n",
            "Train loss: 0.16033167018099997, Test loss: 0.17916362027805277\n",
            "\n",
            "\n",
            "Epoch 533 -\n",
            "Updated K params after epoch 533: [-0.18748761 -0.95982512]\n",
            "Train loss: 0.1599132899166255, Test loss: 0.17872046591713642\n",
            "\n",
            "\n",
            "Epoch 534 -\n",
            "Updated K params after epoch 534: [-0.18747325 -0.95999429]\n",
            "Train loss: 0.15949736770352907, Test loss: 0.17827983815854526\n",
            "\n",
            "\n",
            "Epoch 535 -\n",
            "Updated K params after epoch 535: [-0.18745894 -0.96016306]\n",
            "Train loss: 0.15908388214099467, Test loss: 0.17784171546942024\n",
            "\n",
            "\n",
            "Epoch 536 -\n",
            "Updated K params after epoch 536: [-0.18744468 -0.96033143]\n",
            "Train loss: 0.15867281205178968, Test loss: 0.1774060765383489\n",
            "\n",
            "\n",
            "Epoch 537 -\n",
            "Updated K params after epoch 537: [-0.18743047 -0.96049941]\n",
            "Train loss: 0.15826413647978121, Test loss: 0.17697290027307824\n",
            "\n",
            "\n",
            "Epoch 538 -\n",
            "Updated K params after epoch 538: [-0.1874163  -0.96066699]\n",
            "Train loss: 0.15785783468757028, Test loss: 0.17654216579824167\n",
            "\n",
            "\n",
            "Epoch 539 -\n",
            "Updated K params after epoch 539: [-0.18740218 -0.96083418]\n",
            "Train loss: 0.1574538861541442, Test loss: 0.17611385245310096\n",
            "\n",
            "\n",
            "Epoch 540 -\n",
            "Updated K params after epoch 540: [-0.18738811 -0.96100097]\n",
            "Train loss: 0.15705227057254698, Test loss: 0.17568793978930197\n",
            "\n",
            "\n",
            "Epoch 541 -\n",
            "Updated K params after epoch 541: [-0.18737408 -0.96116738]\n",
            "Train loss: 0.15665296784756838, Test loss: 0.17526440756864672\n",
            "\n",
            "\n",
            "Epoch 542 -\n",
            "Updated K params after epoch 542: [-0.18736011 -0.96133339]\n",
            "Train loss: 0.1562559580934518, Test loss: 0.17484323576088023\n",
            "\n",
            "\n",
            "Epoch 543 -\n",
            "Updated K params after epoch 543: [-0.18734618 -0.96149902]\n",
            "Train loss: 0.15586122163162078, Test loss: 0.1744244045414936\n",
            "\n",
            "\n",
            "Epoch 544 -\n",
            "Updated K params after epoch 544: [-0.1873323  -0.96166427]\n",
            "Train loss: 0.15546873898842484, Test loss: 0.17400789428954283\n",
            "\n",
            "\n",
            "Epoch 545 -\n",
            "Updated K params after epoch 545: [-0.18731846 -0.96182912]\n",
            "Train loss: 0.15507849089290449, Test loss: 0.17359368558548438\n",
            "\n",
            "\n",
            "Epoch 546 -\n",
            "Updated K params after epoch 546: [-0.18730467 -0.9619936 ]\n",
            "Train loss: 0.1546904582745754, Test loss: 0.173181759209027\n",
            "\n",
            "\n",
            "Epoch 547 -\n",
            "Updated K params after epoch 547: [-0.18729093 -0.96215769]\n",
            "Train loss: 0.1543046222612322, Test loss: 0.17277209613700079\n",
            "\n",
            "\n",
            "Epoch 548 -\n",
            "Updated K params after epoch 548: [-0.18727724 -0.9623214 ]\n",
            "Train loss: 0.15392096417677198, Test loss: 0.17236467754124263\n",
            "\n",
            "\n",
            "Epoch 549 -\n",
            "Updated K params after epoch 549: [-0.1872636  -0.96248473]\n",
            "Train loss: 0.15353946553903705, Test loss: 0.17195948478649933\n",
            "\n",
            "\n",
            "Epoch 550 -\n",
            "Updated K params after epoch 550: [-0.18725    -0.96264769]\n",
            "Train loss: 0.15316010805767777, Test loss: 0.17155649942834802\n",
            "\n",
            "\n",
            "Epoch 551 -\n",
            "Updated K params after epoch 551: [-0.18723645 -0.96281027]\n",
            "Train loss: 0.15278287363203516, Test loss: 0.17115570321113385\n",
            "\n",
            "\n",
            "Epoch 552 -\n",
            "Updated K params after epoch 552: [-0.18722294 -0.96297247]\n",
            "Train loss: 0.15240774434904278, Test loss: 0.17075707806592538\n",
            "\n",
            "\n",
            "Epoch 553 -\n",
            "Updated K params after epoch 553: [-0.18720949 -0.9631343 ]\n",
            "Train loss: 0.15203470248114928, Test loss: 0.17036060610848783\n",
            "\n",
            "\n",
            "Epoch 554 -\n",
            "Updated K params after epoch 554: [-0.18719608 -0.96329575]\n",
            "Train loss: 0.15166373048426013, Test loss: 0.16996626963727393\n",
            "\n",
            "\n",
            "Epoch 555 -\n",
            "Updated K params after epoch 555: [-0.18718272 -0.96345684]\n",
            "Train loss: 0.15129481099569955, Test loss: 0.1695740511314327\n",
            "\n",
            "\n",
            "Epoch 556 -\n",
            "Updated K params after epoch 556: [-0.18716941 -0.96361756]\n",
            "Train loss: 0.15092792683219225, Test loss: 0.16918393324883654\n",
            "\n",
            "\n",
            "Epoch 557 -\n",
            "Updated K params after epoch 557: [-0.18715614 -0.96377791]\n",
            "Train loss: 0.15056306098786498, Test loss: 0.1687958988241258\n",
            "\n",
            "\n",
            "Epoch 558 -\n",
            "Updated K params after epoch 558: [-0.18714292 -0.96393789]\n",
            "Train loss: 0.15020019663226772, Test loss: 0.1684099308667718\n",
            "\n",
            "\n",
            "Epoch 559 -\n",
            "Updated K params after epoch 559: [-0.18712975 -0.9640975 ]\n",
            "Train loss: 0.14983931710841505, Test loss: 0.168026012559158\n",
            "\n",
            "\n",
            "Epoch 560 -\n",
            "Updated K params after epoch 560: [-0.18711663 -0.96425676]\n",
            "Train loss: 0.14948040593084716, Test loss: 0.16764412725467884\n",
            "\n",
            "\n",
            "Epoch 561 -\n",
            "Updated K params after epoch 561: [-0.18710355 -0.96441565]\n",
            "Train loss: 0.1491234467837101, Test loss: 0.1672642584758574\n",
            "\n",
            "\n",
            "Epoch 562 -\n",
            "Updated K params after epoch 562: [-0.18709052 -0.96457418]\n",
            "Train loss: 0.14876842351885625, Test loss: 0.16688638991248056\n",
            "\n",
            "\n",
            "Epoch 563 -\n",
            "Updated K params after epoch 563: [-0.18707754 -0.96473234]\n",
            "Train loss: 0.14841532015396416, Test loss: 0.1665105054197527\n",
            "\n",
            "\n",
            "Epoch 564 -\n",
            "Updated K params after epoch 564: [-0.18706461 -0.96489016]\n",
            "Train loss: 0.1480641208706778, Test loss: 0.16613658901646738\n",
            "\n",
            "\n",
            "Epoch 565 -\n",
            "Updated K params after epoch 565: [-0.18705172 -0.96504761]\n",
            "Train loss: 0.14771481001276537, Test loss: 0.16576462488319715\n",
            "\n",
            "\n",
            "Epoch 566 -\n",
            "Updated K params after epoch 566: [-0.18703888 -0.96520471]\n",
            "Train loss: 0.14736737208429723, Test loss: 0.1653945973605014\n",
            "\n",
            "\n",
            "Epoch 567 -\n",
            "Updated K params after epoch 567: [-0.18702609 -0.96536145]\n",
            "Train loss: 0.14702179174784358, Test loss: 0.1650264909471522\n",
            "\n",
            "\n",
            "Epoch 568 -\n",
            "Updated K params after epoch 568: [-0.18701334 -0.96551784]\n",
            "Train loss: 0.14667805382269045, Test loss: 0.1646602902983781\n",
            "\n",
            "\n",
            "Epoch 569 -\n",
            "Updated K params after epoch 569: [-0.18700065 -0.96567388]\n",
            "Train loss: 0.1463361432830758, Test loss: 0.16429598022412614\n",
            "\n",
            "\n",
            "Epoch 570 -\n",
            "Updated K params after epoch 570: [-0.18698799 -0.96582957]\n",
            "Train loss: 0.14599604525644364, Test loss: 0.16393354568734128\n",
            "\n",
            "\n",
            "Epoch 571 -\n",
            "Updated K params after epoch 571: [-0.18697539 -0.96598491]\n",
            "Train loss: 0.14565774502171752, Test loss: 0.16357297180226388\n",
            "\n",
            "\n",
            "Epoch 572 -\n",
            "Updated K params after epoch 572: [-0.18696283 -0.96613991]\n",
            "Train loss: 0.14532122800759267, Test loss: 0.16321424383274494\n",
            "\n",
            "\n",
            "Epoch 573 -\n",
            "Updated K params after epoch 573: [-0.18695032 -0.96629456]\n",
            "Train loss: 0.14498647979084658, Test loss: 0.16285734719057882\n",
            "\n",
            "\n",
            "Epoch 574 -\n",
            "Updated K params after epoch 574: [-0.18693786 -0.96644886]\n",
            "Train loss: 0.14465348609466824, Test loss: 0.1625022674338537\n",
            "\n",
            "\n",
            "Epoch 575 -\n",
            "Updated K params after epoch 575: [-0.18692544 -0.96660282]\n",
            "Train loss: 0.14432223278700576, Test loss: 0.16214899026531937\n",
            "\n",
            "\n",
            "Epoch 576 -\n",
            "Updated K params after epoch 576: [-0.18691307 -0.96675644]\n",
            "Train loss: 0.143992705878932, Test loss: 0.16179750153077255\n",
            "\n",
            "\n",
            "Epoch 577 -\n",
            "Updated K params after epoch 577: [-0.18690075 -0.96690971]\n",
            "Train loss: 0.1436648915230286, Test loss: 0.16144778721745937\n",
            "\n",
            "\n",
            "Epoch 578 -\n",
            "Updated K params after epoch 578: [-0.18688847 -0.96706265]\n",
            "Train loss: 0.1433387760117879, Test loss: 0.161099833452495\n",
            "\n",
            "\n",
            "Epoch 579 -\n",
            "Updated K params after epoch 579: [-0.18687625 -0.96721525]\n",
            "Train loss: 0.14301434577603256, Test loss: 0.16075362650130076\n",
            "\n",
            "\n",
            "Epoch 580 -\n",
            "Updated K params after epoch 580: [-0.18686406 -0.96736752]\n",
            "Train loss: 0.1426915873833531, Test loss: 0.16040915276605752\n",
            "\n",
            "\n",
            "Epoch 581 -\n",
            "Updated K params after epoch 581: [-0.18685193 -0.96751945]\n",
            "Train loss: 0.14237048753656287, Test loss: 0.1600663987841768\n",
            "\n",
            "\n",
            "Epoch 582 -\n",
            "Updated K params after epoch 582: [-0.18683984 -0.96767104]\n",
            "Train loss: 0.14205103307217037, Test loss: 0.1597253512267878\n",
            "\n",
            "\n",
            "Epoch 583 -\n",
            "Updated K params after epoch 583: [-0.18682779 -0.9678223 ]\n",
            "Train loss: 0.14173321095886915, Test loss: 0.15938599689724162\n",
            "\n",
            "\n",
            "Epoch 584 -\n",
            "Updated K params after epoch 584: [-0.1868158  -0.96797323]\n",
            "Train loss: 0.14141700829604456, Test loss: 0.15904832272963196\n",
            "\n",
            "\n",
            "Epoch 585 -\n",
            "Updated K params after epoch 585: [-0.18680385 -0.96812384]\n",
            "Train loss: 0.14110241231229756, Test loss: 0.15871231578733191\n",
            "\n",
            "\n",
            "Epoch 586 -\n",
            "Updated K params after epoch 586: [-0.18679194 -0.96827411]\n",
            "Train loss: 0.1407894103639855, Test loss: 0.1583779632615475\n",
            "\n",
            "\n",
            "Epoch 587 -\n",
            "Updated K params after epoch 587: [-0.18678008 -0.96842405]\n",
            "Train loss: 0.14047798993377947, Test loss: 0.1580452524698866\n",
            "\n",
            "\n",
            "Epoch 588 -\n",
            "Updated K params after epoch 588: [-0.18676827 -0.96857367]\n",
            "Train loss: 0.14016813862923816, Test loss: 0.1577141708549451\n",
            "\n",
            "\n",
            "Epoch 589 -\n",
            "Updated K params after epoch 589: [-0.18675651 -0.96872297]\n",
            "Train loss: 0.1398598441813983, Test loss: 0.15738470598290763\n",
            "\n",
            "\n",
            "Epoch 590 -\n",
            "Updated K params after epoch 590: [-0.18674479 -0.96887194]\n",
            "Train loss: 0.13955309444338115, Test loss: 0.15705684554216492\n",
            "\n",
            "\n",
            "Epoch 591 -\n",
            "Updated K params after epoch 591: [-0.18673312 -0.96902059]\n",
            "Train loss: 0.13924787738901526, Test loss: 0.15673057734194665\n",
            "\n",
            "\n",
            "Epoch 592 -\n",
            "Updated K params after epoch 592: [-0.18672149 -0.96916892]\n",
            "Train loss: 0.13894418111147502, Test loss: 0.15640588931096952\n",
            "\n",
            "\n",
            "Epoch 593 -\n",
            "Updated K params after epoch 593: [-0.18670991 -0.96931693]\n",
            "Train loss: 0.13864199382193504, Test loss: 0.15608276949610098\n",
            "\n",
            "\n",
            "Epoch 594 -\n",
            "Updated K params after epoch 594: [-0.18669837 -0.96946462]\n",
            "Train loss: 0.13834130384824034, Test loss: 0.15576120606103797\n",
            "\n",
            "\n",
            "Epoch 595 -\n",
            "Updated K params after epoch 595: [-0.18668688 -0.969612  ]\n",
            "Train loss: 0.13804209963359163, Test loss: 0.1554411872850011\n",
            "\n",
            "\n",
            "Epoch 596 -\n",
            "Updated K params after epoch 596: [-0.18667544 -0.96975906]\n",
            "Train loss: 0.13774436973524642, Test loss: 0.15512270156144334\n",
            "\n",
            "\n",
            "Epoch 597 -\n",
            "Updated K params after epoch 597: [-0.18666404 -0.9699058 ]\n",
            "Train loss: 0.13744810282323497, Test loss: 0.15480573739677383\n",
            "\n",
            "\n",
            "Epoch 598 -\n",
            "Updated K params after epoch 598: [-0.18665269 -0.97005223]\n",
            "Train loss: 0.13715328767909138, Test loss: 0.1544902834090964\n",
            "\n",
            "\n",
            "Epoch 599 -\n",
            "Updated K params after epoch 599: [-0.18664138 -0.97019835]\n",
            "Train loss: 0.13685991319459942, Test loss: 0.1541763283269623\n",
            "\n",
            "\n",
            "Epoch 600 -\n",
            "Updated K params after epoch 600: [-0.18663012 -0.97034416]\n",
            "Train loss: 0.13656796837055335, Test loss: 0.15386386098813787\n",
            "\n",
            "\n",
            "Epoch 601 -\n",
            "Updated K params after epoch 601: [-0.1866189  -0.97048967]\n",
            "Train loss: 0.13627744231553318, Test loss: 0.15355287033838594\n",
            "\n",
            "\n",
            "Epoch 602 -\n",
            "Updated K params after epoch 602: [-0.18660773 -0.97063486]\n",
            "Train loss: 0.13598832424469404, Test loss: 0.15324334543026177\n",
            "\n",
            "\n",
            "Epoch 603 -\n",
            "Updated K params after epoch 603: [-0.1865966  -0.97077974]\n",
            "Train loss: 0.13570060347857058, Test loss: 0.15293527542192278\n",
            "\n",
            "\n",
            "Epoch 604 -\n",
            "Updated K params after epoch 604: [-0.18658552 -0.97092433]\n",
            "Train loss: 0.1354142694418945, Test loss: 0.152628649575952\n",
            "\n",
            "\n",
            "Epoch 605 -\n",
            "Updated K params after epoch 605: [-0.18657448 -0.9710686 ]\n",
            "Train loss: 0.13512931166242692, Test loss: 0.15232345725819577\n",
            "\n",
            "\n",
            "Epoch 606 -\n",
            "Updated K params after epoch 606: [-0.18656349 -0.97121258]\n",
            "Train loss: 0.13484571976980406, Test loss: 0.15201968793661416\n",
            "\n",
            "\n",
            "Epoch 607 -\n",
            "Updated K params after epoch 607: [-0.18655255 -0.97135625]\n",
            "Train loss: 0.1345634834943967, Test loss: 0.15171733118014571\n",
            "\n",
            "\n",
            "Epoch 608 -\n",
            "Updated K params after epoch 608: [-0.18654165 -0.97149962]\n",
            "Train loss: 0.13428259266618328, Test loss: 0.15141637665758487\n",
            "\n",
            "\n",
            "Epoch 609 -\n",
            "Updated K params after epoch 609: [-0.18653079 -0.97164269]\n",
            "Train loss: 0.1340030372136362, Test loss: 0.1511168141364728\n",
            "\n",
            "\n",
            "Epoch 610 -\n",
            "Updated K params after epoch 610: [-0.18651998 -0.97178546]\n",
            "Train loss: 0.13372480716262142, Test loss: 0.1508186334820011\n",
            "\n",
            "\n",
            "Epoch 611 -\n",
            "Updated K params after epoch 611: [-0.18650921 -0.97192794]\n",
            "Train loss: 0.13344789263531115, Test loss: 0.1505218246559288\n",
            "\n",
            "\n",
            "Epoch 612 -\n",
            "Updated K params after epoch 612: [-0.18649849 -0.97207012]\n",
            "Train loss: 0.1331722838491095, Test loss: 0.15022637771551137\n",
            "\n",
            "\n",
            "Epoch 613 -\n",
            "Updated K params after epoch 613: [-0.18648781 -0.97221201]\n",
            "Train loss: 0.1328979711155908, Test loss: 0.149932282812443\n",
            "\n",
            "\n",
            "Epoch 614 -\n",
            "Updated K params after epoch 614: [-0.18647717 -0.9723536 ]\n",
            "Train loss: 0.1326249448394507, Test loss: 0.1496395301918111\n",
            "\n",
            "\n",
            "Epoch 615 -\n",
            "Updated K params after epoch 615: [-0.18646658 -0.97249491]\n",
            "Train loss: 0.13235319551746966, Test loss: 0.14934811019106303\n",
            "\n",
            "\n",
            "Epoch 616 -\n",
            "Updated K params after epoch 616: [-0.18645604 -0.97263592]\n",
            "Train loss: 0.13208271373748887, Test loss: 0.1490580132389852\n",
            "\n",
            "\n",
            "Epoch 617 -\n",
            "Updated K params after epoch 617: [-0.18644554 -0.97277664]\n",
            "Train loss: 0.1318134901773983, Test loss: 0.14876922985469423\n",
            "\n",
            "\n",
            "Epoch 618 -\n",
            "Updated K params after epoch 618: [-0.18643508 -0.97291707]\n",
            "Train loss: 0.13154551560413708, Test loss: 0.14848175064664\n",
            "\n",
            "\n",
            "Epoch 619 -\n",
            "Updated K params after epoch 619: [-0.18642466 -0.97305722]\n",
            "Train loss: 0.13127878087270542, Test loss: 0.14819556631162017\n",
            "\n",
            "\n",
            "Epoch 620 -\n",
            "Updated K params after epoch 620: [-0.18641429 -0.97319708]\n",
            "Train loss: 0.1310132769251887, Test loss: 0.14791066763380717\n",
            "\n",
            "\n",
            "Epoch 621 -\n",
            "Updated K params after epoch 621: [-0.18640397 -0.97333665]\n",
            "Train loss: 0.1307489947897929, Test loss: 0.14762704548378597\n",
            "\n",
            "\n",
            "Epoch 622 -\n",
            "Updated K params after epoch 622: [-0.18639369 -0.97347594]\n",
            "Train loss: 0.13048592557989186, Test loss: 0.14734469081760362\n",
            "\n",
            "\n",
            "Epoch 623 -\n",
            "Updated K params after epoch 623: [-0.18638345 -0.97361495]\n",
            "Train loss: 0.13022406049308588, Test loss: 0.1470635946758298\n",
            "\n",
            "\n",
            "Epoch 624 -\n",
            "Updated K params after epoch 624: [-0.18637325 -0.97375367]\n",
            "Train loss: 0.12996339081027128, Test loss: 0.14678374818262896\n",
            "\n",
            "\n",
            "Epoch 625 -\n",
            "Updated K params after epoch 625: [-0.1863631  -0.97389212]\n",
            "Train loss: 0.1297039078947216, Test loss: 0.14650514254484312\n",
            "\n",
            "\n",
            "Epoch 626 -\n",
            "Updated K params after epoch 626: [-0.18635299 -0.97403029]\n",
            "Train loss: 0.12944560319117912, Test loss: 0.14622776905108556\n",
            "\n",
            "\n",
            "Epoch 627 -\n",
            "Updated K params after epoch 627: [-0.18634293 -0.97416818]\n",
            "Train loss: 0.12918846822495814, Test loss: 0.1459516190708456\n",
            "\n",
            "\n",
            "Epoch 628 -\n",
            "Updated K params after epoch 628: [-0.1863329  -0.97430579]\n",
            "Train loss: 0.1289324946010581, Test loss: 0.14567668405360362\n",
            "\n",
            "\n",
            "Epoch 629 -\n",
            "Updated K params after epoch 629: [-0.18632292 -0.97444312]\n",
            "Train loss: 0.1286776740032879, Test loss: 0.1454029555279569\n",
            "\n",
            "\n",
            "Epoch 630 -\n",
            "Updated K params after epoch 630: [-0.18631299 -0.97458018]\n",
            "Train loss: 0.12842399819340047, Test loss: 0.14513042510075574\n",
            "\n",
            "\n",
            "Epoch 631 -\n",
            "Updated K params after epoch 631: [-0.18630309 -0.97471697]\n",
            "Train loss: 0.12817145901023794, Test loss: 0.1448590844562499\n",
            "\n",
            "\n",
            "Epoch 632 -\n",
            "Updated K params after epoch 632: [-0.18629324 -0.97485349]\n",
            "Train loss: 0.12792004836888643, Test loss: 0.1445889253552451\n",
            "\n",
            "\n",
            "Epoch 633 -\n",
            "Updated K params after epoch 633: [-0.18628344 -0.97498973]\n",
            "Train loss: 0.12766975825984186, Test loss: 0.1443199396342697\n",
            "\n",
            "\n",
            "Epoch 634 -\n",
            "Updated K params after epoch 634: [-0.18627367 -0.9751257 ]\n",
            "Train loss: 0.12742058074818505, Test loss: 0.1440521192047512\n",
            "\n",
            "\n",
            "Epoch 635 -\n",
            "Updated K params after epoch 635: [-0.18626395 -0.97526141]\n",
            "Train loss: 0.1271725079727668, Test loss: 0.14378545605220258\n",
            "\n",
            "\n",
            "Epoch 636 -\n",
            "Updated K params after epoch 636: [-0.18625427 -0.97539684]\n",
            "Train loss: 0.12692553214540317, Test loss: 0.14351994223541828\n",
            "\n",
            "\n",
            "Epoch 637 -\n",
            "Updated K params after epoch 637: [-0.18624463 -0.97553201]\n",
            "Train loss: 0.1266796455500798, Test loss: 0.14325556988567986\n",
            "\n",
            "\n",
            "Epoch 638 -\n",
            "Updated K params after epoch 638: [-0.18623503 -0.97566692]\n",
            "Train loss: 0.12643484054216605, Test loss: 0.14299233120597094\n",
            "\n",
            "\n",
            "Epoch 639 -\n",
            "Updated K params after epoch 639: [-0.18622548 -0.97580156]\n",
            "Train loss: 0.12619110954763868, Test loss: 0.14273021847020173\n",
            "\n",
            "\n",
            "Epoch 640 -\n",
            "Updated K params after epoch 640: [-0.18621597 -0.97593593]\n",
            "Train loss: 0.12594844506231434, Test loss: 0.1424692240224426\n",
            "\n",
            "\n",
            "Epoch 641 -\n",
            "Updated K params after epoch 641: [-0.1862065  -0.97607004]\n",
            "Train loss: 0.12570683965109183, Test loss: 0.1422093402761666\n",
            "\n",
            "\n",
            "Epoch 642 -\n",
            "Updated K params after epoch 642: [-0.18619707 -0.97620389]\n",
            "Train loss: 0.12546628594720305, Test loss: 0.14195055971350162\n",
            "\n",
            "\n",
            "Epoch 643 -\n",
            "Updated K params after epoch 643: [-0.18618768 -0.97633748]\n",
            "Train loss: 0.12522677665147278, Test loss: 0.1416928748844907\n",
            "\n",
            "\n",
            "Epoch 644 -\n",
            "Updated K params after epoch 644: [-0.18617834 -0.97647081]\n",
            "Train loss: 0.1249883045315878, Test loss: 0.14143627840636158\n",
            "\n",
            "\n",
            "Epoch 645 -\n",
            "Updated K params after epoch 645: [-0.18616904 -0.97660389]\n",
            "Train loss: 0.12475086242137426, Test loss: 0.14118076296280482\n",
            "\n",
            "\n",
            "Epoch 646 -\n",
            "Updated K params after epoch 646: [-0.18615977 -0.9767367 ]\n",
            "Train loss: 0.12451444322008387, Test loss: 0.14092632130326066\n",
            "\n",
            "\n",
            "Epoch 647 -\n",
            "Updated K params after epoch 647: [-0.18615055 -0.97686926]\n",
            "Train loss: 0.12427903989168868, Test loss: 0.1406729462422139\n",
            "\n",
            "\n",
            "Epoch 648 -\n",
            "Updated K params after epoch 648: [-0.18614138 -0.97700156]\n",
            "Train loss: 0.1240446454641841, Test loss: 0.14042063065849789\n",
            "\n",
            "\n",
            "Epoch 649 -\n",
            "Updated K params after epoch 649: [-0.18613224 -0.97713361]\n",
            "Train loss: 0.1238112530289004, Test loss: 0.14016936749460593\n",
            "\n",
            "\n",
            "Epoch 650 -\n",
            "Updated K params after epoch 650: [-0.18612314 -0.97726541]\n",
            "Train loss: 0.12357885573982225, Test loss: 0.13991914975601152\n",
            "\n",
            "\n",
            "Epoch 651 -\n",
            "Updated K params after epoch 651: [-0.18611409 -0.97739695]\n",
            "Train loss: 0.12334744681291661, Test loss: 0.1396699705104964\n",
            "\n",
            "\n",
            "Epoch 652 -\n",
            "Updated K params after epoch 652: [-0.18610507 -0.97752824]\n",
            "Train loss: 0.12311701952546825, Test loss: 0.1394218228874867\n",
            "\n",
            "\n",
            "Epoch 653 -\n",
            "Updated K params after epoch 653: [-0.1860961  -0.97765928]\n",
            "Train loss: 0.12288756721542353, Test loss: 0.13917470007739657\n",
            "\n",
            "\n",
            "Epoch 654 -\n",
            "Updated K params after epoch 654: [-0.18608717 -0.97779008]\n",
            "Train loss: 0.122659083280742, Test loss: 0.13892859533098031\n",
            "\n",
            "\n",
            "Epoch 655 -\n",
            "Updated K params after epoch 655: [-0.18607827 -0.97792062]\n",
            "Train loss: 0.1224315611787552, Test loss: 0.13868350195869134\n",
            "\n",
            "\n",
            "Epoch 656 -\n",
            "Updated K params after epoch 656: [-0.18606942 -0.97805092]\n",
            "Train loss: 0.12220499442553373, Test loss: 0.1384394133300494\n",
            "\n",
            "\n",
            "Epoch 657 -\n",
            "Updated K params after epoch 657: [-0.18606061 -0.97818097]\n",
            "Train loss: 0.12197937659526159, Test loss: 0.13819632287301503\n",
            "\n",
            "\n",
            "Epoch 658 -\n",
            "Updated K params after epoch 658: [-0.18605184 -0.97831077]\n",
            "Train loss: 0.12175470131961762, Test loss: 0.13795422407337135\n",
            "\n",
            "\n",
            "Epoch 659 -\n",
            "Updated K params after epoch 659: [-0.18604311 -0.97844034]\n",
            "Train loss: 0.12153096228716477, Test loss: 0.13771311047411325\n",
            "\n",
            "\n",
            "Epoch 660 -\n",
            "Updated K params after epoch 660: [-0.18603442 -0.97856965]\n",
            "Train loss: 0.12130815324274653, Test loss: 0.13747297567484387\n",
            "\n",
            "\n",
            "Epoch 661 -\n",
            "Updated K params after epoch 661: [-0.18602577 -0.97869873]\n",
            "Train loss: 0.12108626798689008, Test loss: 0.13723381333117798\n",
            "\n",
            "\n",
            "Epoch 662 -\n",
            "Updated K params after epoch 662: [-0.18601716 -0.97882757]\n",
            "Train loss: 0.12086530037521695, Test loss: 0.13699561715415276\n",
            "\n",
            "\n",
            "Epoch 663 -\n",
            "Updated K params after epoch 663: [-0.18600858 -0.97895616]\n",
            "Train loss: 0.12064524431786071, Test loss: 0.13675838090964507\n",
            "\n",
            "\n",
            "Epoch 664 -\n",
            "Updated K params after epoch 664: [-0.18600005 -0.97908452]\n",
            "Train loss: 0.12042609377889102, Test loss: 0.13652209841779603\n",
            "\n",
            "\n",
            "Epoch 665 -\n",
            "Updated K params after epoch 665: [-0.18599156 -0.97921263]\n",
            "Train loss: 0.1202078427757451, Test loss: 0.13628676355244213\n",
            "\n",
            "\n",
            "Epoch 666 -\n",
            "Updated K params after epoch 666: [-0.18598311 -0.97934051]\n",
            "Train loss: 0.1199904853786654, Test loss: 0.1360523702405531\n",
            "\n",
            "\n",
            "Epoch 667 -\n",
            "Updated K params after epoch 667: [-0.1859747  -0.97946816]\n",
            "Train loss: 0.11977401571014436, Test loss: 0.13581891246167646\n",
            "\n",
            "\n",
            "Epoch 668 -\n",
            "Updated K params after epoch 668: [-0.18596632 -0.97959556]\n",
            "Train loss: 0.11955842794437534, Test loss: 0.13558638424738845\n",
            "\n",
            "\n",
            "Epoch 669 -\n",
            "Updated K params after epoch 669: [-0.18595799 -0.97972274]\n",
            "Train loss: 0.1193437163067102, Test loss: 0.1353547796807515\n",
            "\n",
            "\n",
            "Epoch 670 -\n",
            "Updated K params after epoch 670: [-0.1859497  -0.97984968]\n",
            "Train loss: 0.11912987507312338, Test loss: 0.13512409289577818\n",
            "\n",
            "\n",
            "Epoch 671 -\n",
            "Updated K params after epoch 671: [-0.18594144 -0.97997638]\n",
            "Train loss: 0.11891689856968188, Test loss: 0.13489431807690122\n",
            "\n",
            "\n",
            "Epoch 672 -\n",
            "Updated K params after epoch 672: [-0.18593322 -0.98010286]\n",
            "Train loss: 0.118704781172022, Test loss: 0.13466544945845002\n",
            "\n",
            "\n",
            "Epoch 673 -\n",
            "Updated K params after epoch 673: [-0.18592504 -0.9802291 ]\n",
            "Train loss: 0.1184935173048319, Test loss: 0.13443748132413286\n",
            "\n",
            "\n",
            "Epoch 674 -\n",
            "Updated K params after epoch 674: [-0.18591691 -0.98035511]\n",
            "Train loss: 0.11828310144134009, Test loss: 0.13421040800652584\n",
            "\n",
            "\n",
            "Epoch 675 -\n",
            "Updated K params after epoch 675: [-0.18590881 -0.9804809 ]\n",
            "Train loss: 0.11807352810281044, Test loss: 0.13398422388656697\n",
            "\n",
            "\n",
            "Epoch 676 -\n",
            "Updated K params after epoch 676: [-0.18590074 -0.98060646]\n",
            "Train loss: 0.11786479185804251, Test loss: 0.133758923393057\n",
            "\n",
            "\n",
            "Epoch 677 -\n",
            "Updated K params after epoch 677: [-0.18589272 -0.98073178]\n",
            "Train loss: 0.11765688732287827, Test loss: 0.13353450100216535\n",
            "\n",
            "\n",
            "Epoch 678 -\n",
            "Updated K params after epoch 678: [-0.18588473 -0.98085689]\n",
            "Train loss: 0.11744980915971412, Test loss: 0.13331095123694248\n",
            "\n",
            "\n",
            "Epoch 679 -\n",
            "Updated K params after epoch 679: [-0.18587679 -0.98098177]\n",
            "Train loss: 0.1172435520770189, Test loss: 0.13308826866683732\n",
            "\n",
            "\n",
            "Epoch 680 -\n",
            "Updated K params after epoch 680: [-0.18586888 -0.98110642]\n",
            "Train loss: 0.11703811082885753, Test loss: 0.13286644790722088\n",
            "\n",
            "\n",
            "Epoch 681 -\n",
            "Updated K params after epoch 681: [-0.18586101 -0.98123085]\n",
            "Train loss: 0.11683348021442001, Test loss: 0.1326454836189149\n",
            "\n",
            "\n",
            "Epoch 682 -\n",
            "Updated K params after epoch 682: [-0.18585318 -0.98135505]\n",
            "Train loss: 0.1166296550775561, Test loss: 0.13242537050772643\n",
            "\n",
            "\n",
            "Epoch 683 -\n",
            "Updated K params after epoch 683: [-0.18584538 -0.98147904]\n",
            "Train loss: 0.11642663030631531, Test loss: 0.13220610332398758\n",
            "\n",
            "\n",
            "Epoch 684 -\n",
            "Updated K params after epoch 684: [-0.18583762 -0.9816028 ]\n",
            "Train loss: 0.1162244008324924, Test loss: 0.13198767686210044\n",
            "\n",
            "\n",
            "Epoch 685 -\n",
            "Updated K params after epoch 685: [-0.1858299  -0.98172635]\n",
            "Train loss: 0.11602296163117783, Test loss: 0.13177008596008782\n",
            "\n",
            "\n",
            "Epoch 686 -\n",
            "Updated K params after epoch 686: [-0.18582222 -0.98184967]\n",
            "Train loss: 0.11582230772031386, Test loss: 0.13155332549914867\n",
            "\n",
            "\n",
            "Epoch 687 -\n",
            "Updated K params after epoch 687: [-0.18581458 -0.98197278]\n",
            "Train loss: 0.11562243416025551, Test loss: 0.13133739040321885\n",
            "\n",
            "\n",
            "Epoch 688 -\n",
            "Updated K params after epoch 688: [-0.18580697 -0.98209566]\n",
            "Train loss: 0.11542333605333667, Test loss: 0.13112227563853704\n",
            "\n",
            "\n",
            "Epoch 689 -\n",
            "Updated K params after epoch 689: [-0.1857994  -0.98221834]\n",
            "Train loss: 0.11522500854344123, Test loss: 0.1309079762132156\n",
            "\n",
            "\n",
            "Epoch 690 -\n",
            "Updated K params after epoch 690: [-0.18579187 -0.98234079]\n",
            "Train loss: 0.1150274468155793, Test loss: 0.13069448717681642\n",
            "\n",
            "\n",
            "Epoch 691 -\n",
            "Updated K params after epoch 691: [-0.18578437 -0.98246303]\n",
            "Train loss: 0.11483064609546814, Test loss: 0.13048180361993156\n",
            "\n",
            "\n",
            "Epoch 692 -\n",
            "Updated K params after epoch 692: [-0.18577692 -0.98258506]\n",
            "Train loss: 0.11463460164911801, Test loss: 0.1302699206737688\n",
            "\n",
            "\n",
            "Epoch 693 -\n",
            "Updated K params after epoch 693: [-0.18576949 -0.98270687]\n",
            "Train loss: 0.11443930878242287, Test loss: 0.13005883350974212\n",
            "\n",
            "\n",
            "Epoch 694 -\n",
            "Updated K params after epoch 694: [-0.18576211 -0.98282847]\n",
            "Train loss: 0.11424476284075565, Test loss: 0.12984853733906654\n",
            "\n",
            "\n",
            "Epoch 695 -\n",
            "Updated K params after epoch 695: [-0.18575476 -0.98294986]\n",
            "Train loss: 0.11405095920856831, Test loss: 0.12963902741235783\n",
            "\n",
            "\n",
            "Epoch 696 -\n",
            "Updated K params after epoch 696: [-0.18574745 -0.98307104]\n",
            "Train loss: 0.11385789330899644, Test loss: 0.12943029901923717\n",
            "\n",
            "\n",
            "Epoch 697 -\n",
            "Updated K params after epoch 697: [-0.18574017 -0.98319201]\n",
            "Train loss: 0.11366556060346843, Test loss: 0.1292223474879392\n",
            "\n",
            "\n",
            "Epoch 698 -\n",
            "Updated K params after epoch 698: [-0.18573294 -0.98331277]\n",
            "Train loss: 0.11347395659131906, Test loss: 0.12901516818492625\n",
            "\n",
            "\n",
            "Epoch 699 -\n",
            "Updated K params after epoch 699: [-0.18572573 -0.98343332]\n",
            "Train loss: 0.11328307680940757, Test loss: 0.12880875651450532\n",
            "\n",
            "\n",
            "Epoch 700 -\n",
            "Updated K params after epoch 700: [-0.18571857 -0.98355366]\n",
            "Train loss: 0.11309291683174016, Test loss: 0.12860310791845062\n",
            "\n",
            "\n",
            "Epoch 701 -\n",
            "Updated K params after epoch 701: [-0.18571144 -0.9836738 ]\n",
            "Train loss: 0.11290347226909674, Test loss: 0.12839821787562988\n",
            "\n",
            "\n",
            "Epoch 702 -\n",
            "Updated K params after epoch 702: [-0.18570434 -0.98379373]\n",
            "Train loss: 0.11271473876866195, Test loss: 0.12819408190163492\n",
            "\n",
            "\n",
            "Epoch 703 -\n",
            "Updated K params after epoch 703: [-0.18569728 -0.98391345]\n",
            "Train loss: 0.11252671201366031, Test loss: 0.12799069554841677\n",
            "\n",
            "\n",
            "Epoch 704 -\n",
            "Updated K params after epoch 704: [-0.18569026 -0.98403297]\n",
            "Train loss: 0.11233938772299581, Test loss: 0.1277880544039245\n",
            "\n",
            "\n",
            "Epoch 705 -\n",
            "Updated K params after epoch 705: [-0.18568327 -0.98415229]\n",
            "Train loss: 0.1121527616508953, Test loss: 0.12758615409174856\n",
            "\n",
            "\n",
            "Epoch 706 -\n",
            "Updated K params after epoch 706: [-0.18567632 -0.98427141]\n",
            "Train loss: 0.11196682958655606, Test loss: 0.12738499027076783\n",
            "\n",
            "\n",
            "Epoch 707 -\n",
            "Updated K params after epoch 707: [-0.18566941 -0.98439032]\n",
            "Train loss: 0.11178158735379741, Test loss: 0.12718455863480102\n",
            "\n",
            "\n",
            "Epoch 708 -\n",
            "Updated K params after epoch 708: [-0.18566253 -0.98450903]\n",
            "Train loss: 0.11159703081071629, Test loss: 0.1269848549122617\n",
            "\n",
            "\n",
            "Epoch 709 -\n",
            "Updated K params after epoch 709: [-0.18565568 -0.98462754]\n",
            "Train loss: 0.11141315584934652, Test loss: 0.12678587486581747\n",
            "\n",
            "\n",
            "Epoch 710 -\n",
            "Updated K params after epoch 710: [-0.18564887 -0.98474585]\n",
            "Train loss: 0.1112299583953224, Test loss: 0.12658761429205279\n",
            "\n",
            "\n",
            "Epoch 711 -\n",
            "Updated K params after epoch 711: [-0.1856421  -0.98486396]\n",
            "Train loss: 0.11104743440754554, Test loss: 0.12639006902113586\n",
            "\n",
            "\n",
            "Epoch 712 -\n",
            "Updated K params after epoch 712: [-0.18563536 -0.98498188]\n",
            "Train loss: 0.1108655798778559, Test loss: 0.12619323491648912\n",
            "\n",
            "\n",
            "Epoch 713 -\n",
            "Updated K params after epoch 713: [-0.18562865 -0.98509959]\n",
            "Train loss: 0.11068439083070634, Test loss: 0.12599710787446328\n",
            "\n",
            "\n",
            "Epoch 714 -\n",
            "Updated K params after epoch 714: [-0.18562198 -0.98521711]\n",
            "Train loss: 0.11050386332284086, Test loss: 0.1258016838240155\n",
            "\n",
            "\n",
            "Epoch 715 -\n",
            "Updated K params after epoch 715: [-0.18561535 -0.98533444]\n",
            "Train loss: 0.11032399344297654, Test loss: 0.12560695872639072\n",
            "\n",
            "\n",
            "Epoch 716 -\n",
            "Updated K params after epoch 716: [-0.18560875 -0.98545157]\n",
            "Train loss: 0.110144777311489, Test loss: 0.12541292857480682\n",
            "\n",
            "\n",
            "Epoch 717 -\n",
            "Updated K params after epoch 717: [-0.18560218 -0.9855685 ]\n",
            "Train loss: 0.10996621108010139, Test loss: 0.1252195893941432\n",
            "\n",
            "\n",
            "Epoch 718 -\n",
            "Updated K params after epoch 718: [-0.18559565 -0.98568524]\n",
            "Train loss: 0.10978829093157688, Test loss: 0.12502693724063305\n",
            "\n",
            "\n",
            "Epoch 719 -\n",
            "Updated K params after epoch 719: [-0.18558915 -0.98580179]\n",
            "Train loss: 0.1096110130794147, Test loss: 0.12483496820155879\n",
            "\n",
            "\n",
            "Epoch 720 -\n",
            "Updated K params after epoch 720: [-0.18558269 -0.98591814]\n",
            "Train loss: 0.10943437376754937, Test loss: 0.124643678394951\n",
            "\n",
            "\n",
            "Epoch 721 -\n",
            "Updated K params after epoch 721: [-0.18557626 -0.98603431]\n",
            "Train loss: 0.10925836927005352, Test loss: 0.12445306396929103\n",
            "\n",
            "\n",
            "Epoch 722 -\n",
            "Updated K params after epoch 722: [-0.18556987 -0.98615028]\n",
            "Train loss: 0.10908299589084389, Test loss: 0.12426312110321643\n",
            "\n",
            "\n",
            "Epoch 723 -\n",
            "Updated K params after epoch 723: [-0.18556351 -0.98626607]\n",
            "Train loss: 0.10890824996339078, Test loss: 0.12407384600522996\n",
            "\n",
            "\n",
            "Epoch 724 -\n",
            "Updated K params after epoch 724: [-0.18555718 -0.98638166]\n",
            "Train loss: 0.10873412785043036, Test loss: 0.12388523491341173\n",
            "\n",
            "\n",
            "Epoch 725 -\n",
            "Updated K params after epoch 725: [-0.18555089 -0.98649706]\n",
            "Train loss: 0.10856062594368077, Test loss: 0.12369728409513478\n",
            "\n",
            "\n",
            "Epoch 726 -\n",
            "Updated K params after epoch 726: [-0.18554463 -0.98661228]\n",
            "Train loss: 0.10838774066356076, Test loss: 0.12350998984678348\n",
            "\n",
            "\n",
            "Epoch 727 -\n",
            "Updated K params after epoch 727: [-0.1855384  -0.98672731]\n",
            "Train loss: 0.10821546845891197, Test loss: 0.12332334849347516\n",
            "\n",
            "\n",
            "Epoch 728 -\n",
            "Updated K params after epoch 728: [-0.18553221 -0.98684216]\n",
            "Train loss: 0.10804380580672396, Test loss: 0.12313735638878504\n",
            "\n",
            "\n",
            "Epoch 729 -\n",
            "Updated K params after epoch 729: [-0.18552605 -0.98695682]\n",
            "Train loss: 0.10787274921186248, Test loss: 0.12295200991447397\n",
            "\n",
            "\n",
            "Epoch 730 -\n",
            "Updated K params after epoch 730: [-0.18551992 -0.98707129]\n",
            "Train loss: 0.10770229520680061, Test loss: 0.12276730548021923\n",
            "\n",
            "\n",
            "Epoch 731 -\n",
            "Updated K params after epoch 731: [-0.18551383 -0.98718558]\n",
            "Train loss: 0.10753244035135295, Test loss: 0.12258323952334829\n",
            "\n",
            "\n",
            "Epoch 732 -\n",
            "Updated K params after epoch 732: [-0.18550777 -0.98729968]\n",
            "Train loss: 0.10736318123241281, Test loss: 0.12239980850857571\n",
            "\n",
            "\n",
            "Epoch 733 -\n",
            "Updated K params after epoch 733: [-0.18550174 -0.98741361]\n",
            "Train loss: 0.1071945144636921, Test loss: 0.12221700892774251\n",
            "\n",
            "\n",
            "Epoch 734 -\n",
            "Updated K params after epoch 734: [-0.18549575 -0.98752735]\n",
            "Train loss: 0.10702643668546434, Test loss: 0.12203483729955895\n",
            "\n",
            "\n",
            "Epoch 735 -\n",
            "Updated K params after epoch 735: [-0.18548979 -0.9876409 ]\n",
            "Train loss: 0.10685894456431026, Test loss: 0.12185329016934969\n",
            "\n",
            "\n",
            "Epoch 736 -\n",
            "Updated K params after epoch 736: [-0.18548386 -0.98775428]\n",
            "Train loss: 0.10669203479286642, Test loss: 0.1216723641088019\n",
            "\n",
            "\n",
            "Epoch 737 -\n",
            "Updated K params after epoch 737: [-0.18547797 -0.98786748]\n",
            "Train loss: 0.10652570408957641, Test loss: 0.12149205571571633\n",
            "\n",
            "\n",
            "Epoch 738 -\n",
            "Updated K params after epoch 738: [-0.1854721 -0.9879805]\n",
            "Train loss: 0.10635994919844488, Test loss: 0.12131236161376067\n",
            "\n",
            "\n",
            "Epoch 739 -\n",
            "Updated K params after epoch 739: [-0.18546627 -0.98809334]\n",
            "Train loss: 0.1061947668887942, Test loss: 0.12113327845222606\n",
            "\n",
            "\n",
            "Epoch 740 -\n",
            "Updated K params after epoch 740: [-0.18546047 -0.988206  ]\n",
            "Train loss: 0.10603015395502388, Test loss: 0.12095480290578597\n",
            "\n",
            "\n",
            "Epoch 741 -\n",
            "Updated K params after epoch 741: [-0.18545471 -0.98831848]\n",
            "Train loss: 0.10586610721637248, Test loss: 0.12077693167425796\n",
            "\n",
            "\n",
            "Epoch 742 -\n",
            "Updated K params after epoch 742: [-0.18544897 -0.98843079]\n",
            "Train loss: 0.10570262351668226, Test loss: 0.12059966148236757\n",
            "\n",
            "\n",
            "Epoch 743 -\n",
            "Updated K params after epoch 743: [-0.18544327 -0.98854291]\n",
            "Train loss: 0.10553969972416634, Test loss: 0.12042298907951542\n",
            "\n",
            "\n",
            "Epoch 744 -\n",
            "Updated K params after epoch 744: [-0.1854376  -0.98865487]\n",
            "Train loss: 0.10537733273117837, Test loss: 0.12024691123954633\n",
            "\n",
            "\n",
            "Epoch 745 -\n",
            "Updated K params after epoch 745: [-0.18543196 -0.98876665]\n",
            "Train loss: 0.10521551945398461, Test loss: 0.12007142476052116\n",
            "\n",
            "\n",
            "Epoch 746 -\n",
            "Updated K params after epoch 746: [-0.18542636 -0.98887825]\n",
            "Train loss: 0.10505425683253881, Test loss: 0.11989652646449123\n",
            "\n",
            "\n",
            "Epoch 747 -\n",
            "Updated K params after epoch 747: [-0.18542078 -0.98898968]\n",
            "Train loss: 0.10489354183025922, Test loss: 0.11972221319727477\n",
            "\n",
            "\n",
            "Epoch 748 -\n",
            "Updated K params after epoch 748: [-0.18541524 -0.98910094]\n",
            "Train loss: 0.10473337143380804, Test loss: 0.11954848182823641\n",
            "\n",
            "\n",
            "Epoch 749 -\n",
            "Updated K params after epoch 749: [-0.18540973 -0.98921203]\n",
            "Train loss: 0.1045737426528735, Test loss: 0.11937532925006844\n",
            "\n",
            "\n",
            "Epoch 750 -\n",
            "Updated K params after epoch 750: [-0.18540425 -0.98932294]\n",
            "Train loss: 0.10441465251995397, Test loss: 0.11920275237857478\n",
            "\n",
            "\n",
            "Train accuracy: 0.970856102003643, Test accuracy: 0.9708029197080292\n",
            "Train f1: 0.9666666666666667, Test f1: 0.9666666666666667\n",
            "\n",
            "\n",
            "Mean train accuracy: 0.970856102003643, Mean test accuracy: 0.9708029197080292\n",
            "Mean train f1: 0.9666666666666667, Mean test f1: 0.9666666666666667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Final training the model now with the best hyperparameters from above -"
      ],
      "metadata": {
        "id": "Z9ciNnQhLySx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(df.loc[:, :'f4'], df.loc[:, 'target'], test_size = 0.2, stratify=df.loc[:, 'target'], random_state=42)\n",
        "X_train.shape, y_train.shape, X_test.shape, y_test.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ghH5J8tcL_pr",
        "outputId": "4227eb3e-24fe-4d7f-9d92-1c7ac9921b15"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "((1097, 4), (1097,), (275, 4), (275,))"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test = X_train.reset_index(drop=True), X_test.reset_index(drop=True)\n",
        "y_train, y_test = y_train.reset_index(drop=True), y_test.reset_index(drop=True)\n",
        "y_train, y_test = y_train.values.reshape((1, X_train.shape[0])), y_test.values.reshape((1, X_test.shape[0]))\n",
        "\n",
        "# Best hyperparameters -\n",
        "alpha, epochs, pred_thresh, hid_nodes = 1e-2, 750, 0.5, 8 # Defining the hyperparameters\n",
        "\n",
        "# Training the model -\n",
        "tr_hist, best_params, te_hist = NN(X_train.values.T, y_train, X_test.values.T, y_test, alpha, epochs, hid_nodes)\n",
        "\n",
        "# Printing the final metrics -\n",
        "train_probs = fwd_prop(X_train.values.T, best_params)['a2']\n",
        "test_probs = fwd_prop(X_test.values.T, best_params)['a2']\n",
        "\n",
        "train_preds = (train_probs >= pred_thresh).astype('int').flatten()\n",
        "test_preds = (test_probs >= pred_thresh).astype('int').flatten()\n",
        "\n",
        "y_train, y_test = y_train.flatten(), y_test.flatten()\n",
        "\n",
        "train_acc = accuracy_score(y_train, train_preds)\n",
        "test_acc = accuracy_score(y_test, test_preds)\n",
        "\n",
        "train_f1 = f1_score(y_train, train_preds) # f1 score for label 1\n",
        "test_f1 = f1_score(y_test, test_preds) # f1 score for label 1\n",
        "\n",
        "print(f\"Train accuracy: {train_acc}, Test accuracy: {test_acc}\")\n",
        "print(f\"Train f1: {train_f1}, Test f1: {test_f1}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uuyxUtT7vO9a",
        "outputId": "0194f1cd-c729-482c-fd0b-d25ecb59ff4f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial params: {'W1': array([[ 0.01624345, -0.00611756, -0.00528172, -0.01072969],\n",
            "       [ 0.00865408, -0.02301539,  0.01744812, -0.00761207],\n",
            "       [ 0.00319039, -0.0024937 ,  0.01462108, -0.02060141],\n",
            "       [-0.00322417, -0.00384054,  0.01133769, -0.01099891],\n",
            "       [-0.00172428, -0.00877858,  0.00042214,  0.00582815],\n",
            "       [-0.01100619,  0.01144724,  0.00901591,  0.00502494],\n",
            "       [ 0.00900856, -0.00683728, -0.0012289 , -0.00935769],\n",
            "       [-0.00267888,  0.00530355, -0.00691661, -0.00396754]]), 'b1': array([[0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.],\n",
            "       [0.]]), 'W2': array([[-0.00687173, -0.00845206, -0.00671246, -0.00012665, -0.0111731 ,\n",
            "         0.00234416,  0.01659802,  0.00742044]]), 'b2': array([[0.]]), 'K': array([-0.19183555, -0.88762896])}\n",
            "Epoch 1 -\n",
            "Updated K params after epoch 1: [-0.19183507 -0.88762948]\n",
            "Train loss: 0.6928524343289046, Test loss: 0.6928352856625137\n",
            "\n",
            "\n",
            "Epoch 2 -\n",
            "Updated K params after epoch 2: [-0.19183462 -0.88763003]\n",
            "Train loss: 0.692794052479291, Test loss: 0.6927769381811607\n",
            "\n",
            "\n",
            "Epoch 3 -\n",
            "Updated K params after epoch 3: [-0.19183421 -0.88763061]\n",
            "Train loss: 0.692735598004014, Test loss: 0.6927185056332111\n",
            "\n",
            "\n",
            "Epoch 4 -\n",
            "Updated K params after epoch 4: [-0.19183384 -0.88763123]\n",
            "Train loss: 0.6926770418664095, Test loss: 0.6926599594381476\n",
            "\n",
            "\n",
            "Epoch 5 -\n",
            "Updated K params after epoch 5: [-0.19183351 -0.88763187]\n",
            "Train loss: 0.6926183548629719, Test loss: 0.6926012708242265\n",
            "\n",
            "\n",
            "Epoch 6 -\n",
            "Updated K params after epoch 6: [-0.19183321 -0.88763255]\n",
            "Train loss: 0.6925595075959708, Test loss: 0.6925424108016756\n",
            "\n",
            "\n",
            "Epoch 7 -\n",
            "Updated K params after epoch 7: [-0.19183294 -0.88763326]\n",
            "Train loss: 0.6925004704460422, Test loss: 0.6924833501358295\n",
            "\n",
            "\n",
            "Epoch 8 -\n",
            "Updated K params after epoch 8: [-0.19183271 -0.88763401]\n",
            "Train loss: 0.6924412135447259, Test loss: 0.692424059320188\n",
            "\n",
            "\n",
            "Epoch 9 -\n",
            "Updated K params after epoch 9: [-0.19183251 -0.88763479]\n",
            "Train loss: 0.6923817067469261, Test loss: 0.692364508549367\n",
            "\n",
            "\n",
            "Epoch 10 -\n",
            "Updated K params after epoch 10: [-0.19183235 -0.88763561]\n",
            "Train loss: 0.6923219196032772, Test loss: 0.6923046676919251\n",
            "\n",
            "\n",
            "Epoch 11 -\n",
            "Updated K params after epoch 11: [-0.19183222 -0.88763646]\n",
            "Train loss: 0.6922618213323826, Test loss: 0.6922445062630436\n",
            "\n",
            "\n",
            "Epoch 12 -\n",
            "Updated K params after epoch 12: [-0.19183213 -0.88763735]\n",
            "Train loss: 0.6922013807929123, Test loss: 0.6921839933970332\n",
            "\n",
            "\n",
            "Epoch 13 -\n",
            "Updated K params after epoch 13: [-0.19183206 -0.88763828]\n",
            "Train loss: 0.6921405664555338, Test loss: 0.692123097819653\n",
            "\n",
            "\n",
            "Epoch 14 -\n",
            "Updated K params after epoch 14: [-0.19183203 -0.88763925]\n",
            "Train loss: 0.6920793463746533, Test loss: 0.6920617878202161\n",
            "\n",
            "\n",
            "Epoch 15 -\n",
            "Updated K params after epoch 15: [-0.19183203 -0.88764025]\n",
            "Train loss: 0.6920176881599486, Test loss: 0.6920000312234638\n",
            "\n",
            "\n",
            "Epoch 16 -\n",
            "Updated K params after epoch 16: [-0.19183206 -0.8876413 ]\n",
            "Train loss: 0.69195555894767, Test loss: 0.6919377953611848\n",
            "\n",
            "\n",
            "Epoch 17 -\n",
            "Updated K params after epoch 17: [-0.19183212 -0.88764239]\n",
            "Train loss: 0.6918929253716917, Test loss: 0.6918750470435653\n",
            "\n",
            "\n",
            "Epoch 18 -\n",
            "Updated K params after epoch 18: [-0.19183222 -0.88764352]\n",
            "Train loss: 0.6918297535342887, Test loss: 0.6918117525302445\n",
            "\n",
            "\n",
            "Epoch 19 -\n",
            "Updated K params after epoch 19: [-0.19183234 -0.8876447 ]\n",
            "Train loss: 0.6917660089766242, Test loss: 0.6917478775010587\n",
            "\n",
            "\n",
            "Epoch 20 -\n",
            "Updated K params after epoch 20: [-0.19183249 -0.88764592]\n",
            "Train loss: 0.691701656648925, Test loss: 0.6916833870264565\n",
            "\n",
            "\n",
            "Epoch 21 -\n",
            "Updated K params after epoch 21: [-0.19183268 -0.88764719]\n",
            "Train loss: 0.6916366608803256, Test loss: 0.6916182455375647\n",
            "\n",
            "\n",
            "Epoch 22 -\n",
            "Updated K params after epoch 22: [-0.19183289 -0.88764851]\n",
            "Train loss: 0.6915709853483638, Test loss: 0.6915524167958875\n",
            "\n",
            "\n",
            "Epoch 23 -\n",
            "Updated K params after epoch 23: [-0.19183313 -0.88764987]\n",
            "Train loss: 0.691504593048108, Test loss: 0.6914858638626212\n",
            "\n",
            "\n",
            "Epoch 24 -\n",
            "Updated K params after epoch 24: [-0.1918334  -0.88765128]\n",
            "Train loss: 0.6914374462608991, Test loss: 0.6914185490675675\n",
            "\n",
            "\n",
            "Epoch 25 -\n",
            "Updated K params after epoch 25: [-0.1918337  -0.88765275]\n",
            "Train loss: 0.6913695065226875, Test loss: 0.6913504339776276\n",
            "\n",
            "\n",
            "Epoch 26 -\n",
            "Updated K params after epoch 26: [-0.19183403 -0.88765427]\n",
            "Train loss: 0.6913007345919522, Test loss: 0.6912814793648632\n",
            "\n",
            "\n",
            "Epoch 27 -\n",
            "Updated K params after epoch 27: [-0.19183438 -0.88765584]\n",
            "Train loss: 0.6912310904171798, Test loss: 0.6912116451741043\n",
            "\n",
            "\n",
            "Epoch 28 -\n",
            "Updated K params after epoch 28: [-0.19183477 -0.88765747]\n",
            "Train loss: 0.6911605331038926, Test loss: 0.6911408904900934\n",
            "\n",
            "\n",
            "Epoch 29 -\n",
            "Updated K params after epoch 29: [-0.19183518 -0.88765915]\n",
            "Train loss: 0.691089020881206, Test loss: 0.6910691735041478\n",
            "\n",
            "\n",
            "Epoch 30 -\n",
            "Updated K params after epoch 30: [-0.19183562 -0.8876609 ]\n",
            "Train loss: 0.6910165110679036, Test loss: 0.6909964514803262\n",
            "\n",
            "\n",
            "Epoch 31 -\n",
            "Updated K params after epoch 31: [-0.19183608 -0.8876627 ]\n",
            "Train loss: 0.6909429600380121, Test loss: 0.6909226807210888\n",
            "\n",
            "\n",
            "Epoch 32 -\n",
            "Updated K params after epoch 32: [-0.19183657 -0.88766457]\n",
            "Train loss: 0.6908683231858669, Test loss: 0.6908478165324337\n",
            "\n",
            "\n",
            "Epoch 33 -\n",
            "Updated K params after epoch 33: [-0.19183709 -0.8876665 ]\n",
            "Train loss: 0.6907925548906495, Test loss: 0.690771813188502\n",
            "\n",
            "\n",
            "Epoch 34 -\n",
            "Updated K params after epoch 34: [-0.19183763 -0.8876685 ]\n",
            "Train loss: 0.6907156084803903, Test loss: 0.6906946238956361\n",
            "\n",
            "\n",
            "Epoch 35 -\n",
            "Updated K params after epoch 35: [-0.1918382  -0.88767057]\n",
            "Train loss: 0.6906374361954202, Test loss: 0.6906162007558833\n",
            "\n",
            "\n",
            "Epoch 36 -\n",
            "Updated K params after epoch 36: [-0.1918388 -0.8876727]\n",
            "Train loss: 0.6905579891512652, Test loss: 0.6905364947299344\n",
            "\n",
            "\n",
            "Epoch 37 -\n",
            "Updated K params after epoch 37: [-0.19183942 -0.88767491]\n",
            "Train loss: 0.6904772173009681, Test loss: 0.690455455599488\n",
            "\n",
            "\n",
            "Epoch 38 -\n",
            "Updated K params after epoch 38: [-0.19184007 -0.88767719]\n",
            "Train loss: 0.6903950693968364, Test loss: 0.690373031929034\n",
            "\n",
            "\n",
            "Epoch 39 -\n",
            "Updated K params after epoch 39: [-0.19184074 -0.88767955]\n",
            "Train loss: 0.6903114929516001, Test loss: 0.6902891710270468\n",
            "\n",
            "\n",
            "Epoch 40 -\n",
            "Updated K params after epoch 40: [-0.19184144 -0.88768199]\n",
            "Train loss: 0.6902264341989811, Test loss: 0.6902038189065879\n",
            "\n",
            "\n",
            "Epoch 41 -\n",
            "Updated K params after epoch 41: [-0.19184217 -0.88768451]\n",
            "Train loss: 0.6901398380536604, Test loss: 0.6901169202453079\n",
            "\n",
            "\n",
            "Epoch 42 -\n",
            "Updated K params after epoch 42: [-0.19184291 -0.88768711]\n",
            "Train loss: 0.6900516480706473, Test loss: 0.6900284183448487\n",
            "\n",
            "\n",
            "Epoch 43 -\n",
            "Updated K params after epoch 43: [-0.19184369 -0.8876898 ]\n",
            "Train loss: 0.6899618064040407, Test loss: 0.6899382550896441\n",
            "\n",
            "\n",
            "Epoch 44 -\n",
            "Updated K params after epoch 44: [-0.19184449 -0.88769258]\n",
            "Train loss: 0.6898702537651855, Test loss: 0.6898463709051176\n",
            "\n",
            "\n",
            "Epoch 45 -\n",
            "Updated K params after epoch 45: [-0.19184531 -0.88769545]\n",
            "Train loss: 0.6897769293802228, Test loss: 0.6897527047152789\n",
            "\n",
            "\n",
            "Epoch 46 -\n",
            "Updated K params after epoch 46: [-0.19184615 -0.88769841]\n",
            "Train loss: 0.6896817709470364, Test loss: 0.689657193899726\n",
            "\n",
            "\n",
            "Epoch 47 -\n",
            "Updated K params after epoch 47: [-0.19184703 -0.88770147]\n",
            "Train loss: 0.6895847145915981, Test loss: 0.6895597742500532\n",
            "\n",
            "\n",
            "Epoch 48 -\n",
            "Updated K params after epoch 48: [-0.19184792 -0.88770463]\n",
            "Train loss: 0.6894856948237185, Test loss: 0.6894603799256752\n",
            "\n",
            "\n",
            "Epoch 49 -\n",
            "Updated K params after epoch 49: [-0.19184884 -0.8877079 ]\n",
            "Train loss: 0.689384644492211, Test loss: 0.6893589434090786\n",
            "\n",
            "\n",
            "Epoch 50 -\n",
            "Updated K params after epoch 50: [-0.19184978 -0.88771127]\n",
            "Train loss: 0.6892814947394781, Test loss: 0.6892553954605075\n",
            "\n",
            "\n",
            "Epoch 51 -\n",
            "Updated K params after epoch 51: [-0.19185075 -0.88771474]\n",
            "Train loss: 0.6891761749555323, Test loss: 0.6891496650721013\n",
            "\n",
            "\n",
            "Epoch 52 -\n",
            "Updated K params after epoch 52: [-0.19185174 -0.88771833]\n",
            "Train loss: 0.6890686127314664, Test loss: 0.6890416794215011\n",
            "\n",
            "\n",
            "Epoch 53 -\n",
            "Updated K params after epoch 53: [-0.19185275 -0.88772204]\n",
            "Train loss: 0.6889587338123898, Test loss: 0.6889313638249417\n",
            "\n",
            "\n",
            "Epoch 54 -\n",
            "Updated K params after epoch 54: [-0.19185379 -0.88772587]\n",
            "Train loss: 0.6888464620498532, Test loss: 0.688818641689854\n",
            "\n",
            "\n",
            "Epoch 55 -\n",
            "Updated K params after epoch 55: [-0.19185485 -0.88772981]\n",
            "Train loss: 0.6887317193537792, Test loss: 0.6887034344670023\n",
            "\n",
            "\n",
            "Epoch 56 -\n",
            "Updated K params after epoch 56: [-0.19185594 -0.88773389]\n",
            "Train loss: 0.688614425643931, Test loss: 0.6885856616021852\n",
            "\n",
            "\n",
            "Epoch 57 -\n",
            "Updated K params after epoch 57: [-0.19185705 -0.8877381 ]\n",
            "Train loss: 0.6884944988009452, Test loss: 0.6884652404875315\n",
            "\n",
            "\n",
            "Epoch 58 -\n",
            "Updated K params after epoch 58: [-0.19185818 -0.88774244]\n",
            "Train loss: 0.6883718546169613, Test loss: 0.6883420864124294\n",
            "\n",
            "\n",
            "Epoch 59 -\n",
            "Updated K params after epoch 59: [-0.19185933 -0.88774691]\n",
            "Train loss: 0.6882464067458868, Test loss: 0.6882161125141237\n",
            "\n",
            "\n",
            "Epoch 60 -\n",
            "Updated K params after epoch 60: [-0.19186051 -0.88775153]\n",
            "Train loss: 0.6881180666533362, Test loss: 0.6880872297280319\n",
            "\n",
            "\n",
            "Epoch 61 -\n",
            "Updated K params after epoch 61: [-0.19186171 -0.8877563 ]\n",
            "Train loss: 0.6879867435662906, Test loss: 0.6879553467378207\n",
            "\n",
            "\n",
            "Epoch 62 -\n",
            "Updated K params after epoch 62: [-0.19186293 -0.88776122]\n",
            "Train loss: 0.6878523444225249, Test loss: 0.6878203699252986\n",
            "\n",
            "\n",
            "Epoch 63 -\n",
            "Updated K params after epoch 63: [-0.19186418 -0.8877663 ]\n",
            "Train loss: 0.6877147738198578, Test loss: 0.687682203320184\n",
            "\n",
            "\n",
            "Epoch 64 -\n",
            "Updated K params after epoch 64: [-0.19186544 -0.88777153]\n",
            "Train loss: 0.6875739339652852, Test loss: 0.687540748549806\n",
            "\n",
            "\n",
            "Epoch 65 -\n",
            "Updated K params after epoch 65: [-0.19186673 -0.88777693]\n",
            "Train loss: 0.6874297246240557, Test loss: 0.6873959047888117\n",
            "\n",
            "\n",
            "Epoch 66 -\n",
            "Updated K params after epoch 66: [-0.19186805 -0.8877825 ]\n",
            "Train loss: 0.6872820430687642, Test loss: 0.6872475687089503\n",
            "\n",
            "\n",
            "Epoch 67 -\n",
            "Updated K params after epoch 67: [-0.19186938 -0.88778825]\n",
            "Train loss: 0.6871307840285319, Test loss: 0.6870956344290136\n",
            "\n",
            "\n",
            "Epoch 68 -\n",
            "Updated K params after epoch 68: [-0.19187074 -0.88779417]\n",
            "Train loss: 0.6869758396383605, Test loss: 0.6869399934650208\n",
            "\n",
            "\n",
            "Epoch 69 -\n",
            "Updated K params after epoch 69: [-0.19187213 -0.88780028]\n",
            "Train loss: 0.686817099388742, Test loss: 0.686780534680736\n",
            "\n",
            "\n",
            "Epoch 70 -\n",
            "Updated K params after epoch 70: [-0.19187353 -0.88780659]\n",
            "Train loss: 0.6866544500756236, Test loss: 0.6866171442386233\n",
            "\n",
            "\n",
            "Epoch 71 -\n",
            "Updated K params after epoch 71: [-0.19187496 -0.88781309]\n",
            "Train loss: 0.6864877757508234, Test loss: 0.6864497055513386\n",
            "\n",
            "\n",
            "Epoch 72 -\n",
            "Updated K params after epoch 72: [-0.1918764  -0.88781979]\n",
            "Train loss: 0.6863169576730096, Test loss: 0.6862780992338777\n",
            "\n",
            "\n",
            "Epoch 73 -\n",
            "Updated K params after epoch 73: [-0.19187787 -0.8878267 ]\n",
            "Train loss: 0.6861418742593572, Test loss: 0.686102203056499\n",
            "\n",
            "\n",
            "Epoch 74 -\n",
            "Updated K params after epoch 74: [-0.19187937 -0.88783382]\n",
            "Train loss: 0.6859624010380049, Test loss: 0.6859218918985516\n",
            "\n",
            "\n",
            "Epoch 75 -\n",
            "Updated K params after epoch 75: [-0.19188088 -0.88784117]\n",
            "Train loss: 0.6857784106014464, Test loss: 0.6857370377033454\n",
            "\n",
            "\n",
            "Epoch 76 -\n",
            "Updated K params after epoch 76: [-0.19188242 -0.88784874]\n",
            "Train loss: 0.685589772560995, Test loss: 0.685547509434212\n",
            "\n",
            "\n",
            "Epoch 77 -\n",
            "Updated K params after epoch 77: [-0.19188398 -0.88785655]\n",
            "Train loss: 0.6853963535024722, Test loss: 0.6853531730319113\n",
            "\n",
            "\n",
            "Epoch 78 -\n",
            "Updated K params after epoch 78: [-0.19188556 -0.8878646 ]\n",
            "Train loss: 0.6851980169432785, Test loss: 0.685153891373548\n",
            "\n",
            "\n",
            "Epoch 79 -\n",
            "Updated K params after epoch 79: [-0.19188717 -0.88787289]\n",
            "Train loss: 0.684994623291014, Test loss: 0.6849495242331752\n",
            "\n",
            "\n",
            "Epoch 80 -\n",
            "Updated K params after epoch 80: [-0.19188879 -0.88788144]\n",
            "Train loss: 0.6847860298038293, Test loss: 0.6847399282442694\n",
            "\n",
            "\n",
            "Epoch 81 -\n",
            "Updated K params after epoch 81: [-0.19189044 -0.88789025]\n",
            "Train loss: 0.6845720905526939, Test loss: 0.684524956864272\n",
            "\n",
            "\n",
            "Epoch 82 -\n",
            "Updated K params after epoch 82: [-0.19189211 -0.88789934]\n",
            "Train loss: 0.6843526563857805, Test loss: 0.6843044603414044\n",
            "\n",
            "\n",
            "Epoch 83 -\n",
            "Updated K params after epoch 83: [-0.1918938  -0.88790869]\n",
            "Train loss: 0.6841275748951785, Test loss: 0.6840782856839754\n",
            "\n",
            "\n",
            "Epoch 84 -\n",
            "Updated K params after epoch 84: [-0.19189551 -0.88791834]\n",
            "Train loss: 0.6838966903861547, Test loss: 0.6838462766324063\n",
            "\n",
            "\n",
            "Epoch 85 -\n",
            "Updated K params after epoch 85: [-0.19189725 -0.88792828]\n",
            "Train loss: 0.6836598438491963, Test loss: 0.6836082736342179\n",
            "\n",
            "\n",
            "Epoch 86 -\n",
            "Updated K params after epoch 86: [-0.191899   -0.88793851]\n",
            "Train loss: 0.6834168729350797, Test loss: 0.6833641138222296\n",
            "\n",
            "\n",
            "Epoch 87 -\n",
            "Updated K params after epoch 87: [-0.19190078 -0.88794906]\n",
            "Train loss: 0.6831676119332243, Test loss: 0.6831136309962343\n",
            "\n",
            "\n",
            "Epoch 88 -\n",
            "Updated K params after epoch 88: [-0.19190258 -0.88795993]\n",
            "Train loss: 0.6829118917535966, Test loss: 0.6828566556084285\n",
            "\n",
            "\n",
            "Epoch 89 -\n",
            "Updated K params after epoch 89: [-0.1919044  -0.88797113]\n",
            "Train loss: 0.6826495399124497, Test loss: 0.6825930147528825\n",
            "\n",
            "\n",
            "Epoch 90 -\n",
            "Updated K params after epoch 90: [-0.19190624 -0.88798266]\n",
            "Train loss: 0.682380380522191, Test loss: 0.682322532159356\n",
            "\n",
            "\n",
            "Epoch 91 -\n",
            "Updated K params after epoch 91: [-0.19190811 -0.88799454]\n",
            "Train loss: 0.6821042342856837, Test loss: 0.6820450281917709\n",
            "\n",
            "\n",
            "Epoch 92 -\n",
            "Updated K params after epoch 92: [-0.19190999 -0.88800677]\n",
            "Train loss: 0.6818209184953055, Test loss: 0.6817603198516681\n",
            "\n",
            "\n",
            "Epoch 93 -\n",
            "Updated K params after epoch 93: [-0.1919119  -0.88801937]\n",
            "Train loss: 0.6815302470370918, Test loss: 0.681468220786988\n",
            "\n",
            "\n",
            "Epoch 94 -\n",
            "Updated K params after epoch 94: [-0.19191382 -0.88803235]\n",
            "Train loss: 0.6812320304003154, Test loss: 0.6811685413065262\n",
            "\n",
            "\n",
            "Epoch 95 -\n",
            "Updated K params after epoch 95: [-0.19191577 -0.88804571]\n",
            "Train loss: 0.6809260756928542, Test loss: 0.680861088400429\n",
            "\n",
            "\n",
            "Epoch 96 -\n",
            "Updated K params after epoch 96: [-0.19191774 -0.88805947]\n",
            "Train loss: 0.6806121866627217, Test loss: 0.6805456657671024\n",
            "\n",
            "\n",
            "Epoch 97 -\n",
            "Updated K params after epoch 97: [-0.19191973 -0.88807363]\n",
            "Train loss: 0.6802901637261433, Test loss: 0.6802220738469249\n",
            "\n",
            "\n",
            "Epoch 98 -\n",
            "Updated K params after epoch 98: [-0.19192174 -0.88808822]\n",
            "Train loss: 0.6799598040025693, Test loss: 0.6798901098631622\n",
            "\n",
            "\n",
            "Epoch 99 -\n",
            "Updated K params after epoch 99: [-0.19192377 -0.88810323]\n",
            "Train loss: 0.6796209013570381, Test loss: 0.6795495678704917\n",
            "\n",
            "\n",
            "Epoch 100 -\n",
            "Updated K params after epoch 100: [-0.19192582 -0.88811868]\n",
            "Train loss: 0.6792732464502989, Test loss: 0.6792002388115591\n",
            "\n",
            "\n",
            "Epoch 101 -\n",
            "Updated K params after epoch 101: [-0.19192789 -0.88813458]\n",
            "Train loss: 0.6789166267971282, Test loss: 0.678841910581995\n",
            "\n",
            "\n",
            "Epoch 102 -\n",
            "Updated K params after epoch 102: [-0.19192998 -0.88815095]\n",
            "Train loss: 0.6785508268332742, Test loss: 0.6784743681043289\n",
            "\n",
            "\n",
            "Epoch 103 -\n",
            "Updated K params after epoch 103: [-0.19193209 -0.8881678 ]\n",
            "Train loss: 0.6781756279914762, Test loss: 0.6780973934112459\n",
            "\n",
            "\n",
            "Epoch 104 -\n",
            "Updated K params after epoch 104: [-0.19193422 -0.88818513]\n",
            "Train loss: 0.6777908087870138, Test loss: 0.6777107657386366\n",
            "\n",
            "\n",
            "Epoch 105 -\n",
            "Updated K params after epoch 105: [-0.19193637 -0.88820297]\n",
            "Train loss: 0.6773961449132426, Test loss: 0.6773142616288981\n",
            "\n",
            "\n",
            "Epoch 106 -\n",
            "Updated K params after epoch 106: [-0.19193854 -0.88822131]\n",
            "Train loss: 0.6769914093475881, Test loss: 0.6769076550449431\n",
            "\n",
            "\n",
            "Epoch 107 -\n",
            "Updated K params after epoch 107: [-0.19194072 -0.88824019]\n",
            "Train loss: 0.676576372468461, Test loss: 0.6764907174953799\n",
            "\n",
            "\n",
            "Epoch 108 -\n",
            "Updated K params after epoch 108: [-0.19194293 -0.88825961]\n",
            "Train loss: 0.6761508021835677, Test loss: 0.676063218171325\n",
            "\n",
            "\n",
            "Epoch 109 -\n",
            "Updated K params after epoch 109: [-0.19194515 -0.88827958]\n",
            "Train loss: 0.6757144640700873, Test loss: 0.6756249240953043\n",
            "\n",
            "\n",
            "Epoch 110 -\n",
            "Updated K params after epoch 110: [-0.19194739 -0.88830012]\n",
            "Train loss: 0.6752671215271844, Test loss: 0.6751756002827002\n",
            "\n",
            "\n",
            "Epoch 111 -\n",
            "Updated K params after epoch 111: [-0.19194965 -0.88832124]\n",
            "Train loss: 0.67480853594132, Test loss: 0.6747150099161904\n",
            "\n",
            "\n",
            "Epoch 112 -\n",
            "Updated K params after epoch 112: [-0.19195193 -0.88834295]\n",
            "Train loss: 0.6743384668648222, Test loss: 0.6742429145336146\n",
            "\n",
            "\n",
            "Epoch 113 -\n",
            "Updated K params after epoch 113: [-0.19195423 -0.88836528]\n",
            "Train loss: 0.6738566722081629, Test loss: 0.673759074229697\n",
            "\n",
            "\n",
            "Epoch 114 -\n",
            "Updated K params after epoch 114: [-0.19195654 -0.88838824]\n",
            "Train loss: 0.6733629084463746, Test loss: 0.6732632478720296\n",
            "\n",
            "\n",
            "Epoch 115 -\n",
            "Updated K params after epoch 115: [-0.19195887 -0.88841183]\n",
            "Train loss: 0.6728569308400317, Test loss: 0.672755193331708\n",
            "\n",
            "\n",
            "Epoch 116 -\n",
            "Updated K params after epoch 116: [-0.19196121 -0.88843608]\n",
            "Train loss: 0.6723384936711929, Test loss: 0.6722346677289852\n",
            "\n",
            "\n",
            "Epoch 117 -\n",
            "Updated K params after epoch 117: [-0.19196357 -0.888461  ]\n",
            "Train loss: 0.6718073504946838, Test loss: 0.671701427694282\n",
            "\n",
            "\n",
            "Epoch 118 -\n",
            "Updated K params after epoch 118: [-0.19196595 -0.88848661]\n",
            "Train loss: 0.6712632544050701, Test loss: 0.6711552296448628\n",
            "\n",
            "\n",
            "Epoch 119 -\n",
            "Updated K params after epoch 119: [-0.19196834 -0.88851292]\n",
            "Train loss: 0.6707059583196405, Test loss: 0.6705958300774469\n",
            "\n",
            "\n",
            "Epoch 120 -\n",
            "Updated K params after epoch 120: [-0.19197075 -0.88853995]\n",
            "Train loss: 0.670135215277683, Test loss: 0.6700229858769893\n",
            "\n",
            "\n",
            "Epoch 121 -\n",
            "Updated K params after epoch 121: [-0.19197317 -0.88856772]\n",
            "Train loss: 0.6695507787562964, Test loss: 0.6694364546418137\n",
            "\n",
            "\n",
            "Epoch 122 -\n",
            "Updated K params after epoch 122: [-0.1919756  -0.88859623]\n",
            "Train loss: 0.6689524030029357, Test loss: 0.6688359950252386\n",
            "\n",
            "\n",
            "Epoch 123 -\n",
            "Updated K params after epoch 123: [-0.19197805 -0.88862552]\n",
            "Train loss: 0.6683398433848369, Test loss: 0.6682213670937674\n",
            "\n",
            "\n",
            "Epoch 124 -\n",
            "Updated K params after epoch 124: [-0.19198051 -0.88865559]\n",
            "Train loss: 0.667712856755411, Test loss: 0.667592332701871\n",
            "\n",
            "\n",
            "Epoch 125 -\n",
            "Updated K params after epoch 125: [-0.19198298 -0.88868646]\n",
            "Train loss: 0.6670712018376377, Test loss: 0.6669486558833022\n",
            "\n",
            "\n",
            "Epoch 126 -\n",
            "Updated K params after epoch 126: [-0.19198547 -0.88871815]\n",
            "Train loss: 0.6664146396244162, Test loss: 0.6662901032588291\n",
            "\n",
            "\n",
            "Epoch 127 -\n",
            "Updated K params after epoch 127: [-0.19198796 -0.88875068]\n",
            "Train loss: 0.6657429337957627, Test loss: 0.6656164444601786\n",
            "\n",
            "\n",
            "Epoch 128 -\n",
            "Updated K params after epoch 128: [-0.19199047 -0.88878406]\n",
            "Train loss: 0.6650558511526597, Test loss: 0.6649274525699056\n",
            "\n",
            "\n",
            "Epoch 129 -\n",
            "Updated K params after epoch 129: [-0.19199298 -0.88881831]\n",
            "Train loss: 0.6643531620672818, Test loss: 0.664222904576808\n",
            "\n",
            "\n",
            "Epoch 130 -\n",
            "Updated K params after epoch 130: [-0.19199551 -0.88885345]\n",
            "Train loss: 0.6636346409492209, Test loss: 0.6635025818464131\n",
            "\n",
            "\n",
            "Epoch 131 -\n",
            "Updated K params after epoch 131: [-0.19199804 -0.8888895 ]\n",
            "Train loss: 0.6629000667272489, Test loss: 0.6627662706059526\n",
            "\n",
            "\n",
            "Epoch 132 -\n",
            "Updated K params after epoch 132: [-0.19200058 -0.88892646]\n",
            "Train loss: 0.6621492233460371, Test loss: 0.6620137624431388\n",
            "\n",
            "\n",
            "Epoch 133 -\n",
            "Updated K params after epoch 133: [-0.19200313 -0.88896437]\n",
            "Train loss: 0.6613819002771548, Test loss: 0.6612448548179374\n",
            "\n",
            "\n",
            "Epoch 134 -\n",
            "Updated K params after epoch 134: [-0.19200568 -0.88900324]\n",
            "Train loss: 0.6605978930435404, Test loss: 0.6604593515864101\n",
            "\n",
            "\n",
            "Epoch 135 -\n",
            "Updated K params after epoch 135: [-0.19200824 -0.88904309]\n",
            "Train loss: 0.659797003756528, Test loss: 0.6596570635355808\n",
            "\n",
            "\n",
            "Epoch 136 -\n",
            "Updated K params after epoch 136: [-0.1920108  -0.88908393]\n",
            "Train loss: 0.6589790416643774, Test loss: 0.6588378089281398\n",
            "\n",
            "\n",
            "Epoch 137 -\n",
            "Updated K params after epoch 137: [-0.19201337 -0.88912578]\n",
            "Train loss: 0.6581438237111266, Test loss: 0.6580014140556755\n",
            "\n",
            "\n",
            "Epoch 138 -\n",
            "Updated K params after epoch 138: [-0.19201593 -0.88916867]\n",
            "Train loss: 0.6572911751044547, Test loss: 0.6571477137989779\n",
            "\n",
            "\n",
            "Epoch 139 -\n",
            "Updated K params after epoch 139: [-0.1920185 -0.8892126]\n",
            "Train loss: 0.6564209298910928, Test loss: 0.6562765521938274\n",
            "\n",
            "\n",
            "Epoch 140 -\n",
            "Updated K params after epoch 140: [-0.19202107 -0.88925759]\n",
            "Train loss: 0.655532931538191, Test loss: 0.6553877830005274\n",
            "\n",
            "\n",
            "Epoch 141 -\n",
            "Updated K params after epoch 141: [-0.19202364 -0.88930367]\n",
            "Train loss: 0.6546270335188955, Test loss: 0.6544812702753096\n",
            "\n",
            "\n",
            "Epoch 142 -\n",
            "Updated K params after epoch 142: [-0.19202621 -0.88935085]\n",
            "Train loss: 0.6537030999002512, Test loss: 0.6535568889415918\n",
            "\n",
            "\n",
            "Epoch 143 -\n",
            "Updated K params after epoch 143: [-0.19202877 -0.88939914]\n",
            "Train loss: 0.6527610059313974, Test loss: 0.652614525358926\n",
            "\n",
            "\n",
            "Epoch 144 -\n",
            "Updated K params after epoch 144: [-0.19203134 -0.88944856]\n",
            "Train loss: 0.6518006386298786, Test loss: 0.6516540778873385\n",
            "\n",
            "\n",
            "Epoch 145 -\n",
            "Updated K params after epoch 145: [-0.19203389 -0.88949914]\n",
            "Train loss: 0.650821897363757, Test loss: 0.6506754574446331\n",
            "\n",
            "\n",
            "Epoch 146 -\n",
            "Updated K params after epoch 146: [-0.19203644 -0.88955088]\n",
            "Train loss: 0.6498246944270685, Test loss: 0.649678588054089\n",
            "\n",
            "\n",
            "Epoch 147 -\n",
            "Updated K params after epoch 147: [-0.19203898 -0.8896038 ]\n",
            "Train loss: 0.6488089556060401, Test loss: 0.6486634073798779\n",
            "\n",
            "\n",
            "Epoch 148 -\n",
            "Updated K params after epoch 148: [-0.19204152 -0.88965792]\n",
            "Train loss: 0.6477746207333528, Test loss: 0.6476298672474028\n",
            "\n",
            "\n",
            "Epoch 149 -\n",
            "Updated K params after epoch 149: [-0.19204404 -0.88971326]\n",
            "Train loss: 0.6467216442276268, Test loss: 0.6465779341456568\n",
            "\n",
            "\n",
            "Epoch 150 -\n",
            "Updated K params after epoch 150: [-0.19204655 -0.88976982]\n",
            "Train loss: 0.645649995615194, Test loss: 0.6455075897086255\n",
            "\n",
            "\n",
            "Epoch 151 -\n",
            "Updated K params after epoch 151: [-0.19204905 -0.88982762]\n",
            "Train loss: 0.6445596600311333, Test loss: 0.6444188311726647\n",
            "\n",
            "\n",
            "Epoch 152 -\n",
            "Updated K params after epoch 152: [-0.19205153 -0.88988668]\n",
            "Train loss: 0.643450638696461, Test loss: 0.6433116718067362\n",
            "\n",
            "\n",
            "Epoch 153 -\n",
            "Updated K params after epoch 153: [-0.192054   -0.88994701]\n",
            "Train loss: 0.6423229493683096, Test loss: 0.6421861413123423\n",
            "\n",
            "\n",
            "Epoch 154 -\n",
            "Updated K params after epoch 154: [-0.19205645 -0.89000862]\n",
            "Train loss: 0.6411766267598817, Test loss: 0.6410422861899793\n",
            "\n",
            "\n",
            "Epoch 155 -\n",
            "Updated K params after epoch 155: [-0.19205888 -0.89007153]\n",
            "Train loss: 0.6400117229269401, Test loss: 0.6398801700689287\n",
            "\n",
            "\n",
            "Epoch 156 -\n",
            "Updated K params after epoch 156: [-0.1920613  -0.89013575]\n",
            "Train loss: 0.6388283076175892, Test loss: 0.6386998739972346\n",
            "\n",
            "\n",
            "Epoch 157 -\n",
            "Updated K params after epoch 157: [-0.19206368 -0.89020129]\n",
            "Train loss: 0.6376264685821316, Test loss: 0.6375014966887587\n",
            "\n",
            "\n",
            "Epoch 158 -\n",
            "Updated K params after epoch 158: [-0.19206605 -0.89026816]\n",
            "Train loss: 0.636406311839813, Test loss: 0.6362851547242914\n",
            "\n",
            "\n",
            "Epoch 159 -\n",
            "Updated K params after epoch 159: [-0.19206839 -0.89033637]\n",
            "Train loss: 0.63516796189936, Test loss: 0.6350509827037887\n",
            "\n",
            "\n",
            "Epoch 160 -\n",
            "Updated K params after epoch 160: [-0.1920707  -0.89040593]\n",
            "Train loss: 0.6339115619302962, Test loss: 0.6337991333469482\n",
            "\n",
            "\n",
            "Epoch 161 -\n",
            "Updated K params after epoch 161: [-0.19207298 -0.89047685]\n",
            "Train loss: 0.6326372738821592, Test loss: 0.6325297775394962\n",
            "\n",
            "\n",
            "Epoch 162 -\n",
            "Updated K params after epoch 162: [-0.19207523 -0.89054914]\n",
            "Train loss: 0.6313452785489038, Test loss: 0.6312431043227511\n",
            "\n",
            "\n",
            "Epoch 163 -\n",
            "Updated K params after epoch 163: [-0.19207745 -0.8906228 ]\n",
            "Train loss: 0.6300357755759511, Test loss: 0.6299393208242494\n",
            "\n",
            "\n",
            "Epoch 164 -\n",
            "Updated K params after epoch 164: [-0.19207964 -0.89069785]\n",
            "Train loss: 0.6287089834075786, Test loss: 0.6286186521274821\n",
            "\n",
            "\n",
            "Epoch 165 -\n",
            "Updated K params after epoch 165: [-0.19208178 -0.89077428]\n",
            "Train loss: 0.627365139172581, Test loss: 0.6272813410790599\n",
            "\n",
            "\n",
            "Epoch 166 -\n",
            "Updated K params after epoch 166: [-0.19208389 -0.8908521 ]\n",
            "Train loss: 0.626004498506424, Test loss: 0.6259276480319512\n",
            "\n",
            "\n",
            "Epoch 167 -\n",
            "Updated K params after epoch 167: [-0.19208596 -0.89093132]\n",
            "Train loss: 0.6246273353084104, Test loss: 0.6245578505237636\n",
            "\n",
            "\n",
            "Epoch 168 -\n",
            "Updated K params after epoch 168: [-0.19208799 -0.89101194]\n",
            "Train loss: 0.6232339414327245, Test loss: 0.6231722428894133\n",
            "\n",
            "\n",
            "Epoch 169 -\n",
            "Updated K params after epoch 169: [-0.19208997 -0.89109397]\n",
            "Train loss: 0.6218246263125751, Test loss: 0.62177113580791\n",
            "\n",
            "\n",
            "Epoch 170 -\n",
            "Updated K params after epoch 170: [-0.1920919 -0.8911774]\n",
            "Train loss: 0.6203997165170495, Test loss: 0.6203548557833927\n",
            "\n",
            "\n",
            "Epoch 171 -\n",
            "Updated K params after epoch 171: [-0.19209379 -0.89126223]\n",
            "Train loss: 0.6189595552406949, Test loss: 0.6189237445609799\n",
            "\n",
            "\n",
            "Epoch 172 -\n",
            "Updated K params after epoch 172: [-0.19209562 -0.89134847]\n",
            "Train loss: 0.6175045017262724, Test loss: 0.617478158478437\n",
            "\n",
            "\n",
            "Epoch 173 -\n",
            "Updated K params after epoch 173: [-0.1920974  -0.89143611]\n",
            "Train loss: 0.6160349306215658, Test loss: 0.6160184677551093\n",
            "\n",
            "\n",
            "Epoch 174 -\n",
            "Updated K params after epoch 174: [-0.19209913 -0.89152515]\n",
            "Train loss: 0.6145512312715833, Test loss: 0.6145450557200277\n",
            "\n",
            "\n",
            "Epoch 175 -\n",
            "Updated K params after epoch 175: [-0.1921008  -0.89161559]\n",
            "Train loss: 0.6130538069479469, Test loss: 0.6130583179815513\n",
            "\n",
            "\n",
            "Epoch 176 -\n",
            "Updated K params after epoch 176: [-0.19210241 -0.89170743]\n",
            "Train loss: 0.6115430740177192, Test loss: 0.6115586615413576\n",
            "\n",
            "\n",
            "Epoch 177 -\n",
            "Updated K params after epoch 177: [-0.19210396 -0.89180066]\n",
            "Train loss: 0.6100194610543881, Test loss: 0.6100465038560418\n",
            "\n",
            "\n",
            "Epoch 178 -\n",
            "Updated K params after epoch 178: [-0.19210544 -0.89189527]\n",
            "Train loss: 0.6084834078941657, Test loss: 0.6085222718500112\n",
            "\n",
            "\n",
            "Epoch 179 -\n",
            "Updated K params after epoch 179: [-0.19210686 -0.89199125]\n",
            "Train loss: 0.6069353646412057, Test loss: 0.6069864008837772\n",
            "\n",
            "\n",
            "Epoch 180 -\n",
            "Updated K params after epoch 180: [-0.19210821 -0.89208861]\n",
            "Train loss: 0.6053757906257615, Test loss: 0.6054393336821274\n",
            "\n",
            "\n",
            "Epoch 181 -\n",
            "Updated K params after epoch 181: [-0.19210949 -0.89218733]\n",
            "Train loss: 0.6038051533197009, Test loss: 0.6038815192270264\n",
            "\n",
            "\n",
            "Epoch 182 -\n",
            "Updated K params after epoch 182: [-0.19211069 -0.8922874 ]\n",
            "Train loss: 0.6022239272141693, Test loss: 0.602313411620414\n",
            "\n",
            "\n",
            "Epoch 183 -\n",
            "Updated K params after epoch 183: [-0.19211183 -0.89238882]\n",
            "Train loss: 0.600632592664525, Test loss: 0.6007354689223567\n",
            "\n",
            "\n",
            "Epoch 184 -\n",
            "Updated K params after epoch 184: [-0.19211288 -0.89249156]\n",
            "Train loss: 0.5990316347079763, Test loss: 0.5991481519702554\n",
            "\n",
            "\n",
            "Epoch 185 -\n",
            "Updated K params after epoch 185: [-0.19211385 -0.89259563]\n",
            "Train loss: 0.5974215418596024, Test loss: 0.5975519231850074\n",
            "\n",
            "\n",
            "Epoch 186 -\n",
            "Updated K params after epoch 186: [-0.19211475 -0.892701  ]\n",
            "Train loss: 0.59580280489266, Test loss: 0.5959472453701776\n",
            "\n",
            "\n",
            "Epoch 187 -\n",
            "Updated K params after epoch 187: [-0.19211556 -0.89280767]\n",
            "Train loss: 0.5941759156092459, Test loss: 0.5943345805103246\n",
            "\n",
            "\n",
            "Epoch 188 -\n",
            "Updated K params after epoch 188: [-0.19211628 -0.89291562]\n",
            "Train loss: 0.592541365607491, Test loss: 0.5927143885746817\n",
            "\n",
            "\n",
            "Epoch 189 -\n",
            "Updated K params after epoch 189: [-0.19211691 -0.89302483]\n",
            "Train loss: 0.5908996450515407, Test loss: 0.5910871263323819\n",
            "\n",
            "\n",
            "Epoch 190 -\n",
            "Updated K params after epoch 190: [-0.19211746 -0.8931353 ]\n",
            "Train loss: 0.589251241450571, Test loss: 0.5894532461853527\n",
            "\n",
            "\n",
            "Epoch 191 -\n",
            "Updated K params after epoch 191: [-0.19211791 -0.893247  ]\n",
            "Train loss: 0.5875966384530564, Test loss: 0.5878131950248914\n",
            "\n",
            "\n",
            "Epoch 192 -\n",
            "Updated K params after epoch 192: [-0.19211827 -0.89335992]\n",
            "Train loss: 0.5859363146624018, Test loss: 0.5861674131177697\n",
            "\n",
            "\n",
            "Epoch 193 -\n",
            "Updated K params after epoch 193: [-0.19211853 -0.89347404]\n",
            "Train loss: 0.5842707424798969, Test loss: 0.5845163330274801\n",
            "\n",
            "\n",
            "Epoch 194 -\n",
            "Updated K params after epoch 194: [-0.19211869 -0.89358934]\n",
            "Train loss: 0.5826003869807553, Test loss: 0.5828603785759876\n",
            "\n",
            "\n",
            "Epoch 195 -\n",
            "Updated K params after epoch 195: [-0.19211875 -0.89370581]\n",
            "Train loss: 0.5809257048287386, Test loss: 0.5811999638510209\n",
            "\n",
            "\n",
            "Epoch 196 -\n",
            "Updated K params after epoch 196: [-0.19211871 -0.89382343]\n",
            "Train loss: 0.5792471432345736, Test loss: 0.5795354922635891\n",
            "\n",
            "\n",
            "Epoch 197 -\n",
            "Updated K params after epoch 197: [-0.19211857 -0.89394218]\n",
            "Train loss: 0.5775651389630249, Test loss: 0.5778673556600176\n",
            "\n",
            "\n",
            "Epoch 198 -\n",
            "Updated K params after epoch 198: [-0.19211832 -0.89406204]\n",
            "Train loss: 0.5758801173931057, Test loss: 0.576195933492365\n",
            "\n",
            "\n",
            "Epoch 199 -\n",
            "Updated K params after epoch 199: [-0.19211795 -0.89418299]\n",
            "Train loss: 0.5741924916354961, Test loss: 0.5745215920506379\n",
            "\n",
            "\n",
            "Epoch 200 -\n",
            "Updated K params after epoch 200: [-0.19211748 -0.89430502]\n",
            "Train loss: 0.572502661710791, Test loss: 0.5728446837597387\n",
            "\n",
            "\n",
            "Epoch 201 -\n",
            "Updated K params after epoch 201: [-0.1921169 -0.8944281]\n",
            "Train loss: 0.5708110137917376, Test loss: 0.5711655465435963\n",
            "\n",
            "\n",
            "Epoch 202 -\n",
            "Updated K params after epoch 202: [-0.1921162  -0.89455221]\n",
            "Train loss: 0.569117919512134, Test loss: 0.5694845032584221\n",
            "\n",
            "\n",
            "Epoch 203 -\n",
            "Updated K params after epoch 203: [-0.19211539 -0.89467734]\n",
            "Train loss: 0.5674237353445601, Test loss: 0.567801861196531\n",
            "\n",
            "\n",
            "Epoch 204 -\n",
            "Updated K params after epoch 204: [-0.19211446 -0.89480346]\n",
            "Train loss: 0.5657288020486145, Test loss: 0.5661179116616605\n",
            "\n",
            "\n",
            "Epoch 205 -\n",
            "Updated K params after epoch 205: [-0.19211341 -0.89493056]\n",
            "Train loss: 0.5640334441908178, Test loss: 0.5644329296162194\n",
            "\n",
            "\n",
            "Epoch 206 -\n",
            "Updated K params after epoch 206: [-0.19211224 -0.89505862]\n",
            "Train loss: 0.5623379697368445, Test loss: 0.5627471734004117\n",
            "\n",
            "\n",
            "Epoch 207 -\n",
            "Updated K params after epoch 207: [-0.19211095 -0.89518761]\n",
            "Train loss: 0.5606426697162562, Test loss: 0.5610608845227072\n",
            "\n",
            "\n",
            "Epoch 208 -\n",
            "Updated K params after epoch 208: [-0.19210953 -0.89531753]\n",
            "Train loss: 0.5589478179594237, Test loss: 0.5593742875206817\n",
            "\n",
            "\n",
            "Epoch 209 -\n",
            "Updated K params after epoch 209: [-0.19210799 -0.89544834]\n",
            "Train loss: 0.5572536709058767, Test loss: 0.5576875898908205\n",
            "\n",
            "\n",
            "Epoch 210 -\n",
            "Updated K params after epoch 210: [-0.19210632 -0.89558003]\n",
            "Train loss: 0.5555604674828758, Test loss: 0.556000982085481\n",
            "\n",
            "\n",
            "Epoch 211 -\n",
            "Updated K params after epoch 211: [-0.19210453 -0.89571259]\n",
            "Train loss: 0.5538684290525979, Test loss: 0.5543146375748478\n",
            "\n",
            "\n",
            "Epoch 212 -\n",
            "Updated K params after epoch 212: [-0.1921026  -0.89584599]\n",
            "Train loss: 0.5521777594259455, Test loss: 0.5526287129713738\n",
            "\n",
            "\n",
            "Epoch 213 -\n",
            "Updated K params after epoch 213: [-0.19210055 -0.89598021]\n",
            "Train loss: 0.5504886449406431, Test loss: 0.5509433482139054\n",
            "\n",
            "\n",
            "Epoch 214 -\n",
            "Updated K params after epoch 214: [-0.19209836 -0.89611525]\n",
            "Train loss: 0.5488012546009754, Test loss: 0.5492586668084363\n",
            "\n",
            "\n",
            "Epoch 215 -\n",
            "Updated K params after epoch 215: [-0.19209604 -0.89625108]\n",
            "Train loss: 0.5471157402762415, Test loss: 0.5475747761221963\n",
            "\n",
            "\n",
            "Epoch 216 -\n",
            "Updated K params after epoch 216: [-0.19209359 -0.89638769]\n",
            "Train loss: 0.5454322369547643, Test loss: 0.5458917677276145\n",
            "\n",
            "\n",
            "Epoch 217 -\n",
            "Updated K params after epoch 217: [-0.192091   -0.89652506]\n",
            "Train loss: 0.5437508630500899, Test loss: 0.5442097177925339\n",
            "\n",
            "\n",
            "Epoch 218 -\n",
            "Updated K params after epoch 218: [-0.19208828 -0.89666317]\n",
            "Train loss: 0.5420717207558531, Test loss: 0.5425286875129522\n",
            "\n",
            "\n",
            "Epoch 219 -\n",
            "Updated K params after epoch 219: [-0.19208542 -0.89680202]\n",
            "Train loss: 0.5403948964456478, Test loss: 0.5408487235844882\n",
            "\n",
            "\n",
            "Epoch 220 -\n",
            "Updated K params after epoch 220: [-0.19208242 -0.89694158]\n",
            "Train loss: 0.5387204611141633, Test loss: 0.5391698587087306\n",
            "\n",
            "\n",
            "Epoch 221 -\n",
            "Updated K params after epoch 221: [-0.19207928 -0.89708185]\n",
            "Train loss: 0.5370484708557801, Test loss: 0.5374921121306218\n",
            "\n",
            "\n",
            "Epoch 222 -\n",
            "Updated K params after epoch 222: [-0.19207601 -0.8972228 ]\n",
            "Train loss: 0.5353789673767979, Test loss: 0.5358154902030502\n",
            "\n",
            "\n",
            "Epoch 223 -\n",
            "Updated K params after epoch 223: [-0.19207259 -0.89736444]\n",
            "Train loss: 0.5337119785374814, Test loss: 0.5341399869748784\n",
            "\n",
            "\n",
            "Epoch 224 -\n",
            "Updated K params after epoch 224: [-0.19206904 -0.89750674]\n",
            "Train loss: 0.5320475189201391, Test loss: 0.5324655847987085\n",
            "\n",
            "\n",
            "Epoch 225 -\n",
            "Updated K params after epoch 225: [-0.19206534 -0.89764969]\n",
            "Train loss: 0.5303855904195149, Test loss: 0.5307922549547904\n",
            "\n",
            "\n",
            "Epoch 226 -\n",
            "Updated K params after epoch 226: [-0.1920615  -0.89779329]\n",
            "Train loss: 0.5287261828518627, Test loss: 0.529119958287598\n",
            "\n",
            "\n",
            "Epoch 227 -\n",
            "Updated K params after epoch 227: [-0.19205752 -0.89793753]\n",
            "Train loss: 0.5270692745791793, Test loss: 0.5274486458517359\n",
            "\n",
            "\n",
            "Epoch 228 -\n",
            "Updated K params after epoch 228: [-0.1920534  -0.89808239]\n",
            "Train loss: 0.5254148331451993, Test loss: 0.5257782595639967\n",
            "\n",
            "\n",
            "Epoch 229 -\n",
            "Updated K params after epoch 229: [-0.19204914 -0.89822786]\n",
            "Train loss: 0.5237628159198985, Test loss: 0.524108732858553\n",
            "\n",
            "\n",
            "Epoch 230 -\n",
            "Updated K params after epoch 230: [-0.19204473 -0.89837395]\n",
            "Train loss: 0.5221131707494121, Test loss: 0.522439991342446\n",
            "\n",
            "\n",
            "Epoch 231 -\n",
            "Updated K params after epoch 231: [-0.19204018 -0.89852063]\n",
            "Train loss: 0.5204658366084405, Test loss: 0.5207719534487181\n",
            "\n",
            "\n",
            "Epoch 232 -\n",
            "Updated K params after epoch 232: [-0.19203548 -0.89866791]\n",
            "Train loss: 0.5188207442523912, Test loss: 0.5191045310847239\n",
            "\n",
            "\n",
            "Epoch 233 -\n",
            "Updated K params after epoch 233: [-0.19203064 -0.89881578]\n",
            "Train loss: 0.5171778168666943, Test loss: 0.5174376302733488\n",
            "\n",
            "\n",
            "Epoch 234 -\n",
            "Updated K params after epoch 234: [-0.19202566 -0.89896423]\n",
            "Train loss: 0.5155369707109076, Test loss: 0.51577115178506\n",
            "\n",
            "\n",
            "Epoch 235 -\n",
            "Updated K params after epoch 235: [-0.19202053 -0.89911326]\n",
            "Train loss: 0.5138981157554295, Test loss: 0.5141049917588997\n",
            "\n",
            "\n",
            "Epoch 236 -\n",
            "Updated K params after epoch 236: [-0.19201526 -0.89926286]\n",
            "Train loss: 0.5122611563088149, Test loss: 0.5124390423107351\n",
            "\n",
            "\n",
            "Epoch 237 -\n",
            "Updated K params after epoch 237: [-0.19200984 -0.89941303]\n",
            "Train loss: 0.5106259916338877, Test loss: 0.5107731921272485\n",
            "\n",
            "\n",
            "Epoch 238 -\n",
            "Updated K params after epoch 238: [-0.19200428 -0.89956377]\n",
            "Train loss: 0.5089925165510257, Test loss: 0.5091073270443502\n",
            "\n",
            "\n",
            "Epoch 239 -\n",
            "Updated K params after epoch 239: [-0.19199857 -0.89971507]\n",
            "Train loss: 0.5073606220271736, Test loss: 0.5074413306088597\n",
            "\n",
            "\n",
            "Epoch 240 -\n",
            "Updated K params after epoch 240: [-0.19199272 -0.89986693]\n",
            "Train loss: 0.5057301957493178, Test loss: 0.505775084622472\n",
            "\n",
            "\n",
            "Epoch 241 -\n",
            "Updated K params after epoch 241: [-0.19198673 -0.90001936]\n",
            "Train loss: 0.5041011226813278, Test loss: 0.5041084696671908\n",
            "\n",
            "\n",
            "Epoch 242 -\n",
            "Updated K params after epoch 242: [-0.19198059 -0.90017234]\n",
            "Train loss: 0.5024732856032254, Test loss: 0.5024413656115542\n",
            "\n",
            "\n",
            "Epoch 243 -\n",
            "Updated K params after epoch 243: [-0.1919743  -0.90032588]\n",
            "Train loss: 0.5008465656321033, Test loss: 0.500773652097124\n",
            "\n",
            "\n",
            "Epoch 244 -\n",
            "Updated K params after epoch 244: [-0.19196788 -0.90047999]\n",
            "Train loss: 0.4992208427240572, Test loss: 0.49910520900484634\n",
            "\n",
            "\n",
            "Epoch 245 -\n",
            "Updated K params after epoch 245: [-0.19196131 -0.90063465]\n",
            "Train loss: 0.497595996156631, Test loss: 0.4974359169010073\n",
            "\n",
            "\n",
            "Epoch 246 -\n",
            "Updated K params after epoch 246: [-0.19195459 -0.90078988]\n",
            "Train loss: 0.4959719049914057, Test loss: 0.49576565746262635\n",
            "\n",
            "\n",
            "Epoch 247 -\n",
            "Updated K params after epoch 247: [-0.19194774 -0.90094566]\n",
            "Train loss: 0.4943484485164771, Test loss: 0.4940943138822333\n",
            "\n",
            "\n",
            "Epoch 248 -\n",
            "Updated K params after epoch 248: [-0.19194074 -0.90110201]\n",
            "Train loss: 0.49272550666867954, Test loss: 0.4924217712520658\n",
            "\n",
            "\n",
            "Epoch 249 -\n",
            "Updated K params after epoch 249: [-0.19193359 -0.90125893]\n",
            "Train loss: 0.4911029604355078, Test loss: 0.49074791692781444\n",
            "\n",
            "\n",
            "Epoch 250 -\n",
            "Updated K params after epoch 250: [-0.19192631 -0.90141642]\n",
            "Train loss: 0.4894806922367854, Test loss: 0.4890726408721139\n",
            "\n",
            "\n",
            "Epoch 251 -\n",
            "Updated K params after epoch 251: [-0.19191888 -0.90157448]\n",
            "Train loss: 0.48785858628620404, Test loss: 0.48739583597804725\n",
            "\n",
            "\n",
            "Epoch 252 -\n",
            "Updated K params after epoch 252: [-0.19191132 -0.90173311]\n",
            "Train loss: 0.4862365289329318, Test loss: 0.4857173983729921\n",
            "\n",
            "\n",
            "Epoch 253 -\n",
            "Updated K params after epoch 253: [-0.19190361 -0.90189232]\n",
            "Train loss: 0.48461440898355657, Test loss: 0.4840372277031807\n",
            "\n",
            "\n",
            "Epoch 254 -\n",
            "Updated K params after epoch 254: [-0.19189576 -0.90205212]\n",
            "Train loss: 0.4829921180046806, Test loss: 0.4823552273993999\n",
            "\n",
            "\n",
            "Epoch 255 -\n",
            "Updated K params after epoch 255: [-0.19188778 -0.9022125 ]\n",
            "Train loss: 0.48136955060653636, Test loss: 0.48067130492428056\n",
            "\n",
            "\n",
            "Epoch 256 -\n",
            "Updated K params after epoch 256: [-0.19187965 -0.90237348]\n",
            "Train loss: 0.47974660470803066, Test loss: 0.4789853720016673\n",
            "\n",
            "\n",
            "Epoch 257 -\n",
            "Updated K params after epoch 257: [-0.19187138 -0.90253505]\n",
            "Train loss: 0.47812318178366336, Test loss: 0.47729734482857245\n",
            "\n",
            "\n",
            "Epoch 258 -\n",
            "Updated K params after epoch 258: [-0.19186298 -0.90269722]\n",
            "Train loss: 0.4764991870927902, Test loss: 0.4756071442702445\n",
            "\n",
            "\n",
            "Epoch 259 -\n",
            "Updated K params after epoch 259: [-0.19185444 -0.90286   ]\n",
            "Train loss: 0.4748745298917263, Test loss: 0.47391469603888503\n",
            "\n",
            "\n",
            "Epoch 260 -\n",
            "Updated K params after epoch 260: [-0.19184576 -0.90302339]\n",
            "Train loss: 0.47324912362919985, Test loss: 0.47221993085656444\n",
            "\n",
            "\n",
            "Epoch 261 -\n",
            "Updated K params after epoch 261: [-0.19183694 -0.9031874 ]\n",
            "Train loss: 0.4716228861256826, Test loss: 0.47052278460288255\n",
            "\n",
            "\n",
            "Epoch 262 -\n",
            "Updated K params after epoch 262: [-0.19182799 -0.90335203]\n",
            "Train loss: 0.4699957397371255, Test loss: 0.46882319844792414\n",
            "\n",
            "\n",
            "Epoch 263 -\n",
            "Updated K params after epoch 263: [-0.19181891 -0.90351729]\n",
            "Train loss: 0.46836761150363787, Test loss: 0.4671211189710545\n",
            "\n",
            "\n",
            "Epoch 264 -\n",
            "Updated K params after epoch 264: [-0.19180969 -0.90368318]\n",
            "Train loss: 0.46673843328364245, Test loss: 0.4654164982660943\n",
            "\n",
            "\n",
            "Epoch 265 -\n",
            "Updated K params after epoch 265: [-0.19180033 -0.90384971]\n",
            "Train loss: 0.4651081418740403, Test loss: 0.46370929403339867\n",
            "\n",
            "\n",
            "Epoch 266 -\n",
            "Updated K params after epoch 266: [-0.19179085 -0.90401689]\n",
            "Train loss: 0.46347667911691315, Test loss: 0.46199946965936267\n",
            "\n",
            "\n",
            "Epoch 267 -\n",
            "Updated K params after epoch 267: [-0.19178122 -0.90418472]\n",
            "Train loss: 0.4618439919932764, Test loss: 0.46028699428385317\n",
            "\n",
            "\n",
            "Epoch 268 -\n",
            "Updated K params after epoch 268: [-0.19177147 -0.90435321]\n",
            "Train loss: 0.460210032704396, Test loss: 0.45857184285605734\n",
            "\n",
            "\n",
            "Epoch 269 -\n",
            "Updated K params after epoch 269: [-0.19176159 -0.90452236]\n",
            "Train loss: 0.45857475874115994, Test loss: 0.4568539961792199\n",
            "\n",
            "\n",
            "Epoch 270 -\n",
            "Updated K params after epoch 270: [-0.19175158 -0.90469218]\n",
            "Train loss: 0.45693813294198926, Test loss: 0.4551334409447236\n",
            "\n",
            "\n",
            "Epoch 271 -\n",
            "Updated K params after epoch 271: [-0.19174143 -0.90486268]\n",
            "Train loss: 0.4553001235397523, Test loss: 0.45341016975595266\n",
            "\n",
            "\n",
            "Epoch 272 -\n",
            "Updated K params after epoch 272: [-0.19173116 -0.90503386]\n",
            "Train loss: 0.4536607041981364, Test loss: 0.451684181142355\n",
            "\n",
            "\n",
            "Epoch 273 -\n",
            "Updated K params after epoch 273: [-0.19172076 -0.90520572]\n",
            "Train loss: 0.4520198540379085, Test loss: 0.4499554795641063\n",
            "\n",
            "\n",
            "Epoch 274 -\n",
            "Updated K params after epoch 274: [-0.19171023 -0.90537828]\n",
            "Train loss: 0.45037755765348575, Test loss: 0.44822407540775816\n",
            "\n",
            "\n",
            "Epoch 275 -\n",
            "Updated K params after epoch 275: [-0.19169958 -0.90555154]\n",
            "Train loss: 0.4487338051202139, Test loss: 0.4464899849732319\n",
            "\n",
            "\n",
            "Epoch 276 -\n",
            "Updated K params after epoch 276: [-0.19168879 -0.9057255 ]\n",
            "Train loss: 0.44708859199273965, Test loss: 0.44475323045250537\n",
            "\n",
            "\n",
            "Epoch 277 -\n",
            "Updated K params after epoch 277: [-0.19167789 -0.90590017]\n",
            "Train loss: 0.4454419192948423, Test loss: 0.4430138399003216\n",
            "\n",
            "\n",
            "Epoch 278 -\n",
            "Updated K params after epoch 278: [-0.19166686 -0.90607555]\n",
            "Train loss: 0.4437937935010752, Test loss: 0.44127184719722756\n",
            "\n",
            "\n",
            "Epoch 279 -\n",
            "Updated K params after epoch 279: [-0.1916557  -0.90625166]\n",
            "Train loss: 0.4421442265105508, Test loss: 0.43952729200524077\n",
            "\n",
            "\n",
            "Epoch 280 -\n",
            "Updated K params after epoch 280: [-0.19164443 -0.90642849]\n",
            "Train loss: 0.44049323561318754, Test loss: 0.4377802197164207\n",
            "\n",
            "\n",
            "Epoch 281 -\n",
            "Updated K params after epoch 281: [-0.19163303 -0.90660605]\n",
            "Train loss: 0.4388408434487208, Test loss: 0.4360306813946109\n",
            "\n",
            "\n",
            "Epoch 282 -\n",
            "Updated K params after epoch 282: [-0.19162151 -0.90678434]\n",
            "Train loss: 0.43718707795876677, Test loss: 0.4342787337106013\n",
            "\n",
            "\n",
            "Epoch 283 -\n",
            "Updated K params after epoch 283: [-0.19160987 -0.90696338]\n",
            "Train loss: 0.4355319723322143, Test loss: 0.4325244388709493\n",
            "\n",
            "\n",
            "Epoch 284 -\n",
            "Updated K params after epoch 284: [-0.19159811 -0.90714315]\n",
            "Train loss: 0.43387556494420476, Test loss: 0.4307678645406862\n",
            "\n",
            "\n",
            "Epoch 285 -\n",
            "Updated K params after epoch 285: [-0.19158623 -0.90732368]\n",
            "Train loss: 0.4322178992889528, Test loss: 0.42900908376012176\n",
            "\n",
            "\n",
            "Epoch 286 -\n",
            "Updated K params after epoch 286: [-0.19157424 -0.90750495]\n",
            "Train loss: 0.4305590239066435, Test loss: 0.42724817485595645\n",
            "\n",
            "\n",
            "Epoch 287 -\n",
            "Updated K params after epoch 287: [-0.19156213 -0.90768698]\n",
            "Train loss: 0.4288989923046364, Test loss: 0.42548522134689404\n",
            "\n",
            "\n",
            "Epoch 288 -\n",
            "Updated K params after epoch 288: [-0.1915499  -0.90786977]\n",
            "Train loss: 0.42723786287319554, Test loss: 0.42372031184394754\n",
            "\n",
            "\n",
            "Epoch 289 -\n",
            "Updated K params after epoch 289: [-0.19153756 -0.90805332]\n",
            "Train loss: 0.42557569879595614, Test loss: 0.4219535399456187\n",
            "\n",
            "\n",
            "Epoch 290 -\n",
            "Updated K params after epoch 290: [-0.1915251  -0.90823763]\n",
            "Train loss: 0.42391256795533266, Test loss: 0.4201850041281294\n",
            "\n",
            "\n",
            "Epoch 291 -\n",
            "Updated K params after epoch 291: [-0.19151253 -0.90842271]\n",
            "Train loss: 0.42224854283306423, Test loss: 0.41841480763087907\n",
            "\n",
            "\n",
            "Epoch 292 -\n",
            "Updated K params after epoch 292: [-0.19149985 -0.90860856]\n",
            "Train loss: 0.42058370040609133, Test loss: 0.4166430583372944\n",
            "\n",
            "\n",
            "Epoch 293 -\n",
            "Updated K params after epoch 293: [-0.19148705 -0.90879518]\n",
            "Train loss: 0.4189181220379504, Test loss: 0.4148698686512416\n",
            "\n",
            "\n",
            "Epoch 294 -\n",
            "Updated K params after epoch 294: [-0.19147415 -0.90898257]\n",
            "Train loss: 0.41725189336587015, Test loss: 0.4130953553691646\n",
            "\n",
            "\n",
            "Epoch 295 -\n",
            "Updated K params after epoch 295: [-0.19146114 -0.90917073]\n",
            "Train loss: 0.4155851041837511, Test loss: 0.41131963954811396\n",
            "\n",
            "\n",
            "Epoch 296 -\n",
            "Updated K params after epoch 296: [-0.19144801 -0.90935967]\n",
            "Train loss: 0.41391784832120765, Test loss: 0.40954284636983157\n",
            "\n",
            "\n",
            "Epoch 297 -\n",
            "Updated K params after epoch 297: [-0.19143478 -0.90954938]\n",
            "Train loss: 0.4122502235188505, Test loss: 0.407765105001056\n",
            "\n",
            "\n",
            "Epoch 298 -\n",
            "Updated K params after epoch 298: [-0.19142145 -0.90973987]\n",
            "Train loss: 0.4105823312999864, Test loss: 0.4059865484502158\n",
            "\n",
            "\n",
            "Epoch 299 -\n",
            "Updated K params after epoch 299: [-0.191408   -0.90993114]\n",
            "Train loss: 0.4089142768389134, Test loss: 0.4042073134206798\n",
            "\n",
            "\n",
            "Epoch 300 -\n",
            "Updated K params after epoch 300: [-0.19139446 -0.91012318]\n",
            "Train loss: 0.40724616882598974, Test loss: 0.40242754016073645\n",
            "\n",
            "\n",
            "Epoch 301 -\n",
            "Updated K params after epoch 301: [-0.1913808  -0.91031599]\n",
            "Train loss: 0.40557811932965554, Test loss: 0.40064737231047703\n",
            "\n",
            "\n",
            "Epoch 302 -\n",
            "Updated K params after epoch 302: [-0.19136705 -0.91050958]\n",
            "Train loss: 0.4039102436555887, Test loss: 0.3988669567457616\n",
            "\n",
            "\n",
            "Epoch 303 -\n",
            "Updated K params after epoch 303: [-0.19135319 -0.91070395]\n",
            "Train loss: 0.40224266020317945, Test loss: 0.3970864434194515\n",
            "\n",
            "\n",
            "Epoch 304 -\n",
            "Updated K params after epoch 304: [-0.19133924 -0.91089908]\n",
            "Train loss: 0.4005754903195077, Test loss: 0.39530598520009386\n",
            "\n",
            "\n",
            "Epoch 305 -\n",
            "Updated K params after epoch 305: [-0.19132518 -0.91109499]\n",
            "Train loss: 0.3989088581510142, Test loss: 0.39352573770825305\n",
            "\n",
            "\n",
            "Epoch 306 -\n",
            "Updated K params after epoch 306: [-0.19131102 -0.91129166]\n",
            "Train loss: 0.39724289049305556, Test loss: 0.3917458591506825\n",
            "\n",
            "\n",
            "Epoch 307 -\n",
            "Updated K params after epoch 307: [-0.19129677 -0.9114891 ]\n",
            "Train loss: 0.39557771663754016, Test loss: 0.3899665101525414\n",
            "\n",
            "\n",
            "Epoch 308 -\n",
            "Updated K params after epoch 308: [-0.19128241 -0.9116873 ]\n",
            "Train loss: 0.3939134682188419, Test loss: 0.3881878535878615\n",
            "\n",
            "\n",
            "Epoch 309 -\n",
            "Updated K params after epoch 309: [-0.19126797 -0.91188626]\n",
            "Train loss: 0.3922502790581942, Test loss: 0.38641005440847637\n",
            "\n",
            "\n",
            "Epoch 310 -\n",
            "Updated K params after epoch 310: [-0.19125342 -0.91208597]\n",
            "Train loss: 0.39058828500677123, Test loss: 0.38463327947162784\n",
            "\n",
            "\n",
            "Epoch 311 -\n",
            "Updated K params after epoch 311: [-0.19123879 -0.91228644]\n",
            "Train loss: 0.38892762378766166, Test loss: 0.38285769736647385\n",
            "\n",
            "\n",
            "Epoch 312 -\n",
            "Updated K params after epoch 312: [-0.19122406 -0.91248765]\n",
            "Train loss: 0.3872684348369505, Test loss: 0.38108347823971894\n",
            "\n",
            "\n",
            "Epoch 313 -\n",
            "Updated K params after epoch 313: [-0.19120923 -0.91268961]\n",
            "Train loss: 0.3856108591441205, Test loss: 0.3793107936206006\n",
            "\n",
            "\n",
            "Epoch 314 -\n",
            "Updated K params after epoch 314: [-0.19119432 -0.91289231]\n",
            "Train loss: 0.38395503909199297, Test loss: 0.3775398162454627\n",
            "\n",
            "\n",
            "Epoch 315 -\n",
            "Updated K params after epoch 315: [-0.19117932 -0.91309574]\n",
            "Train loss: 0.38230111829642605, Test loss: 0.375770719882152\n",
            "\n",
            "\n",
            "Epoch 316 -\n",
            "Updated K params after epoch 316: [-0.19116422 -0.9132999 ]\n",
            "Train loss: 0.3806492414459937, Test loss: 0.37400367915447974\n",
            "\n",
            "\n",
            "Epoch 317 -\n",
            "Updated K params after epoch 317: [-0.19114904 -0.91350477]\n",
            "Train loss: 0.37899955414186826, Test loss: 0.37223886936698697\n",
            "\n",
            "\n",
            "Epoch 318 -\n",
            "Updated K params after epoch 318: [-0.19113378 -0.91371037]\n",
            "Train loss: 0.3773522027381305, Test loss: 0.37047646633026016\n",
            "\n",
            "\n",
            "Epoch 319 -\n",
            "Updated K params after epoch 319: [-0.19111842 -0.91391668]\n",
            "Train loss: 0.37570733418273455, Test loss: 0.368716646187039\n",
            "\n",
            "\n",
            "Epoch 320 -\n",
            "Updated K params after epoch 320: [-0.19110298 -0.91412368]\n",
            "Train loss: 0.3740650958593507, Test loss: 0.3669595852393648\n",
            "\n",
            "\n",
            "Epoch 321 -\n",
            "Updated K params after epoch 321: [-0.19108746 -0.91433139]\n",
            "Train loss: 0.37242563543031365, Test loss: 0.36520545977701224\n",
            "\n",
            "\n",
            "Epoch 322 -\n",
            "Updated K params after epoch 322: [-0.19107186 -0.91453978]\n",
            "Train loss: 0.37078910068089876, Test loss: 0.3634544459074516\n",
            "\n",
            "\n",
            "Epoch 323 -\n",
            "Updated K params after epoch 323: [-0.19105617 -0.91474885]\n",
            "Train loss: 0.36915563936514956, Test loss: 0.3617067193875818\n",
            "\n",
            "\n",
            "Epoch 324 -\n",
            "Updated K params after epoch 324: [-0.1910404 -0.9149586]\n",
            "Train loss: 0.36752539905347764, Test loss: 0.35996245545747846\n",
            "\n",
            "\n",
            "Epoch 325 -\n",
            "Updated K params after epoch 325: [-0.19102456 -0.91516901]\n",
            "Train loss: 0.36589852698225084, Test loss: 0.3582218286763924\n",
            "\n",
            "\n",
            "Epoch 326 -\n",
            "Updated K params after epoch 326: [-0.19100863 -0.91538008]\n",
            "Train loss: 0.3642751699055861, Test loss: 0.35648501276123523\n",
            "\n",
            "\n",
            "Epoch 327 -\n",
            "Updated K params after epoch 327: [-0.19099263 -0.91559179]\n",
            "Train loss: 0.3626554739495563, Test loss: 0.3547521804277806\n",
            "\n",
            "\n",
            "Epoch 328 -\n",
            "Updated K params after epoch 328: [-0.19097655 -0.91580415]\n",
            "Train loss: 0.3610395844690152, Test loss: 0.35302350323480713\n",
            "\n",
            "\n",
            "Epoch 329 -\n",
            "Updated K params after epoch 329: [-0.1909604  -0.91601713]\n",
            "Train loss: 0.35942764590724324, Test loss: 0.3512991514314026\n",
            "\n",
            "\n",
            "Epoch 330 -\n",
            "Updated K params after epoch 330: [-0.19094417 -0.91623074]\n",
            "Train loss: 0.35781980165860583, Test loss: 0.3495792938076405\n",
            "\n",
            "\n",
            "Epoch 331 -\n",
            "Updated K params after epoch 331: [-0.19092787 -0.91644495]\n",
            "Train loss: 0.3562161939344129, Test loss: 0.3478640975488372\n",
            "\n",
            "\n",
            "Epoch 332 -\n",
            "Updated K params after epoch 332: [-0.19091149 -0.91665976]\n",
            "Train loss: 0.35461696363215955, Test loss: 0.34615372809358436\n",
            "\n",
            "\n",
            "Epoch 333 -\n",
            "Updated K params after epoch 333: [-0.19089505 -0.91687517]\n",
            "Train loss: 0.35302225020832056, Test loss: 0.34444834899574805\n",
            "\n",
            "\n",
            "Epoch 334 -\n",
            "Updated K params after epoch 334: [-0.19087853 -0.91709115]\n",
            "Train loss: 0.3514321915548631, Test loss: 0.3427481217906142\n",
            "\n",
            "\n",
            "Epoch 335 -\n",
            "Updated K params after epoch 335: [-0.19086195 -0.91730771]\n",
            "Train loss: 0.34984692387963195, Test loss: 0.3410532058653496\n",
            "\n",
            "\n",
            "Epoch 336 -\n",
            "Updated K params after epoch 336: [-0.1908453  -0.91752482]\n",
            "Train loss: 0.3482665815907549, Test loss: 0.3393637583339399\n",
            "\n",
            "\n",
            "Epoch 337 -\n",
            "Updated K params after epoch 337: [-0.19082858 -0.91774248]\n",
            "Train loss: 0.34669129718520403, Test loss: 0.3376799339167538\n",
            "\n",
            "\n",
            "Epoch 338 -\n",
            "Updated K params after epoch 338: [-0.1908118  -0.91796068]\n",
            "Train loss: 0.3451212011416382, Test loss: 0.33600188482487076\n",
            "\n",
            "\n",
            "Epoch 339 -\n",
            "Updated K params after epoch 339: [-0.19079495 -0.9181794 ]\n",
            "Train loss: 0.34355642181764384, Test loss: 0.33432976064930087\n",
            "\n",
            "\n",
            "Epoch 340 -\n",
            "Updated K params after epoch 340: [-0.19077804 -0.91839864]\n",
            "Train loss: 0.34199708535147877, Test loss: 0.33266370825520936\n",
            "\n",
            "\n",
            "Epoch 341 -\n",
            "Updated K params after epoch 341: [-0.19076107 -0.91861837]\n",
            "Train loss: 0.34044331556841134, Test loss: 0.33100387168125117\n",
            "\n",
            "\n",
            "Epoch 342 -\n",
            "Updated K params after epoch 342: [-0.19074403 -0.9188386 ]\n",
            "Train loss: 0.33889523389173964, Test loss: 0.3293503920441029\n",
            "\n",
            "\n",
            "Epoch 343 -\n",
            "Updated K params after epoch 343: [-0.19072694 -0.91905931]\n",
            "Train loss: 0.3373529592585605, Test loss: 0.32770340744827275\n",
            "\n",
            "\n",
            "Epoch 344 -\n",
            "Updated K params after epoch 344: [-0.19070978 -0.91928048]\n",
            "Train loss: 0.33581660804034785, Test loss: 0.326063052901251\n",
            "\n",
            "\n",
            "Epoch 345 -\n",
            "Updated K params after epoch 345: [-0.19069257 -0.91950211]\n",
            "Train loss: 0.33428629396838855, Test loss: 0.324429460234054\n",
            "\n",
            "\n",
            "Epoch 346 -\n",
            "Updated K params after epoch 346: [-0.1906753  -0.91972417]\n",
            "Train loss: 0.3327621280641124, Test loss: 0.3228027580272017\n",
            "\n",
            "\n",
            "Epoch 347 -\n",
            "Updated K params after epoch 347: [-0.19065798 -0.91994667]\n",
            "Train loss: 0.3312442185743388, Test loss: 0.3211830715421541\n",
            "\n",
            "\n",
            "Epoch 348 -\n",
            "Updated K params after epoch 348: [-0.1906406  -0.92016958]\n",
            "Train loss: 0.3297326709114559, Test loss: 0.31957052265822056\n",
            "\n",
            "\n",
            "Epoch 349 -\n",
            "Updated K params after epoch 349: [-0.19062317 -0.9203929 ]\n",
            "Train loss: 0.3282275875985313, Test loss: 0.31796522981494324\n",
            "\n",
            "\n",
            "Epoch 350 -\n",
            "Updated K params after epoch 350: [-0.19060569 -0.92061661]\n",
            "Train loss: 0.326729068219345, Test loss: 0.31636730795994406\n",
            "\n",
            "\n",
            "Epoch 351 -\n",
            "Updated K params after epoch 351: [-0.19058815 -0.9208407 ]\n",
            "Train loss: 0.32523720937332484, Test loss: 0.31477686850220993\n",
            "\n",
            "\n",
            "Epoch 352 -\n",
            "Updated K params after epoch 352: [-0.19057057 -0.92106516]\n",
            "Train loss: 0.3237521046353513, Test loss: 0.3131940192707834\n",
            "\n",
            "\n",
            "Epoch 353 -\n",
            "Updated K params after epoch 353: [-0.19055293 -0.92128996]\n",
            "Train loss: 0.32227384452038993, Test loss: 0.31161886447880943\n",
            "\n",
            "\n",
            "Epoch 354 -\n",
            "Updated K params after epoch 354: [-0.19053525 -0.92151511]\n",
            "Train loss: 0.3208025164528991, Test loss: 0.31005150469288184\n",
            "\n",
            "\n",
            "Epoch 355 -\n",
            "Updated K params after epoch 355: [-0.19051752 -0.92174059]\n",
            "Train loss: 0.31933820474095115, Test loss: 0.30849203680762033\n",
            "\n",
            "\n",
            "Epoch 356 -\n",
            "Updated K params after epoch 356: [-0.19049975 -0.92196638]\n",
            "Train loss: 0.3178809905549938, Test loss: 0.3069405540253975\n",
            "\n",
            "\n",
            "Epoch 357 -\n",
            "Updated K params after epoch 357: [-0.19048193 -0.92219247]\n",
            "Train loss: 0.3164309519111732, Test loss: 0.30539714584112876\n",
            "\n",
            "\n",
            "Epoch 358 -\n",
            "Updated K params after epoch 358: [-0.19046407 -0.92241885]\n",
            "Train loss: 0.3149881636591281, Test loss: 0.30386189803202546\n",
            "\n",
            "\n",
            "Epoch 359 -\n",
            "Updated K params after epoch 359: [-0.19044616 -0.9226455 ]\n",
            "Train loss: 0.31355269747415787, Test loss: 0.30233489265220337\n",
            "\n",
            "\n",
            "Epoch 360 -\n",
            "Updated K params after epoch 360: [-0.19042821 -0.92287242]\n",
            "Train loss: 0.3121246218536612, Test loss: 0.30081620803203296\n",
            "\n",
            "\n",
            "Epoch 361 -\n",
            "Updated K params after epoch 361: [-0.19041023 -0.92309959]\n",
            "Train loss: 0.3107040021177312, Test loss: 0.299305918782105\n",
            "\n",
            "\n",
            "Epoch 362 -\n",
            "Updated K params after epoch 362: [-0.1903922  -0.92332699]\n",
            "Train loss: 0.30929090041379076, Test loss: 0.29780409580168393\n",
            "\n",
            "\n",
            "Epoch 363 -\n",
            "Updated K params after epoch 363: [-0.19037414 -0.92355462]\n",
            "Train loss: 0.3078853757251433, Test loss: 0.29631080629150996\n",
            "\n",
            "\n",
            "Epoch 364 -\n",
            "Updated K params after epoch 364: [-0.19035604 -0.92378246]\n",
            "Train loss: 0.30648748388330943, Test loss: 0.2948261137708078\n",
            "\n",
            "\n",
            "Epoch 365 -\n",
            "Updated K params after epoch 365: [-0.1903379 -0.9240105]\n",
            "Train loss: 0.3050972775840142, Test loss: 0.29335007809835256\n",
            "\n",
            "\n",
            "Epoch 366 -\n",
            "Updated K params after epoch 366: [-0.19031973 -0.92423872]\n",
            "Train loss: 0.30371480640668824, Test loss: 0.2918827554974408\n",
            "\n",
            "\n",
            "Epoch 367 -\n",
            "Updated K params after epoch 367: [-0.19030152 -0.92446711]\n",
            "Train loss: 0.30234011683733836, Test loss: 0.29042419858460855\n",
            "\n",
            "\n",
            "Epoch 368 -\n",
            "Updated K params after epoch 368: [-0.19028328 -0.92469567]\n",
            "Train loss: 0.3009732522946439, Test loss: 0.2889744564019362\n",
            "\n",
            "\n",
            "Epoch 369 -\n",
            "Updated K params after epoch 369: [-0.19026501 -0.92492437]\n",
            "Train loss: 0.2996142531591291, Test loss: 0.28753357445277505\n",
            "\n",
            "\n",
            "Epoch 370 -\n",
            "Updated K params after epoch 370: [-0.19024671 -0.9251532 ]\n",
            "Train loss: 0.2982631568052624, Test loss: 0.2861015947407314\n",
            "\n",
            "\n",
            "Epoch 371 -\n",
            "Updated K params after epoch 371: [-0.19022838 -0.92538216]\n",
            "Train loss: 0.2969199976363306, Test loss: 0.2846785558117379\n",
            "\n",
            "\n",
            "Epoch 372 -\n",
            "Updated K params after epoch 372: [-0.19021002 -0.92561123]\n",
            "Train loss: 0.2955848071219353, Test loss: 0.2832644927990455\n",
            "\n",
            "\n",
            "Epoch 373 -\n",
            "Updated K params after epoch 373: [-0.19019164 -0.92584039]\n",
            "Train loss: 0.2942576138379575, Test loss: 0.28185943747096354\n",
            "\n",
            "\n",
            "Epoch 374 -\n",
            "Updated K params after epoch 374: [-0.19017323 -0.92606965]\n",
            "Train loss: 0.29293844350883846, Test loss: 0.28046341828118004\n",
            "\n",
            "\n",
            "Epoch 375 -\n",
            "Updated K params after epoch 375: [-0.19015479 -0.92629897]\n",
            "Train loss: 0.2916273190520225, Test loss: 0.27907646042149165\n",
            "\n",
            "\n",
            "Epoch 376 -\n",
            "Updated K params after epoch 376: [-0.19013632 -0.92652836]\n",
            "Train loss: 0.2903242606244102, Test loss: 0.27769858587677415\n",
            "\n",
            "\n",
            "Epoch 377 -\n",
            "Updated K params after epoch 377: [-0.19011784 -0.92675779]\n",
            "Train loss: 0.2890292856706717, Test loss: 0.27632981348202684\n",
            "\n",
            "\n",
            "Epoch 378 -\n",
            "Updated K params after epoch 378: [-0.19009933 -0.92698726]\n",
            "Train loss: 0.2877424089732699, Test loss: 0.2749701589813241\n",
            "\n",
            "\n",
            "Epoch 379 -\n",
            "Updated K params after epoch 379: [-0.1900808  -0.92721676]\n",
            "Train loss: 0.28646364270404706, Test loss: 0.2736196350885108\n",
            "\n",
            "\n",
            "Epoch 380 -\n",
            "Updated K params after epoch 380: [-0.19006225 -0.92744628]\n",
            "Train loss: 0.28519299647723, Test loss: 0.27227825154948104\n",
            "\n",
            "\n",
            "Epoch 381 -\n",
            "Updated K params after epoch 381: [-0.19004369 -0.92767579]\n",
            "Train loss: 0.28393047740371186, Test loss: 0.2709460152058805\n",
            "\n",
            "\n",
            "Epoch 382 -\n",
            "Updated K params after epoch 382: [-0.1900251 -0.9279053]\n",
            "Train loss: 0.28267609014647105, Test loss: 0.2696229300600788\n",
            "\n",
            "\n",
            "Epoch 383 -\n",
            "Updated K params after epoch 383: [-0.1900065  -0.92813479]\n",
            "Train loss: 0.2814298369769921, Test loss: 0.2683089973412593\n",
            "\n",
            "\n",
            "Epoch 384 -\n",
            "Updated K params after epoch 384: [-0.18998788 -0.92836425]\n",
            "Train loss: 0.280191717832556, Test loss: 0.26700421557247894\n",
            "\n",
            "\n",
            "Epoch 385 -\n",
            "Updated K params after epoch 385: [-0.18996924 -0.92859366]\n",
            "Train loss: 0.2789617303742711, Test loss: 0.2657085806385545\n",
            "\n",
            "\n",
            "Epoch 386 -\n",
            "Updated K params after epoch 386: [-0.18995059 -0.92882303]\n",
            "Train loss: 0.277739870045721, Test loss: 0.2644220858546359\n",
            "\n",
            "\n",
            "Epoch 387 -\n",
            "Updated K params after epoch 387: [-0.18993193 -0.92905233]\n",
            "Train loss: 0.2765261301321078, Test loss: 0.2631447220353302\n",
            "\n",
            "\n",
            "Epoch 388 -\n",
            "Updated K params after epoch 388: [-0.18991325 -0.92928156]\n",
            "Train loss: 0.2753205018197758, Test loss: 0.2618764775642483\n",
            "\n",
            "\n",
            "Epoch 389 -\n",
            "Updated K params after epoch 389: [-0.18989457 -0.92951071]\n",
            "Train loss: 0.2741229742560025, Test loss: 0.2606173384638469\n",
            "\n",
            "\n",
            "Epoch 390 -\n",
            "Updated K params after epoch 390: [-0.18987587 -0.92973976]\n",
            "Train loss: 0.2729335346089516, Test loss: 0.2593672884654445\n",
            "\n",
            "\n",
            "Epoch 391 -\n",
            "Updated K params after epoch 391: [-0.18985717 -0.92996871]\n",
            "Train loss: 0.27175216812768266, Test loss: 0.25812630907929834\n",
            "\n",
            "\n",
            "Epoch 392 -\n",
            "Updated K params after epoch 392: [-0.18983845 -0.93019755]\n",
            "Train loss: 0.2705788582021221, Test loss: 0.2568943796646291\n",
            "\n",
            "\n",
            "Epoch 393 -\n",
            "Updated K params after epoch 393: [-0.18981973 -0.93042626]\n",
            "Train loss: 0.2694135864229003, Test loss: 0.2556714774994876\n",
            "\n",
            "\n",
            "Epoch 394 -\n",
            "Updated K params after epoch 394: [-0.189801   -0.93065485]\n",
            "Train loss: 0.2682563326409674, Test loss: 0.2544575778503667\n",
            "\n",
            "\n",
            "Epoch 395 -\n",
            "Updated K params after epoch 395: [-0.18978227 -0.93088329]\n",
            "Train loss: 0.2671070750269025, Test loss: 0.25325265404145814\n",
            "\n",
            "\n",
            "Epoch 396 -\n",
            "Updated K params after epoch 396: [-0.18976353 -0.93111157]\n",
            "Train loss: 0.2659657901298387, Test loss: 0.2520566775234681\n",
            "\n",
            "\n",
            "Epoch 397 -\n",
            "Updated K params after epoch 397: [-0.18974479 -0.9313397 ]\n",
            "Train loss: 0.2648324529359283, Test loss: 0.250869617941905\n",
            "\n",
            "\n",
            "Epoch 398 -\n",
            "Updated K params after epoch 398: [-0.18972604 -0.93156766]\n",
            "Train loss: 0.26370703692627806, Test loss: 0.24969144320475978\n",
            "\n",
            "\n",
            "Epoch 399 -\n",
            "Updated K params after epoch 399: [-0.1897073  -0.93179545]\n",
            "Train loss: 0.26258951413429066, Test loss: 0.24852211954950443\n",
            "\n",
            "\n",
            "Epoch 400 -\n",
            "Updated K params after epoch 400: [-0.18968855 -0.93202304]\n",
            "Train loss: 0.2614798552023498, Test loss: 0.24736161160933787\n",
            "\n",
            "\n",
            "Epoch 401 -\n",
            "Updated K params after epoch 401: [-0.1896698  -0.93225044]\n",
            "Train loss: 0.26037802943779326, Test loss: 0.24620988247861594\n",
            "\n",
            "\n",
            "Epoch 402 -\n",
            "Updated K params after epoch 402: [-0.18965105 -0.93247764]\n",
            "Train loss: 0.25928400486812203, Test loss: 0.24506689377740434\n",
            "\n",
            "\n",
            "Epoch 403 -\n",
            "Updated K params after epoch 403: [-0.18963231 -0.93270463]\n",
            "Train loss: 0.2581977482953977, Test loss: 0.24393260571509864\n",
            "\n",
            "\n",
            "Epoch 404 -\n",
            "Updated K params after epoch 404: [-0.18961356 -0.9329314 ]\n",
            "Train loss: 0.2571192253497835, Test loss: 0.24280697715306235\n",
            "\n",
            "\n",
            "Epoch 405 -\n",
            "Updated K params after epoch 405: [-0.18959482 -0.93315794]\n",
            "Train loss: 0.2560484005421904, Test loss: 0.2416899656662348\n",
            "\n",
            "\n",
            "Epoch 406 -\n",
            "Updated K params after epoch 406: [-0.18957609 -0.93338425]\n",
            "Train loss: 0.254985237315992, Test loss: 0.24058152760366827\n",
            "\n",
            "\n",
            "Epoch 407 -\n",
            "Updated K params after epoch 407: [-0.18955736 -0.93361031]\n",
            "Train loss: 0.25392969809777505, Test loss: 0.239481618147956\n",
            "\n",
            "\n",
            "Epoch 408 -\n",
            "Updated K params after epoch 408: [-0.18953863 -0.93383613]\n",
            "Train loss: 0.2528817443471, Test loss: 0.23839019137351727\n",
            "\n",
            "\n",
            "Epoch 409 -\n",
            "Updated K params after epoch 409: [-0.18951991 -0.93406169]\n",
            "Train loss: 0.2518413366052424, Test loss: 0.23730720030371025\n",
            "\n",
            "\n",
            "Epoch 410 -\n",
            "Updated K params after epoch 410: [-0.1895012  -0.93428699]\n",
            "Train loss: 0.25080843454289825, Test loss: 0.23623259696674628\n",
            "\n",
            "\n",
            "Epoch 411 -\n",
            "Updated K params after epoch 411: [-0.1894825  -0.93451202]\n",
            "Train loss: 0.2497829970068302, Test loss: 0.23516633245038318\n",
            "\n",
            "\n",
            "Epoch 412 -\n",
            "Updated K params after epoch 412: [-0.18946381 -0.93473677]\n",
            "Train loss: 0.24876498206544315, Test loss: 0.23410835695537932\n",
            "\n",
            "\n",
            "Epoch 413 -\n",
            "Updated K params after epoch 413: [-0.18944512 -0.93496123]\n",
            "Train loss: 0.24775434705327473, Test loss: 0.23305861984769174\n",
            "\n",
            "\n",
            "Epoch 414 -\n",
            "Updated K params after epoch 414: [-0.18942645 -0.93518541]\n",
            "Train loss: 0.2467510486143905, Test loss: 0.23201706970940716\n",
            "\n",
            "\n",
            "Epoch 415 -\n",
            "Updated K params after epoch 415: [-0.18940779 -0.93540929]\n",
            "Train loss: 0.2457550427446793, Test loss: 0.23098365438839547\n",
            "\n",
            "\n",
            "Epoch 416 -\n",
            "Updated K params after epoch 416: [-0.18938914 -0.93563287]\n",
            "Train loss: 0.24476628483304116, Test loss: 0.2299583210466798\n",
            "\n",
            "\n",
            "Epoch 417 -\n",
            "Updated K params after epoch 417: [-0.18937051 -0.93585614]\n",
            "Train loss: 0.24378472970146803, Test loss: 0.2289410162075192\n",
            "\n",
            "\n",
            "Epoch 418 -\n",
            "Updated K params after epoch 418: [-0.18935188 -0.9360791 ]\n",
            "Train loss: 0.24281033164401558, Test loss: 0.22793168580120216\n",
            "\n",
            "\n",
            "Epoch 419 -\n",
            "Updated K params after epoch 419: [-0.18933328 -0.93630174]\n",
            "Train loss: 0.24184304446466975, Test loss: 0.22693027520955292\n",
            "\n",
            "\n",
            "Epoch 420 -\n",
            "Updated K params after epoch 420: [-0.18931468 -0.93652406]\n",
            "Train loss: 0.24088282151411045, Test loss: 0.22593672930915298\n",
            "\n",
            "\n",
            "Epoch 421 -\n",
            "Updated K params after epoch 421: [-0.18929611 -0.93674604]\n",
            "Train loss: 0.2399296157253793, Test loss: 0.2249509925132842\n",
            "\n",
            "\n",
            "Epoch 422 -\n",
            "Updated K params after epoch 422: [-0.18927755 -0.93696769]\n",
            "Train loss: 0.23898337964845862, Test loss: 0.2239730088126002\n",
            "\n",
            "\n",
            "Epoch 423 -\n",
            "Updated K params after epoch 423: [-0.189259 -0.937189]\n",
            "Train loss: 0.23804406548377083, Test loss: 0.22300272181453573\n",
            "\n",
            "\n",
            "Epoch 424 -\n",
            "Updated K params after epoch 424: [-0.18924048 -0.93740996]\n",
            "Train loss: 0.23711162511460832, Test loss: 0.22204007478146479\n",
            "\n",
            "\n",
            "Epoch 425 -\n",
            "Updated K params after epoch 425: [-0.18922197 -0.93763058]\n",
            "Train loss: 0.2361860101385063, Test loss: 0.22108501066762024\n",
            "\n",
            "\n",
            "Epoch 426 -\n",
            "Updated K params after epoch 426: [-0.18920349 -0.93785084]\n",
            "Train loss: 0.23526717189757096, Test loss: 0.22013747215478868\n",
            "\n",
            "\n",
            "Epoch 427 -\n",
            "Updated K params after epoch 427: [-0.18918502 -0.93807074]\n",
            "Train loss: 0.23435506150777796, Test loss: 0.21919740168679594\n",
            "\n",
            "\n",
            "Epoch 428 -\n",
            "Updated K params after epoch 428: [-0.18916658 -0.93829027]\n",
            "Train loss: 0.23344962988725487, Test loss: 0.21826474150279981\n",
            "\n",
            "\n",
            "Epoch 429 -\n",
            "Updated K params after epoch 429: [-0.18914815 -0.93850944]\n",
            "Train loss: 0.23255082778356548, Test loss: 0.21733943366940767\n",
            "\n",
            "\n",
            "Epoch 430 -\n",
            "Updated K params after epoch 430: [-0.18912975 -0.93872824]\n",
            "Train loss: 0.23165860580001132, Test loss: 0.21642142011163717\n",
            "\n",
            "\n",
            "Epoch 431 -\n",
            "Updated K params after epoch 431: [-0.18911137 -0.93894666]\n",
            "Train loss: 0.23077291442096867, Test loss: 0.2155106426427401\n",
            "\n",
            "\n",
            "Epoch 432 -\n",
            "Updated K params after epoch 432: [-0.18909302 -0.9391647 ]\n",
            "Train loss: 0.22989370403627837, Test loss: 0.21460704299290914\n",
            "\n",
            "\n",
            "Epoch 433 -\n",
            "Updated K params after epoch 433: [-0.18907469 -0.93938236]\n",
            "Train loss: 0.22902092496470894, Test loss: 0.213710562836889\n",
            "\n",
            "\n",
            "Epoch 434 -\n",
            "Updated K params after epoch 434: [-0.18905638 -0.93959963]\n",
            "Train loss: 0.22815452747650972, Test loss: 0.21282114382051268\n",
            "\n",
            "\n",
            "Epoch 435 -\n",
            "Updated K params after epoch 435: [-0.1890381  -0.93981651]\n",
            "Train loss: 0.22729446181507562, Test loss: 0.21193872758618618\n",
            "\n",
            "\n",
            "Epoch 436 -\n",
            "Updated K params after epoch 436: [-0.18901985 -0.940033  ]\n",
            "Train loss: 0.22644067821774222, Test loss: 0.21106325579734272\n",
            "\n",
            "\n",
            "Epoch 437 -\n",
            "Updated K params after epoch 437: [-0.18900162 -0.94024909]\n",
            "Train loss: 0.2255931269357318, Test loss: 0.21019467016189075\n",
            "\n",
            "\n",
            "Epoch 438 -\n",
            "Updated K params after epoch 438: [-0.18898342 -0.94046478]\n",
            "Train loss: 0.22475175825327062, Test loss: 0.20933291245467786\n",
            "\n",
            "\n",
            "Epoch 439 -\n",
            "Updated K params after epoch 439: [-0.18896524 -0.94068007]\n",
            "Train loss: 0.22391652250589766, Test loss: 0.20847792453899439\n",
            "\n",
            "\n",
            "Epoch 440 -\n",
            "Updated K params after epoch 440: [-0.1889471  -0.94089495]\n",
            "Train loss: 0.22308737009798563, Test loss: 0.20762964838714063\n",
            "\n",
            "\n",
            "Epoch 441 -\n",
            "Updated K params after epoch 441: [-0.18892898 -0.94110942]\n",
            "Train loss: 0.22226425151949453, Test loss: 0.20678802610008076\n",
            "\n",
            "\n",
            "Epoch 442 -\n",
            "Updated K params after epoch 442: [-0.18891089 -0.94132348]\n",
            "Train loss: 0.22144711736197908, Test loss: 0.20595299992620733\n",
            "\n",
            "\n",
            "Epoch 443 -\n",
            "Updated K params after epoch 443: [-0.18889284 -0.94153713]\n",
            "Train loss: 0.22063591833386942, Test loss: 0.2051245122792403\n",
            "\n",
            "\n",
            "Epoch 444 -\n",
            "Updated K params after epoch 444: [-0.18887481 -0.94175036]\n",
            "Train loss: 0.21983060527504616, Test loss: 0.20430250575528408\n",
            "\n",
            "\n",
            "Epoch 445 -\n",
            "Updated K params after epoch 445: [-0.18885681 -0.94196317]\n",
            "Train loss: 0.21903112917073053, Test loss: 0.20348692314906605\n",
            "\n",
            "\n",
            "Epoch 446 -\n",
            "Updated K params after epoch 446: [-0.18883885 -0.94217555]\n",
            "Train loss: 0.21823744116470864, Test loss: 0.20267770746938013\n",
            "\n",
            "\n",
            "Epoch 447 -\n",
            "Updated K params after epoch 447: [-0.18882092 -0.94238752]\n",
            "Train loss: 0.21744949257191046, Test loss: 0.20187480195375865\n",
            "\n",
            "\n",
            "Epoch 448 -\n",
            "Updated K params after epoch 448: [-0.18880302 -0.94259906]\n",
            "Train loss: 0.2166672348903638, Test loss: 0.20107815008239555\n",
            "\n",
            "\n",
            "Epoch 449 -\n",
            "Updated K params after epoch 449: [-0.18878515 -0.94281017]\n",
            "Train loss: 0.21589061981254115, Test loss: 0.20028769559134346\n",
            "\n",
            "\n",
            "Epoch 450 -\n",
            "Updated K params after epoch 450: [-0.18876732 -0.94302085]\n",
            "Train loss: 0.2151195992361205, Test loss: 0.19950338248500787\n",
            "\n",
            "\n",
            "Epoch 451 -\n",
            "Updated K params after epoch 451: [-0.18874952 -0.9432311 ]\n",
            "Train loss: 0.2143541252741773, Test loss: 0.19872515504795965\n",
            "\n",
            "\n",
            "Epoch 452 -\n",
            "Updated K params after epoch 452: [-0.18873175 -0.94344091]\n",
            "Train loss: 0.21359415026482775, Test loss: 0.19795295785608905\n",
            "\n",
            "\n",
            "Epoch 453 -\n",
            "Updated K params after epoch 453: [-0.18871402 -0.94365029]\n",
            "Train loss: 0.21283962678034027, Test loss: 0.19718673578712123\n",
            "\n",
            "\n",
            "Epoch 454 -\n",
            "Updated K params after epoch 454: [-0.18869633 -0.94385924]\n",
            "Train loss: 0.21209050763573437, Test loss: 0.19642643403051643\n",
            "\n",
            "\n",
            "Epoch 455 -\n",
            "Updated K params after epoch 455: [-0.18867867 -0.94406775]\n",
            "Train loss: 0.21134674589688332, Test loss: 0.19567199809677394\n",
            "\n",
            "\n",
            "Epoch 456 -\n",
            "Updated K params after epoch 456: [-0.18866104 -0.94427581]\n",
            "Train loss: 0.2106082948881392, Test loss: 0.19492337382616126\n",
            "\n",
            "\n",
            "Epoch 457 -\n",
            "Updated K params after epoch 457: [-0.18864346 -0.94448344]\n",
            "Train loss: 0.20987510819949567, Test loss: 0.19418050739688839\n",
            "\n",
            "\n",
            "Epoch 458 -\n",
            "Updated K params after epoch 458: [-0.18862591 -0.94469062]\n",
            "Train loss: 0.20914713969330656, Test loss: 0.19344334533274682\n",
            "\n",
            "\n",
            "Epoch 459 -\n",
            "Updated K params after epoch 459: [-0.1886084  -0.94489736]\n",
            "Train loss: 0.20842434351057493, Test loss: 0.1927118345102324\n",
            "\n",
            "\n",
            "Epoch 460 -\n",
            "Updated K params after epoch 460: [-0.18859093 -0.94510366]\n",
            "Train loss: 0.2077066740768296, Test loss: 0.19198592216517127\n",
            "\n",
            "\n",
            "Epoch 461 -\n",
            "Updated K params after epoch 461: [-0.18857349 -0.94530951]\n",
            "Train loss: 0.2069940861076036, Test loss: 0.1912655558988666\n",
            "\n",
            "\n",
            "Epoch 462 -\n",
            "Updated K params after epoch 462: [-0.18855609 -0.94551492]\n",
            "Train loss: 0.20628653461352991, Test loss: 0.19055068368378478\n",
            "\n",
            "\n",
            "Epoch 463 -\n",
            "Updated K params after epoch 463: [-0.18853874 -0.94571987]\n",
            "Train loss: 0.2055839749050692, Test loss: 0.18984125386879835\n",
            "\n",
            "\n",
            "Epoch 464 -\n",
            "Updated K params after epoch 464: [-0.18852142 -0.94592438]\n",
            "Train loss: 0.20488636259688323, Test loss: 0.1891372151840024\n",
            "\n",
            "\n",
            "Epoch 465 -\n",
            "Updated K params after epoch 465: [-0.18850414 -0.94612845]\n",
            "Train loss: 0.20419365361186845, Test loss: 0.18843851674512152\n",
            "\n",
            "\n",
            "Epoch 466 -\n",
            "Updated K params after epoch 466: [-0.18848691 -0.94633206]\n",
            "Train loss: 0.20350580418486264, Test loss: 0.18774510805752362\n",
            "\n",
            "\n",
            "Epoch 467 -\n",
            "Updated K params after epoch 467: [-0.18846971 -0.94653522]\n",
            "Train loss: 0.20282277086603775, Test loss: 0.18705693901985557\n",
            "\n",
            "\n",
            "Epoch 468 -\n",
            "Updated K params after epoch 468: [-0.18845255 -0.94673793]\n",
            "Train loss: 0.20214451052399185, Test loss: 0.18637395992731703\n",
            "\n",
            "\n",
            "Epoch 469 -\n",
            "Updated K params after epoch 469: [-0.18843544 -0.94694019]\n",
            "Train loss: 0.20147098034855215, Test loss: 0.18569612147458672\n",
            "\n",
            "\n",
            "Epoch 470 -\n",
            "Updated K params after epoch 470: [-0.18841837 -0.947142  ]\n",
            "Train loss: 0.20080213785330134, Test loss: 0.18502337475841563\n",
            "\n",
            "\n",
            "Epoch 471 -\n",
            "Updated K params after epoch 471: [-0.18840134 -0.94734335]\n",
            "Train loss: 0.2001379408778385, Test loss: 0.18435567127990143\n",
            "\n",
            "\n",
            "Epoch 472 -\n",
            "Updated K params after epoch 472: [-0.18838435 -0.94754426]\n",
            "Train loss: 0.1994783475897858, Test loss: 0.18369296294645784\n",
            "\n",
            "\n",
            "Epoch 473 -\n",
            "Updated K params after epoch 473: [-0.1883674  -0.94774471]\n",
            "Train loss: 0.19882331648655213, Test loss: 0.18303520207349125\n",
            "\n",
            "\n",
            "Epoch 474 -\n",
            "Updated K params after epoch 474: [-0.1883505  -0.94794471]\n",
            "Train loss: 0.1981728063968637, Test loss: 0.18238234138579892\n",
            "\n",
            "\n",
            "Epoch 475 -\n",
            "Updated K params after epoch 475: [-0.18833364 -0.94814425]\n",
            "Train loss: 0.19752677648207237, Test loss: 0.18173433401869962\n",
            "\n",
            "\n",
            "Epoch 476 -\n",
            "Updated K params after epoch 476: [-0.18831682 -0.94834335]\n",
            "Train loss: 0.19688518623725076, Test loss: 0.18109113351890974\n",
            "\n",
            "\n",
            "Epoch 477 -\n",
            "Updated K params after epoch 477: [-0.18830005 -0.94854199]\n",
            "Train loss: 0.196247995492084, Test loss: 0.1804526938451758\n",
            "\n",
            "\n",
            "Epoch 478 -\n",
            "Updated K params after epoch 478: [-0.18828332 -0.94874017]\n",
            "Train loss: 0.19561516441156776, Test loss: 0.17981896936867536\n",
            "\n",
            "\n",
            "Epoch 479 -\n",
            "Updated K params after epoch 479: [-0.18826664 -0.94893791]\n",
            "Train loss: 0.1949866534965205, Test loss: 0.17918991487319638\n",
            "\n",
            "\n",
            "Epoch 480 -\n",
            "Updated K params after epoch 480: [-0.18825    -0.94913519]\n",
            "Train loss: 0.19436242358391892, Test loss: 0.1785654855551057\n",
            "\n",
            "\n",
            "Epoch 481 -\n",
            "Updated K params after epoch 481: [-0.18823341 -0.94933202]\n",
            "Train loss: 0.19374243584706535, Test loss: 0.17794563702311753\n",
            "\n",
            "\n",
            "Epoch 482 -\n",
            "Updated K params after epoch 482: [-0.18821686 -0.94952839]\n",
            "Train loss: 0.19312665179559396, Test loss: 0.17733032529787032\n",
            "\n",
            "\n",
            "Epoch 483 -\n",
            "Updated K params after epoch 483: [-0.18820036 -0.94972431]\n",
            "Train loss: 0.19251503327532435, Test loss: 0.1767195068113231\n",
            "\n",
            "\n",
            "Epoch 484 -\n",
            "Updated K params after epoch 484: [-0.1881839  -0.94991978]\n",
            "Train loss: 0.19190754246796976, Test loss: 0.1761131384059786\n",
            "\n",
            "\n",
            "Epoch 485 -\n",
            "Updated K params after epoch 485: [-0.18816749 -0.9501148 ]\n",
            "Train loss: 0.19130414189070652, Test loss: 0.17551117733394367\n",
            "\n",
            "\n",
            "Epoch 486 -\n",
            "Updated K params after epoch 486: [-0.18815112 -0.95030937]\n",
            "Train loss: 0.19070479439561205, Test loss: 0.17491358125583417\n",
            "\n",
            "\n",
            "Epoch 487 -\n",
            "Updated K params after epoch 487: [-0.1881348  -0.95050348]\n",
            "Train loss: 0.1901094631689785, Test loss: 0.17432030823953298\n",
            "\n",
            "\n",
            "Epoch 488 -\n",
            "Updated K params after epoch 488: [-0.18811853 -0.95069715]\n",
            "Train loss: 0.18951811173050662, Test loss: 0.1737313167588091\n",
            "\n",
            "\n",
            "Epoch 489 -\n",
            "Updated K params after epoch 489: [-0.18810231 -0.95089036]\n",
            "Train loss: 0.18893070393238848, Test loss: 0.173146565691805\n",
            "\n",
            "\n",
            "Epoch 490 -\n",
            "Updated K params after epoch 490: [-0.18808613 -0.95108313]\n",
            "Train loss: 0.1883472039582829, Test loss: 0.17256601431939977\n",
            "\n",
            "\n",
            "Epoch 491 -\n",
            "Updated K params after epoch 491: [-0.18807    -0.95127544]\n",
            "Train loss: 0.18776757632218996, Test loss: 0.17198962232345527\n",
            "\n",
            "\n",
            "Epoch 492 -\n",
            "Updated K params after epoch 492: [-0.18805392 -0.95146731]\n",
            "Train loss: 0.18719178586723045, Test loss: 0.17141734978495118\n",
            "\n",
            "\n",
            "Epoch 493 -\n",
            "Updated K params after epoch 493: [-0.18803788 -0.95165873]\n",
            "Train loss: 0.18661979776433485, Test loss: 0.1708491571820163\n",
            "\n",
            "\n",
            "Epoch 494 -\n",
            "Updated K params after epoch 494: [-0.18802189 -0.9518497 ]\n",
            "Train loss: 0.18605157751084705, Test loss: 0.17028500538786204\n",
            "\n",
            "\n",
            "Epoch 495 -\n",
            "Updated K params after epoch 495: [-0.18800595 -0.95204022]\n",
            "Train loss: 0.18548709092904792, Test loss: 0.16972485566862358\n",
            "\n",
            "\n",
            "Epoch 496 -\n",
            "Updated K params after epoch 496: [-0.18799006 -0.95223029]\n",
            "Train loss: 0.1849263041646031, Test loss: 0.16916866968111513\n",
            "\n",
            "\n",
            "Epoch 497 -\n",
            "Updated K params after epoch 497: [-0.18797422 -0.95241993]\n",
            "Train loss: 0.18436918368493924, Test loss: 0.16861640947050432\n",
            "\n",
            "\n",
            "Epoch 498 -\n",
            "Updated K params after epoch 498: [-0.18795843 -0.95260911]\n",
            "Train loss: 0.18381569627755387, Test loss: 0.1680680374679113\n",
            "\n",
            "\n",
            "Epoch 499 -\n",
            "Updated K params after epoch 499: [-0.18794268 -0.95279785]\n",
            "Train loss: 0.18326580904826162, Test loss: 0.1675235164879369\n",
            "\n",
            "\n",
            "Epoch 500 -\n",
            "Updated K params after epoch 500: [-0.18792698 -0.95298615]\n",
            "Train loss: 0.18271948941938196, Test loss: 0.1669828097261261\n",
            "\n",
            "\n",
            "Epoch 501 -\n",
            "Updated K params after epoch 501: [-0.18791134 -0.95317401]\n",
            "Train loss: 0.18217670512787165, Test loss: 0.16644588075636968\n",
            "\n",
            "\n",
            "Epoch 502 -\n",
            "Updated K params after epoch 502: [-0.18789574 -0.95336142]\n",
            "Train loss: 0.1816374242234056, Test loss: 0.16591269352824983\n",
            "\n",
            "\n",
            "Epoch 503 -\n",
            "Updated K params after epoch 503: [-0.18788019 -0.95354839]\n",
            "Train loss: 0.1811016150664094, Test loss: 0.16538321236433354\n",
            "\n",
            "\n",
            "Epoch 504 -\n",
            "Updated K params after epoch 504: [-0.18786469 -0.95373493]\n",
            "Train loss: 0.18056924632604743, Test loss: 0.16485740195741777\n",
            "\n",
            "\n",
            "Epoch 505 -\n",
            "Updated K params after epoch 505: [-0.18784924 -0.95392102]\n",
            "Train loss: 0.18004028697816907, Test loss: 0.16433522736773007\n",
            "\n",
            "\n",
            "Epoch 506 -\n",
            "Updated K params after epoch 506: [-0.18783384 -0.95410668]\n",
            "Train loss: 0.17951470630321636, Test loss: 0.16381665402008935\n",
            "\n",
            "\n",
            "Epoch 507 -\n",
            "Updated K params after epoch 507: [-0.18781849 -0.95429189]\n",
            "Train loss: 0.17899247388409595, Test loss: 0.16330164770102898\n",
            "\n",
            "\n",
            "Epoch 508 -\n",
            "Updated K params after epoch 508: [-0.18780319 -0.95447668]\n",
            "Train loss: 0.1784735596040185, Test loss: 0.16279017455588654\n",
            "\n",
            "\n",
            "Epoch 509 -\n",
            "Updated K params after epoch 509: [-0.18778794 -0.95466102]\n",
            "Train loss: 0.1779579336443074, Test loss: 0.16228220108586355\n",
            "\n",
            "\n",
            "Epoch 510 -\n",
            "Updated K params after epoch 510: [-0.18777274 -0.95484494]\n",
            "Train loss: 0.17744556648217996, Test loss: 0.16177769414505766\n",
            "\n",
            "\n",
            "Epoch 511 -\n",
            "Updated K params after epoch 511: [-0.18775759 -0.95502841]\n",
            "Train loss: 0.17693642888850372, Test loss: 0.16127662093747064\n",
            "\n",
            "\n",
            "Epoch 512 -\n",
            "Updated K params after epoch 512: [-0.18774249 -0.95521146]\n",
            "Train loss: 0.17643049192552926, Test loss: 0.16077894901399517\n",
            "\n",
            "\n",
            "Epoch 513 -\n",
            "Updated K params after epoch 513: [-0.18772745 -0.95539408]\n",
            "Train loss: 0.17592772694460274, Test loss: 0.1602846462693827\n",
            "\n",
            "\n",
            "Epoch 514 -\n",
            "Updated K params after epoch 514: [-0.18771245 -0.95557626]\n",
            "Train loss: 0.17542810558385993, Test loss: 0.15979368093919533\n",
            "\n",
            "\n",
            "Epoch 515 -\n",
            "Updated K params after epoch 515: [-0.1876975  -0.95575802]\n",
            "Train loss: 0.17493159976590342, Test loss: 0.15930602159674384\n",
            "\n",
            "\n",
            "Epoch 516 -\n",
            "Updated K params after epoch 516: [-0.18768261 -0.95593935]\n",
            "Train loss: 0.17443818169546535, Test loss: 0.15882163715001452\n",
            "\n",
            "\n",
            "Epoch 517 -\n",
            "Updated K params after epoch 517: [-0.18766776 -0.95612025]\n",
            "Train loss: 0.17394782385705768, Test loss: 0.15834049683858673\n",
            "\n",
            "\n",
            "Epoch 518 -\n",
            "Updated K params after epoch 518: [-0.18765297 -0.95630072]\n",
            "Train loss: 0.17346049901261104, Test loss: 0.15786257023054345\n",
            "\n",
            "\n",
            "Epoch 519 -\n",
            "Updated K params after epoch 519: [-0.18763823 -0.95648077]\n",
            "Train loss: 0.17297618019910457, Test loss: 0.15738782721937708\n",
            "\n",
            "\n",
            "Epoch 520 -\n",
            "Updated K params after epoch 520: [-0.18762354 -0.9566604 ]\n",
            "Train loss: 0.1724948407261879, Test loss: 0.15691623802089164\n",
            "\n",
            "\n",
            "Epoch 521 -\n",
            "Updated K params after epoch 521: [-0.1876089  -0.95683961]\n",
            "Train loss: 0.17201645417379727, Test loss: 0.1564477731701041\n",
            "\n",
            "\n",
            "Epoch 522 -\n",
            "Updated K params after epoch 522: [-0.18759431 -0.95701839]\n",
            "Train loss: 0.17154099438976653, Test loss: 0.15598240351814602\n",
            "\n",
            "\n",
            "Epoch 523 -\n",
            "Updated K params after epoch 523: [-0.18757977 -0.95719675]\n",
            "Train loss: 0.1710684354874349, Test loss: 0.15552010022916687\n",
            "\n",
            "\n",
            "Epoch 524 -\n",
            "Updated K params after epoch 524: [-0.18756529 -0.9573747 ]\n",
            "Train loss: 0.170598751843253, Test loss: 0.15506083477724178\n",
            "\n",
            "\n",
            "Epoch 525 -\n",
            "Updated K params after epoch 525: [-0.18755085 -0.95755223]\n",
            "Train loss: 0.1701319180943878, Test loss: 0.15460457894328336\n",
            "\n",
            "\n",
            "Epoch 526 -\n",
            "Updated K params after epoch 526: [-0.18753647 -0.95772934]\n",
            "Train loss: 0.16966790913632804, Test loss: 0.15415130481196074\n",
            "\n",
            "\n",
            "Epoch 527 -\n",
            "Updated K params after epoch 527: [-0.18752214 -0.95790604]\n",
            "Train loss: 0.1692067001204912, Test loss: 0.15370098476862595\n",
            "\n",
            "\n",
            "Epoch 528 -\n",
            "Updated K params after epoch 528: [-0.18750786 -0.95808232]\n",
            "Train loss: 0.16874826645183327, Test loss: 0.15325359149624956\n",
            "\n",
            "\n",
            "Epoch 529 -\n",
            "Updated K params after epoch 529: [-0.18749363 -0.95825819]\n",
            "Train loss: 0.1682925837864619, Test loss: 0.15280909797236622\n",
            "\n",
            "\n",
            "Epoch 530 -\n",
            "Updated K params after epoch 530: [-0.18747945 -0.95843365]\n",
            "Train loss: 0.16783962802925406, Test loss: 0.15236747746603146\n",
            "\n",
            "\n",
            "Epoch 531 -\n",
            "Updated K params after epoch 531: [-0.18746533 -0.9586087 ]\n",
            "Train loss: 0.1673893753314798, Test loss: 0.1519287035347914\n",
            "\n",
            "\n",
            "Epoch 532 -\n",
            "Updated K params after epoch 532: [-0.18745126 -0.95878334]\n",
            "Train loss: 0.16694180208843162, Test loss: 0.15149275002166465\n",
            "\n",
            "\n",
            "Epoch 533 -\n",
            "Updated K params after epoch 533: [-0.18743724 -0.95895757]\n",
            "Train loss: 0.16649688493706144, Test loss: 0.15105959105213956\n",
            "\n",
            "\n",
            "Epoch 534 -\n",
            "Updated K params after epoch 534: [-0.18742327 -0.9591314 ]\n",
            "Train loss: 0.1660546007536256, Test loss: 0.15062920103118577\n",
            "\n",
            "\n",
            "Epoch 535 -\n",
            "Updated K params after epoch 535: [-0.18740935 -0.95930482]\n",
            "Train loss: 0.16561492665133828, Test loss: 0.15020155464028184\n",
            "\n",
            "\n",
            "Epoch 536 -\n",
            "Updated K params after epoch 536: [-0.18739549 -0.95947784]\n",
            "Train loss: 0.16517783997803454, Test loss: 0.14977662683445975\n",
            "\n",
            "\n",
            "Epoch 537 -\n",
            "Updated K params after epoch 537: [-0.18738167 -0.95965045]\n",
            "Train loss: 0.16474331831384337, Test loss: 0.14935439283936647\n",
            "\n",
            "\n",
            "Epoch 538 -\n",
            "Updated K params after epoch 538: [-0.18736791 -0.95982267]\n",
            "Train loss: 0.16431133946887105, Test loss: 0.14893482814834355\n",
            "\n",
            "\n",
            "Epoch 539 -\n",
            "Updated K params after epoch 539: [-0.1873542  -0.95999448]\n",
            "Train loss: 0.16388188148089644, Test loss: 0.14851790851952554\n",
            "\n",
            "\n",
            "Epoch 540 -\n",
            "Updated K params after epoch 540: [-0.18734055 -0.9601659 ]\n",
            "Train loss: 0.16345492261307715, Test loss: 0.14810360997295752\n",
            "\n",
            "\n",
            "Epoch 541 -\n",
            "Updated K params after epoch 541: [-0.18732694 -0.96033692]\n",
            "Train loss: 0.16303044135166891, Test loss: 0.14769190878773228\n",
            "\n",
            "\n",
            "Epoch 542 -\n",
            "Updated K params after epoch 542: [-0.18731339 -0.96050755]\n",
            "Train loss: 0.16260841640375684, Test loss: 0.1472827814991478\n",
            "\n",
            "\n",
            "Epoch 543 -\n",
            "Updated K params after epoch 543: [-0.18729989 -0.96067778]\n",
            "Train loss: 0.16218882669500054, Test loss: 0.14687620489588565\n",
            "\n",
            "\n",
            "Epoch 544 -\n",
            "Updated K params after epoch 544: [-0.18728644 -0.96084761]\n",
            "Train loss: 0.1617716513673927, Test loss: 0.14647215601721006\n",
            "\n",
            "\n",
            "Epoch 545 -\n",
            "Updated K params after epoch 545: [-0.18727304 -0.96101706]\n",
            "Train loss: 0.16135686977703123, Test loss: 0.14607061215018888\n",
            "\n",
            "\n",
            "Epoch 546 -\n",
            "Updated K params after epoch 546: [-0.1872597  -0.96118612]\n",
            "Train loss: 0.1609444614919066, Test loss: 0.14567155082693617\n",
            "\n",
            "\n",
            "Epoch 547 -\n",
            "Updated K params after epoch 547: [-0.1872464  -0.96135478]\n",
            "Train loss: 0.16053440628970322, Test loss: 0.14527494982187736\n",
            "\n",
            "\n",
            "Epoch 548 -\n",
            "Updated K params after epoch 548: [-0.18723316 -0.96152306]\n",
            "Train loss: 0.16012668415561607, Test loss: 0.14488078714903618\n",
            "\n",
            "\n",
            "Epoch 549 -\n",
            "Updated K params after epoch 549: [-0.18721997 -0.96169096]\n",
            "Train loss: 0.15972127528018276, Test loss: 0.14448904105934524\n",
            "\n",
            "\n",
            "Epoch 550 -\n",
            "Updated K params after epoch 550: [-0.18720683 -0.96185846]\n",
            "Train loss: 0.15931816005713118, Test loss: 0.1440996900379789\n",
            "\n",
            "\n",
            "Epoch 551 -\n",
            "Updated K params after epoch 551: [-0.18719375 -0.96202559]\n",
            "Train loss: 0.15891731908124268, Test loss: 0.1437127128017097\n",
            "\n",
            "\n",
            "Epoch 552 -\n",
            "Updated K params after epoch 552: [-0.18718071 -0.96219233]\n",
            "Train loss: 0.15851873314623188, Test loss: 0.14332808829628807\n",
            "\n",
            "\n",
            "Epoch 553 -\n",
            "Updated K params after epoch 553: [-0.18716773 -0.96235869]\n",
            "Train loss: 0.15812238324264213, Test loss: 0.1429457956938457\n",
            "\n",
            "\n",
            "Epoch 554 -\n",
            "Updated K params after epoch 554: [-0.1871548  -0.96252468]\n",
            "Train loss: 0.15772825055575804, Test loss: 0.14256581439032243\n",
            "\n",
            "\n",
            "Epoch 555 -\n",
            "Updated K params after epoch 555: [-0.18714192 -0.96269028]\n",
            "Train loss: 0.15733631646353421, Test loss: 0.14218812400291742\n",
            "\n",
            "\n",
            "Epoch 556 -\n",
            "Updated K params after epoch 556: [-0.1871291  -0.96285551]\n",
            "Train loss: 0.1569465625345409, Test loss: 0.14181270436756366\n",
            "\n",
            "\n",
            "Epoch 557 -\n",
            "Updated K params after epoch 557: [-0.18711632 -0.96302036]\n",
            "Train loss: 0.1565589705259268, Test loss: 0.1414395355364271\n",
            "\n",
            "\n",
            "Epoch 558 -\n",
            "Updated K params after epoch 558: [-0.1871036  -0.96318484]\n",
            "Train loss: 0.1561735223813986, Test loss: 0.14106859777542932\n",
            "\n",
            "\n",
            "Epoch 559 -\n",
            "Updated K params after epoch 559: [-0.18709093 -0.96334895]\n",
            "Train loss: 0.1557902002292179, Test loss: 0.1406998715617949\n",
            "\n",
            "\n",
            "Epoch 560 -\n",
            "Updated K params after epoch 560: [-0.18707831 -0.96351268]\n",
            "Train loss: 0.1554089863802155, Test loss: 0.14033333758162259\n",
            "\n",
            "\n",
            "Epoch 561 -\n",
            "Updated K params after epoch 561: [-0.18706574 -0.96367605]\n",
            "Train loss: 0.1550298633258226, Test loss: 0.13996897672748082\n",
            "\n",
            "\n",
            "Epoch 562 -\n",
            "Updated K params after epoch 562: [-0.18705322 -0.96383905]\n",
            "Train loss: 0.15465281373611997, Test loss: 0.13960677009602745\n",
            "\n",
            "\n",
            "Epoch 563 -\n",
            "Updated K params after epoch 563: [-0.18704076 -0.96400168]\n",
            "Train loss: 0.15427782045790425, Test loss: 0.13924669898565362\n",
            "\n",
            "\n",
            "Epoch 564 -\n",
            "Updated K params after epoch 564: [-0.18702834 -0.96416394]\n",
            "Train loss: 0.15390486651277183, Test loss: 0.13888874489415193\n",
            "\n",
            "\n",
            "Epoch 565 -\n",
            "Updated K params after epoch 565: [-0.18701598 -0.96432584]\n",
            "Train loss: 0.15353393509522037, Test loss: 0.1385328895164086\n",
            "\n",
            "\n",
            "Epoch 566 -\n",
            "Updated K params after epoch 566: [-0.18700367 -0.96448738]\n",
            "Train loss: 0.15316500957076798, Test loss: 0.1381791147421198\n",
            "\n",
            "\n",
            "Epoch 567 -\n",
            "Updated K params after epoch 567: [-0.18699141 -0.96464856]\n",
            "Train loss: 0.15279807347408975, Test loss: 0.1378274026535321\n",
            "\n",
            "\n",
            "Epoch 568 -\n",
            "Updated K params after epoch 568: [-0.1869792  -0.96480937]\n",
            "Train loss: 0.15243311050717198, Test loss: 0.13747773552320672\n",
            "\n",
            "\n",
            "Epoch 569 -\n",
            "Updated K params after epoch 569: [-0.18696704 -0.96496983]\n",
            "Train loss: 0.15207010453748424, Test loss: 0.1371300958118078\n",
            "\n",
            "\n",
            "Epoch 570 -\n",
            "Updated K params after epoch 570: [-0.18695494 -0.96512993]\n",
            "Train loss: 0.15170903959616874, Test loss: 0.1367844661659145\n",
            "\n",
            "\n",
            "Epoch 571 -\n",
            "Updated K params after epoch 571: [-0.18694288 -0.96528967]\n",
            "Train loss: 0.15134989987624728, Test loss: 0.13644082941585664\n",
            "\n",
            "\n",
            "Epoch 572 -\n",
            "Updated K params after epoch 572: [-0.18693088 -0.96544906]\n",
            "Train loss: 0.15099266973084607, Test loss: 0.13609916857357424\n",
            "\n",
            "\n",
            "Epoch 573 -\n",
            "Updated K params after epoch 573: [-0.18691892 -0.9656081 ]\n",
            "Train loss: 0.15063733367143758, Test loss: 0.13575946683050008\n",
            "\n",
            "\n",
            "Epoch 574 -\n",
            "Updated K params after epoch 574: [-0.18690702 -0.96576678]\n",
            "Train loss: 0.1502838763661002, Test loss: 0.13542170755546637\n",
            "\n",
            "\n",
            "Epoch 575 -\n",
            "Updated K params after epoch 575: [-0.18689517 -0.96592511]\n",
            "Train loss: 0.14993228263779523, Test loss: 0.1350858742926339\n",
            "\n",
            "\n",
            "Epoch 576 -\n",
            "Updated K params after epoch 576: [-0.18688337 -0.9660831 ]\n",
            "Train loss: 0.14958253746266123, Test loss: 0.13475195075944468\n",
            "\n",
            "\n",
            "Epoch 577 -\n",
            "Updated K params after epoch 577: [-0.18687162 -0.96624073]\n",
            "Train loss: 0.1492346259683256, Test loss: 0.1344199208445977\n",
            "\n",
            "\n",
            "Epoch 578 -\n",
            "Updated K params after epoch 578: [-0.18685992 -0.96639802]\n",
            "Train loss: 0.14888853343223343, Test loss: 0.13408976860604724\n",
            "\n",
            "\n",
            "Epoch 579 -\n",
            "Updated K params after epoch 579: [-0.18684827 -0.96655497]\n",
            "Train loss: 0.14854424527999377, Test loss: 0.1337614782690241\n",
            "\n",
            "\n",
            "Epoch 580 -\n",
            "Updated K params after epoch 580: [-0.18683667 -0.96671157]\n",
            "Train loss: 0.14820174708374245, Test loss: 0.1334350342240791\n",
            "\n",
            "\n",
            "Epoch 581 -\n",
            "Updated K params after epoch 581: [-0.18682512 -0.96686782]\n",
            "Train loss: 0.14786102456052247, Test loss: 0.13311042102514922\n",
            "\n",
            "\n",
            "Epoch 582 -\n",
            "Updated K params after epoch 582: [-0.18681363 -0.96702374]\n",
            "Train loss: 0.1475220635706809, Test loss: 0.1327876233876459\n",
            "\n",
            "\n",
            "Epoch 583 -\n",
            "Updated K params after epoch 583: [-0.18680218 -0.96717932]\n",
            "Train loss: 0.14718485011628307, Test loss: 0.1324666261865652\n",
            "\n",
            "\n",
            "Epoch 584 -\n",
            "Updated K params after epoch 584: [-0.18679078 -0.96733455]\n",
            "Train loss: 0.14684937033954285, Test loss: 0.13214741445462022\n",
            "\n",
            "\n",
            "Epoch 585 -\n",
            "Updated K params after epoch 585: [-0.18677944 -0.96748945]\n",
            "Train loss: 0.14651561052127013, Test loss: 0.13182997338039476\n",
            "\n",
            "\n",
            "Epoch 586 -\n",
            "Updated K params after epoch 586: [-0.18676814 -0.96764402]\n",
            "Train loss: 0.14618355707933484, Test loss: 0.13151428830651896\n",
            "\n",
            "\n",
            "Epoch 587 -\n",
            "Updated K params after epoch 587: [-0.18675689 -0.96779825]\n",
            "Train loss: 0.14585319656714693, Test loss: 0.1312003447278659\n",
            "\n",
            "\n",
            "Epoch 588 -\n",
            "Updated K params after epoch 588: [-0.18674569 -0.96795215]\n",
            "Train loss: 0.14552451567215322, Test loss: 0.13088812828976978\n",
            "\n",
            "\n",
            "Epoch 589 -\n",
            "Updated K params after epoch 589: [-0.18673455 -0.96810571]\n",
            "Train loss: 0.14519750121435018, Test loss: 0.13057762478626483\n",
            "\n",
            "\n",
            "Epoch 590 -\n",
            "Updated K params after epoch 590: [-0.18672345 -0.96825895]\n",
            "Train loss: 0.144872140144813, Test loss: 0.1302688201583449\n",
            "\n",
            "\n",
            "Epoch 591 -\n",
            "Updated K params after epoch 591: [-0.1867124  -0.96841185]\n",
            "Train loss: 0.14454841954424053, Test loss: 0.12996170049224406\n",
            "\n",
            "\n",
            "Epoch 592 -\n",
            "Updated K params after epoch 592: [-0.1867014  -0.96856443]\n",
            "Train loss: 0.14422632662151605, Test loss: 0.1296562520177375\n",
            "\n",
            "\n",
            "Epoch 593 -\n",
            "Updated K params after epoch 593: [-0.18669046 -0.96871668]\n",
            "Train loss: 0.14390584871228415, Test loss: 0.1293524611064623\n",
            "\n",
            "\n",
            "Epoch 594 -\n",
            "Updated K params after epoch 594: [-0.18667956 -0.9688686 ]\n",
            "Train loss: 0.14358697327754297, Test loss: 0.12905031427025893\n",
            "\n",
            "\n",
            "Epoch 595 -\n",
            "Updated K params after epoch 595: [-0.18666871 -0.9690202 ]\n",
            "Train loss: 0.1432696879022519, Test loss: 0.12874979815953194\n",
            "\n",
            "\n",
            "Epoch 596 -\n",
            "Updated K params after epoch 596: [-0.18665791 -0.96917148]\n",
            "Train loss: 0.14295398029395504, Test loss: 0.12845089956163092\n",
            "\n",
            "\n",
            "Epoch 597 -\n",
            "Updated K params after epoch 597: [-0.18664715 -0.96932244]\n",
            "Train loss: 0.1426398382814196, Test loss: 0.12815360539925028\n",
            "\n",
            "\n",
            "Epoch 598 -\n",
            "Updated K params after epoch 598: [-0.18663645 -0.96947307]\n",
            "Train loss: 0.14232724981328976, Test loss: 0.1278579027288491\n",
            "\n",
            "\n",
            "Epoch 599 -\n",
            "Updated K params after epoch 599: [-0.1866258  -0.96962339]\n",
            "Train loss: 0.14201620295675524, Test loss: 0.1275637787390893\n",
            "\n",
            "\n",
            "Epoch 600 -\n",
            "Updated K params after epoch 600: [-0.18661519 -0.96977339]\n",
            "Train loss: 0.14170668589623509, Test loss: 0.12727122074929337\n",
            "\n",
            "\n",
            "Epoch 601 -\n",
            "Updated K params after epoch 601: [-0.18660464 -0.96992307]\n",
            "Train loss: 0.141398686932076, Test loss: 0.12698021620792016\n",
            "\n",
            "\n",
            "Epoch 602 -\n",
            "Updated K params after epoch 602: [-0.18659413 -0.97007244]\n",
            "Train loss: 0.1410921944792654, Test loss: 0.12669075269105978\n",
            "\n",
            "\n",
            "Epoch 603 -\n",
            "Updated K params after epoch 603: [-0.18658367 -0.9702215 ]\n",
            "Train loss: 0.1407871970661592, Test loss: 0.12640281790094643\n",
            "\n",
            "\n",
            "Epoch 604 -\n",
            "Updated K params after epoch 604: [-0.18657326 -0.97037024]\n",
            "Train loss: 0.14048368333322356, Test loss: 0.1261163996644894\n",
            "\n",
            "\n",
            "Epoch 605 -\n",
            "Updated K params after epoch 605: [-0.1865629  -0.97051867]\n",
            "Train loss: 0.1401816420317913, Test loss: 0.12583148593182222\n",
            "\n",
            "\n",
            "Epoch 606 -\n",
            "Updated K params after epoch 606: [-0.18655259 -0.97066679]\n",
            "Train loss: 0.13988106202283215, Test loss: 0.1255480647748691\n",
            "\n",
            "\n",
            "Epoch 607 -\n",
            "Updated K params after epoch 607: [-0.18654232 -0.9708146 ]\n",
            "Train loss: 0.13958193227573717, Test loss: 0.12526612438592957\n",
            "\n",
            "\n",
            "Epoch 608 -\n",
            "Updated K params after epoch 608: [-0.1865321 -0.9709621]\n",
            "Train loss: 0.13928424186711694, Test loss: 0.12498565307627967\n",
            "\n",
            "\n",
            "Epoch 609 -\n",
            "Updated K params after epoch 609: [-0.18652193 -0.9711093 ]\n",
            "Train loss: 0.13898797997961362, Test loss: 0.12470663927479099\n",
            "\n",
            "\n",
            "Epoch 610 -\n",
            "Updated K params after epoch 610: [-0.18651181 -0.97125619]\n",
            "Train loss: 0.13869313590072635, Test loss: 0.12442907152656625\n",
            "\n",
            "\n",
            "Epoch 611 -\n",
            "Updated K params after epoch 611: [-0.18650174 -0.97140278]\n",
            "Train loss: 0.1383996990216503, Test loss: 0.12415293849159181\n",
            "\n",
            "\n",
            "Epoch 612 -\n",
            "Updated K params after epoch 612: [-0.18649171 -0.97154907]\n",
            "Train loss: 0.13810765883612908, Test loss: 0.12387822894340675\n",
            "\n",
            "\n",
            "Epoch 613 -\n",
            "Updated K params after epoch 613: [-0.18648174 -0.97169505]\n",
            "Train loss: 0.1378170049393203, Test loss: 0.12360493176778835\n",
            "\n",
            "\n",
            "Epoch 614 -\n",
            "Updated K params after epoch 614: [-0.18647181 -0.97184073]\n",
            "Train loss: 0.137527727026674, Test loss: 0.12333303596145366\n",
            "\n",
            "\n",
            "Epoch 615 -\n",
            "Updated K params after epoch 615: [-0.18646192 -0.97198612]\n",
            "Train loss: 0.13723981489282455, Test loss: 0.12306253063077739\n",
            "\n",
            "\n",
            "Epoch 616 -\n",
            "Updated K params after epoch 616: [-0.18645209 -0.97213121]\n",
            "Train loss: 0.13695325843049458, Test loss: 0.12279340499052518\n",
            "\n",
            "\n",
            "Epoch 617 -\n",
            "Updated K params after epoch 617: [-0.1864423 -0.972276 ]\n",
            "Train loss: 0.13666804762941237, Test loss: 0.1225256483626029\n",
            "\n",
            "\n",
            "Epoch 618 -\n",
            "Updated K params after epoch 618: [-0.18643256 -0.97242049]\n",
            "Train loss: 0.1363841725752411, Test loss: 0.12225925017482145\n",
            "\n",
            "\n",
            "Epoch 619 -\n",
            "Updated K params after epoch 619: [-0.18642286 -0.97256469]\n",
            "Train loss: 0.13610162344852114, Test loss: 0.12199419995967627\n",
            "\n",
            "\n",
            "Epoch 620 -\n",
            "Updated K params after epoch 620: [-0.18641322 -0.9727086 ]\n",
            "Train loss: 0.13582039052362416, Test loss: 0.12173048735314296\n",
            "\n",
            "\n",
            "Epoch 621 -\n",
            "Updated K params after epoch 621: [-0.18640361 -0.97285221]\n",
            "Train loss: 0.13554046416771948, Test loss: 0.12146810209348664\n",
            "\n",
            "\n",
            "Epoch 622 -\n",
            "Updated K params after epoch 622: [-0.18639406 -0.97299554]\n",
            "Train loss: 0.13526183483975257, Test loss: 0.12120703402008712\n",
            "\n",
            "\n",
            "Epoch 623 -\n",
            "Updated K params after epoch 623: [-0.18638455 -0.97313857]\n",
            "Train loss: 0.13498449308943555, Test loss: 0.12094727307227784\n",
            "\n",
            "\n",
            "Epoch 624 -\n",
            "Updated K params after epoch 624: [-0.18637509 -0.97328132]\n",
            "Train loss: 0.13470842955624893, Test loss: 0.1206888092881999\n",
            "\n",
            "\n",
            "Epoch 625 -\n",
            "Updated K params after epoch 625: [-0.18636568 -0.97342378]\n",
            "Train loss: 0.13443363496845562, Test loss: 0.12043163280366959\n",
            "\n",
            "\n",
            "Epoch 626 -\n",
            "Updated K params after epoch 626: [-0.18635631 -0.97356596]\n",
            "Train loss: 0.13416010014212615, Test loss: 0.12017573385106056\n",
            "\n",
            "\n",
            "Epoch 627 -\n",
            "Updated K params after epoch 627: [-0.18634699 -0.97370784]\n",
            "Train loss: 0.13388781598017527, Test loss: 0.11992110275819964\n",
            "\n",
            "\n",
            "Epoch 628 -\n",
            "Updated K params after epoch 628: [-0.18633771 -0.97384945]\n",
            "Train loss: 0.13361677347140977, Test loss: 0.11966772994727641\n",
            "\n",
            "\n",
            "Epoch 629 -\n",
            "Updated K params after epoch 629: [-0.18632848 -0.97399077]\n",
            "Train loss: 0.13334696368958768, Test loss: 0.11941560593376607\n",
            "\n",
            "\n",
            "Epoch 630 -\n",
            "Updated K params after epoch 630: [-0.1863193  -0.97413181]\n",
            "Train loss: 0.1330783777924882, Test loss: 0.11916472132536621\n",
            "\n",
            "\n",
            "Epoch 631 -\n",
            "Updated K params after epoch 631: [-0.18631016 -0.97427258]\n",
            "Train loss: 0.13281100702099283, Test loss: 0.11891506682094626\n",
            "\n",
            "\n",
            "Epoch 632 -\n",
            "Updated K params after epoch 632: [-0.18630106 -0.97441306]\n",
            "Train loss: 0.1325448426981768, Test loss: 0.11866663320951024\n",
            "\n",
            "\n",
            "Epoch 633 -\n",
            "Updated K params after epoch 633: [-0.18629202 -0.97455326]\n",
            "Train loss: 0.13227987622841156, Test loss: 0.11841941136917254\n",
            "\n",
            "\n",
            "Epoch 634 -\n",
            "Updated K params after epoch 634: [-0.18628301 -0.97469319]\n",
            "Train loss: 0.1320160990964779, Test loss: 0.11817339226614598\n",
            "\n",
            "\n",
            "Epoch 635 -\n",
            "Updated K params after epoch 635: [-0.18627406 -0.97483284]\n",
            "Train loss: 0.13175350286668902, Test loss: 0.11792856695374306\n",
            "\n",
            "\n",
            "Epoch 636 -\n",
            "Updated K params after epoch 636: [-0.18626515 -0.97497222]\n",
            "Train loss: 0.13149207918202413, Test loss: 0.11768492657138882\n",
            "\n",
            "\n",
            "Epoch 637 -\n",
            "Updated K params after epoch 637: [-0.18625628 -0.97511132]\n",
            "Train loss: 0.13123181976327236, Test loss: 0.117442462343647\n",
            "\n",
            "\n",
            "Epoch 638 -\n",
            "Updated K params after epoch 638: [-0.18624746 -0.97525015]\n",
            "Train loss: 0.13097271640818647, Test loss: 0.11720116557925711\n",
            "\n",
            "\n",
            "Epoch 639 -\n",
            "Updated K params after epoch 639: [-0.18623868 -0.97538871]\n",
            "Train loss: 0.1307147609906468, Test loss: 0.11696102767018454\n",
            "\n",
            "\n",
            "Epoch 640 -\n",
            "Updated K params after epoch 640: [-0.18622995 -0.975527  ]\n",
            "Train loss: 0.1304579454598347, Test loss: 0.1167220400906817\n",
            "\n",
            "\n",
            "Epoch 641 -\n",
            "Updated K params after epoch 641: [-0.18622126 -0.97566502]\n",
            "Train loss: 0.13020226183941588, Test loss: 0.11648419439636125\n",
            "\n",
            "\n",
            "Epoch 642 -\n",
            "Updated K params after epoch 642: [-0.18621262 -0.97580277]\n",
            "Train loss: 0.12994770222673357, Test loss: 0.11624748222328055\n",
            "\n",
            "\n",
            "Epoch 643 -\n",
            "Updated K params after epoch 643: [-0.18620402 -0.97594026]\n",
            "Train loss: 0.12969425879201063, Test loss: 0.11601189528703772\n",
            "\n",
            "\n",
            "Epoch 644 -\n",
            "Updated K params after epoch 644: [-0.18619547 -0.97607748]\n",
            "Train loss: 0.12944192377756156, Test loss: 0.11577742538187878\n",
            "\n",
            "\n",
            "Epoch 645 -\n",
            "Updated K params after epoch 645: [-0.18618696 -0.97621443]\n",
            "Train loss: 0.12919068949701348, Test loss: 0.11554406437981583\n",
            "\n",
            "\n",
            "Epoch 646 -\n",
            "Updated K params after epoch 646: [-0.18617849 -0.97635112]\n",
            "Train loss: 0.12894054833453647, Test loss: 0.1153118042297562\n",
            "\n",
            "\n",
            "Epoch 647 -\n",
            "Updated K params after epoch 647: [-0.18617007 -0.97648755]\n",
            "Train loss: 0.12869149274408268, Test loss: 0.11508063695664249\n",
            "\n",
            "\n",
            "Epoch 648 -\n",
            "Updated K params after epoch 648: [-0.18616169 -0.97662372]\n",
            "Train loss: 0.1284435152486348, Test loss: 0.11485055466060307\n",
            "\n",
            "\n",
            "Epoch 649 -\n",
            "Updated K params after epoch 649: [-0.18615336 -0.97675962]\n",
            "Train loss: 0.12819660843946293, Test loss: 0.11462154951611316\n",
            "\n",
            "\n",
            "Epoch 650 -\n",
            "Updated K params after epoch 650: [-0.18614506 -0.97689527]\n",
            "Train loss: 0.12795076497539068, Test loss: 0.11439361377116657\n",
            "\n",
            "\n",
            "Epoch 651 -\n",
            "Updated K params after epoch 651: [-0.18613682 -0.97703066]\n",
            "Train loss: 0.12770597758206925, Test loss: 0.11416673974645716\n",
            "\n",
            "\n",
            "Epoch 652 -\n",
            "Updated K params after epoch 652: [-0.18612861 -0.97716579]\n",
            "Train loss: 0.1274622390512608, Test loss: 0.1139409198345709\n",
            "\n",
            "\n",
            "Epoch 653 -\n",
            "Updated K params after epoch 653: [-0.18612045 -0.97730066]\n",
            "Train loss: 0.12721954224012968, Test loss: 0.11371614649918765\n",
            "\n",
            "\n",
            "Epoch 654 -\n",
            "Updated K params after epoch 654: [-0.18611233 -0.97743528]\n",
            "Train loss: 0.12697788007054225, Test loss: 0.1134924122742929\n",
            "\n",
            "\n",
            "Epoch 655 -\n",
            "Updated K params after epoch 655: [-0.18610426 -0.97756964]\n",
            "Train loss: 0.12673724552837473, Test loss: 0.11326970976339915\n",
            "\n",
            "\n",
            "Epoch 656 -\n",
            "Updated K params after epoch 656: [-0.18609623 -0.97770375]\n",
            "Train loss: 0.1264976316628295, Test loss: 0.1130480316387771\n",
            "\n",
            "\n",
            "Epoch 657 -\n",
            "Updated K params after epoch 657: [-0.18608824 -0.97783761]\n",
            "Train loss: 0.12625903158575932, Test loss: 0.11282737064069605\n",
            "\n",
            "\n",
            "Epoch 658 -\n",
            "Updated K params after epoch 658: [-0.18608029 -0.97797122]\n",
            "Train loss: 0.12602143847099906, Test loss: 0.11260771957667388\n",
            "\n",
            "\n",
            "Epoch 659 -\n",
            "Updated K params after epoch 659: [-0.18607239 -0.97810458]\n",
            "Train loss: 0.125784845553706, Test loss: 0.11238907132073618\n",
            "\n",
            "\n",
            "Epoch 660 -\n",
            "Updated K params after epoch 660: [-0.18606453 -0.97823768]\n",
            "Train loss: 0.12554924612970747, Test loss: 0.11217141881268444\n",
            "\n",
            "\n",
            "Epoch 661 -\n",
            "Updated K params after epoch 661: [-0.18605671 -0.97837054]\n",
            "Train loss: 0.12531463355485598, Test loss: 0.11195475505737337\n",
            "\n",
            "\n",
            "Epoch 662 -\n",
            "Updated K params after epoch 662: [-0.18604893 -0.97850315]\n",
            "Train loss: 0.12508100124439234, Test loss: 0.11173907312399706\n",
            "\n",
            "\n",
            "Epoch 663 -\n",
            "Updated K params after epoch 663: [-0.1860412  -0.97863552]\n",
            "Train loss: 0.12484834267231598, Test loss: 0.11152436614538379\n",
            "\n",
            "\n",
            "Epoch 664 -\n",
            "Updated K params after epoch 664: [-0.18603351 -0.97876764]\n",
            "Train loss: 0.12461665137076271, Test loss: 0.11131062731729974\n",
            "\n",
            "\n",
            "Epoch 665 -\n",
            "Updated K params after epoch 665: [-0.18602586 -0.97889952]\n",
            "Train loss: 0.12438592092938974, Test loss: 0.11109784989776102\n",
            "\n",
            "\n",
            "Epoch 666 -\n",
            "Updated K params after epoch 666: [-0.18601825 -0.97903115]\n",
            "Train loss: 0.124156144994768, Test loss: 0.11088602720635427\n",
            "\n",
            "\n",
            "Epoch 667 -\n",
            "Updated K params after epoch 667: [-0.18601068 -0.97916254]\n",
            "Train loss: 0.12392731726978148, Test loss: 0.11067515262356535\n",
            "\n",
            "\n",
            "Epoch 668 -\n",
            "Updated K params after epoch 668: [-0.18600315 -0.97929369]\n",
            "Train loss: 0.12369943151303363, Test loss: 0.11046521959011674\n",
            "\n",
            "\n",
            "Epoch 669 -\n",
            "Updated K params after epoch 669: [-0.18599567 -0.97942459]\n",
            "Train loss: 0.12347248153826071, Test loss: 0.11025622160631246\n",
            "\n",
            "\n",
            "Epoch 670 -\n",
            "Updated K params after epoch 670: [-0.18598823 -0.97955526]\n",
            "Train loss: 0.12324646121375211, Test loss: 0.11004815223139137\n",
            "\n",
            "\n",
            "Epoch 671 -\n",
            "Updated K params after epoch 671: [-0.18598083 -0.97968569]\n",
            "Train loss: 0.1230213644617771, Test loss: 0.10984100508288829\n",
            "\n",
            "\n",
            "Epoch 672 -\n",
            "Updated K params after epoch 672: [-0.18597347 -0.97981588]\n",
            "Train loss: 0.12279718525801875, Test loss: 0.1096347738360029\n",
            "\n",
            "\n",
            "Epoch 673 -\n",
            "Updated K params after epoch 673: [-0.18596615 -0.97994584]\n",
            "Train loss: 0.12257391763101406, Test loss: 0.10942945222297623\n",
            "\n",
            "\n",
            "Epoch 674 -\n",
            "Updated K params after epoch 674: [-0.18595887 -0.98007556]\n",
            "Train loss: 0.12235155566160083, Test loss: 0.10922503403247494\n",
            "\n",
            "\n",
            "Epoch 675 -\n",
            "Updated K params after epoch 675: [-0.18595163 -0.98020505]\n",
            "Train loss: 0.12213009348237104, Test loss: 0.10902151310898298\n",
            "\n",
            "\n",
            "Epoch 676 -\n",
            "Updated K params after epoch 676: [-0.18594443 -0.9803343 ]\n",
            "Train loss: 0.12190952527713027, Test loss: 0.10881888335220063\n",
            "\n",
            "\n",
            "Epoch 677 -\n",
            "Updated K params after epoch 677: [-0.18593728 -0.98046331]\n",
            "Train loss: 0.12168984528036385, Test loss: 0.10861713871645094\n",
            "\n",
            "\n",
            "Epoch 678 -\n",
            "Updated K params after epoch 678: [-0.18593016 -0.9805921 ]\n",
            "Train loss: 0.12147104777670895, Test loss: 0.10841627321009323\n",
            "\n",
            "\n",
            "Epoch 679 -\n",
            "Updated K params after epoch 679: [-0.18592308 -0.98072066]\n",
            "Train loss: 0.12125312710043275, Test loss: 0.10821628089494388\n",
            "\n",
            "\n",
            "Epoch 680 -\n",
            "Updated K params after epoch 680: [-0.18591605 -0.98084898]\n",
            "Train loss: 0.12103607763491701, Test loss: 0.10801715588570397\n",
            "\n",
            "\n",
            "Epoch 681 -\n",
            "Updated K params after epoch 681: [-0.18590905 -0.98097708]\n",
            "Train loss: 0.12081989381214833, Test loss: 0.10781889234939404\n",
            "\n",
            "\n",
            "Epoch 682 -\n",
            "Updated K params after epoch 682: [-0.1859021  -0.98110494]\n",
            "Train loss: 0.12060457011221436, Test loss: 0.10762148450479536\n",
            "\n",
            "\n",
            "Epoch 683 -\n",
            "Updated K params after epoch 683: [-0.18589518 -0.98123258]\n",
            "Train loss: 0.12039010106280597, Test loss: 0.10742492662189851\n",
            "\n",
            "\n",
            "Epoch 684 -\n",
            "Updated K params after epoch 684: [-0.1858883  -0.98135999]\n",
            "Train loss: 0.12017648123872524, Test loss: 0.10722921302135789\n",
            "\n",
            "\n",
            "Epoch 685 -\n",
            "Updated K params after epoch 685: [-0.18588147 -0.98148718]\n",
            "Train loss: 0.11996370526139877, Test loss: 0.10703433807395353\n",
            "\n",
            "\n",
            "Epoch 686 -\n",
            "Updated K params after epoch 686: [-0.18587467 -0.98161414]\n",
            "Train loss: 0.11975176779839727, Test loss: 0.1068402962000588\n",
            "\n",
            "\n",
            "Epoch 687 -\n",
            "Updated K params after epoch 687: [-0.18586791 -0.98174088]\n",
            "Train loss: 0.11954066356296006, Test loss: 0.10664708186911499\n",
            "\n",
            "\n",
            "Epoch 688 -\n",
            "Updated K params after epoch 688: [-0.18586119 -0.9818674 ]\n",
            "Train loss: 0.11933038731352537, Test loss: 0.10645468959911183\n",
            "\n",
            "\n",
            "Epoch 689 -\n",
            "Updated K params after epoch 689: [-0.18585451 -0.98199369]\n",
            "Train loss: 0.11912093385326628, Test loss: 0.10626311395607455\n",
            "\n",
            "\n",
            "Epoch 690 -\n",
            "Updated K params after epoch 690: [-0.18584787 -0.98211976]\n",
            "Train loss: 0.11891229802963155, Test loss: 0.10607234955355684\n",
            "\n",
            "\n",
            "Epoch 691 -\n",
            "Updated K params after epoch 691: [-0.18584127 -0.98224561]\n",
            "Train loss: 0.11870447473389201, Test loss: 0.10588239105214019\n",
            "\n",
            "\n",
            "Epoch 692 -\n",
            "Updated K params after epoch 692: [-0.18583471 -0.98237124]\n",
            "Train loss: 0.11849745890069226, Test loss: 0.10569323315893882\n",
            "\n",
            "\n",
            "Epoch 693 -\n",
            "Updated K params after epoch 693: [-0.18582818 -0.98249666]\n",
            "Train loss: 0.11829124550760725, Test loss: 0.10550487062711109\n",
            "\n",
            "\n",
            "Epoch 694 -\n",
            "Updated K params after epoch 694: [-0.1858217  -0.98262185]\n",
            "Train loss: 0.11808582957470433, Test loss: 0.1053172982553761\n",
            "\n",
            "\n",
            "Epoch 695 -\n",
            "Updated K params after epoch 695: [-0.18581525 -0.98274683]\n",
            "Train loss: 0.11788120616411003, Test loss: 0.10513051088753661\n",
            "\n",
            "\n",
            "Epoch 696 -\n",
            "Updated K params after epoch 696: [-0.18580884 -0.98287159]\n",
            "Train loss: 0.1176773703795819, Test loss: 0.1049445034120074\n",
            "\n",
            "\n",
            "Epoch 697 -\n",
            "Updated K params after epoch 697: [-0.18580247 -0.98299614]\n",
            "Train loss: 0.11747431736608531, Test loss: 0.10475927076134921\n",
            "\n",
            "\n",
            "Epoch 698 -\n",
            "Updated K params after epoch 698: [-0.18579614 -0.98312047]\n",
            "Train loss: 0.11727204230937513, Test loss: 0.10457480791180837\n",
            "\n",
            "\n",
            "Epoch 699 -\n",
            "Updated K params after epoch 699: [-0.18578985 -0.98324459]\n",
            "Train loss: 0.11707054043558211, Test loss: 0.10439110988286184\n",
            "\n",
            "\n",
            "Epoch 700 -\n",
            "Updated K params after epoch 700: [-0.18578359 -0.98336849]\n",
            "Train loss: 0.11686980701080403, Test loss: 0.10420817173676762\n",
            "\n",
            "\n",
            "Epoch 701 -\n",
            "Updated K params after epoch 701: [-0.18577737 -0.98349219]\n",
            "Train loss: 0.11666983734070167, Test loss: 0.10402598857812055\n",
            "\n",
            "\n",
            "Epoch 702 -\n",
            "Updated K params after epoch 702: [-0.18577119 -0.98361567]\n",
            "Train loss: 0.11647062677009903, Test loss: 0.10384455555341342\n",
            "\n",
            "\n",
            "Epoch 703 -\n",
            "Updated K params after epoch 703: [-0.18576505 -0.98373894]\n",
            "Train loss: 0.11627217068258856, Test loss: 0.10366386785060301\n",
            "\n",
            "\n",
            "Epoch 704 -\n",
            "Updated K params after epoch 704: [-0.18575894 -0.983862  ]\n",
            "Train loss: 0.11607446450014057, Test loss: 0.10348392069868169\n",
            "\n",
            "\n",
            "Epoch 705 -\n",
            "Updated K params after epoch 705: [-0.18575287 -0.98398485]\n",
            "Train loss: 0.11587750368271729, Test loss: 0.10330470936725375\n",
            "\n",
            "\n",
            "Epoch 706 -\n",
            "Updated K params after epoch 706: [-0.18574684 -0.9841075 ]\n",
            "Train loss: 0.11568128372789116, Test loss: 0.10312622916611677\n",
            "\n",
            "\n",
            "Epoch 707 -\n",
            "Updated K params after epoch 707: [-0.18574085 -0.98422993]\n",
            "Train loss: 0.11548580017046758, Test loss: 0.10294847544484814\n",
            "\n",
            "\n",
            "Epoch 708 -\n",
            "Updated K params after epoch 708: [-0.18573489 -0.98435217]\n",
            "Train loss: 0.11529104858211195, Test loss: 0.10277144359239614\n",
            "\n",
            "\n",
            "Epoch 709 -\n",
            "Updated K params after epoch 709: [-0.18572897 -0.98447419]\n",
            "Train loss: 0.11509702457098085, Test loss: 0.10259512903667613\n",
            "\n",
            "\n",
            "Epoch 710 -\n",
            "Updated K params after epoch 710: [-0.18572308 -0.98459601]\n",
            "Train loss: 0.11490372378135749, Test loss: 0.1024195272441712\n",
            "\n",
            "\n",
            "Epoch 711 -\n",
            "Updated K params after epoch 711: [-0.18571724 -0.98471763]\n",
            "Train loss: 0.11471114189329126, Test loss: 0.1022446337195377\n",
            "\n",
            "\n",
            "Epoch 712 -\n",
            "Updated K params after epoch 712: [-0.18571143 -0.98483904]\n",
            "Train loss: 0.11451927462224139, Test loss: 0.10207044400521542\n",
            "\n",
            "\n",
            "Epoch 713 -\n",
            "Updated K params after epoch 713: [-0.18570565 -0.98496025]\n",
            "Train loss: 0.11432811771872446, Test loss: 0.10189695368104208\n",
            "\n",
            "\n",
            "Epoch 714 -\n",
            "Updated K params after epoch 714: [-0.18569991 -0.98508126]\n",
            "Train loss: 0.11413766696796643, Test loss: 0.10172415836387266\n",
            "\n",
            "\n",
            "Epoch 715 -\n",
            "Updated K params after epoch 715: [-0.18569421 -0.98520207]\n",
            "Train loss: 0.11394791818955773, Test loss: 0.10155205370720294\n",
            "\n",
            "\n",
            "Epoch 716 -\n",
            "Updated K params after epoch 716: [-0.18568855 -0.98532268]\n",
            "Train loss: 0.11375886723711309, Test loss: 0.10138063540079746\n",
            "\n",
            "\n",
            "Epoch 717 -\n",
            "Updated K params after epoch 717: [-0.18568292 -0.98544309]\n",
            "Train loss: 0.11357050999793468, Test loss: 0.10120989917032194\n",
            "\n",
            "\n",
            "Epoch 718 -\n",
            "Updated K params after epoch 718: [-0.18567732 -0.9855633 ]\n",
            "Train loss: 0.11338284239267925, Test loss: 0.10103984077697994\n",
            "\n",
            "\n",
            "Epoch 719 -\n",
            "Updated K params after epoch 719: [-0.18567177 -0.98568331]\n",
            "Train loss: 0.11319586037502893, Test loss: 0.10087045601715348\n",
            "\n",
            "\n",
            "Epoch 720 -\n",
            "Updated K params after epoch 720: [-0.18566625 -0.98580312]\n",
            "Train loss: 0.11300955993136584, Test loss: 0.10070174072204835\n",
            "\n",
            "\n",
            "Epoch 721 -\n",
            "Updated K params after epoch 721: [-0.18566076 -0.98592274]\n",
            "Train loss: 0.11282393708045022, Test loss: 0.10053369075734296\n",
            "\n",
            "\n",
            "Epoch 722 -\n",
            "Updated K params after epoch 722: [-0.18565531 -0.98604216]\n",
            "Train loss: 0.11263898787310216, Test loss: 0.10036630202284186\n",
            "\n",
            "\n",
            "Epoch 723 -\n",
            "Updated K params after epoch 723: [-0.18564989 -0.98616139]\n",
            "Train loss: 0.11245470839188708, Test loss: 0.10019957045213253\n",
            "\n",
            "\n",
            "Epoch 724 -\n",
            "Updated K params after epoch 724: [-0.18564451 -0.98628042]\n",
            "Train loss: 0.11227109475080456, Test loss: 0.10003349201224691\n",
            "\n",
            "\n",
            "Epoch 725 -\n",
            "Updated K params after epoch 725: [-0.18563917 -0.98639926]\n",
            "Train loss: 0.11208814309498064, Test loss: 0.09986806270332628\n",
            "\n",
            "\n",
            "Epoch 726 -\n",
            "Updated K params after epoch 726: [-0.18563386 -0.98651791]\n",
            "Train loss: 0.11190584960036364, Test loss: 0.09970327855829025\n",
            "\n",
            "\n",
            "Epoch 727 -\n",
            "Updated K params after epoch 727: [-0.18562858 -0.98663637]\n",
            "Train loss: 0.11172421047342343, Test loss: 0.09953913564250955\n",
            "\n",
            "\n",
            "Epoch 728 -\n",
            "Updated K params after epoch 728: [-0.18562334 -0.98675463]\n",
            "Train loss: 0.11154322195085384, Test loss: 0.09937563005348243\n",
            "\n",
            "\n",
            "Epoch 729 -\n",
            "Updated K params after epoch 729: [-0.18561813 -0.9868727 ]\n",
            "Train loss: 0.11136288029927853, Test loss: 0.099212757920515\n",
            "\n",
            "\n",
            "Epoch 730 -\n",
            "Updated K params after epoch 730: [-0.18561296 -0.98699059]\n",
            "Train loss: 0.1111831818149601, Test loss: 0.09905051540440506\n",
            "\n",
            "\n",
            "Epoch 731 -\n",
            "Updated K params after epoch 731: [-0.18560783 -0.98710828]\n",
            "Train loss: 0.11100412282351248, Test loss: 0.09888889869712963\n",
            "\n",
            "\n",
            "Epoch 732 -\n",
            "Updated K params after epoch 732: [-0.18560272 -0.98722579]\n",
            "Train loss: 0.11082569967961645, Test loss: 0.09872790402153608\n",
            "\n",
            "\n",
            "Epoch 733 -\n",
            "Updated K params after epoch 733: [-0.18559766 -0.9873431 ]\n",
            "Train loss: 0.11064790876673822, Test loss: 0.09856752763103661\n",
            "\n",
            "\n",
            "Epoch 734 -\n",
            "Updated K params after epoch 734: [-0.18559262 -0.98746023]\n",
            "Train loss: 0.11047074649685135, Test loss: 0.09840776580930662\n",
            "\n",
            "\n",
            "Epoch 735 -\n",
            "Updated K params after epoch 735: [-0.18558762 -0.98757718]\n",
            "Train loss: 0.11029420931016162, Test loss: 0.0982486148699859\n",
            "\n",
            "\n",
            "Epoch 736 -\n",
            "Updated K params after epoch 736: [-0.18558266 -0.98769394]\n",
            "Train loss: 0.11011829367483485, Test loss: 0.09809007115638389\n",
            "\n",
            "\n",
            "Epoch 737 -\n",
            "Updated K params after epoch 737: [-0.18557772 -0.98781051]\n",
            "Train loss: 0.10994299608672789, Test loss: 0.09793213104118784\n",
            "\n",
            "\n",
            "Epoch 738 -\n",
            "Updated K params after epoch 738: [-0.18557282 -0.9879269 ]\n",
            "Train loss: 0.10976831306912241, Test loss: 0.09777479092617437\n",
            "\n",
            "\n",
            "Epoch 739 -\n",
            "Updated K params after epoch 739: [-0.18556796 -0.98804311]\n",
            "Train loss: 0.10959424117246189, Test loss: 0.09761804724192452\n",
            "\n",
            "\n",
            "Epoch 740 -\n",
            "Updated K params after epoch 740: [-0.18556313 -0.98815913]\n",
            "Train loss: 0.10942077697409106, Test loss: 0.09746189644754165\n",
            "\n",
            "\n",
            "Epoch 741 -\n",
            "Updated K params after epoch 741: [-0.18555833 -0.98827497]\n",
            "Train loss: 0.10924791707799873, Test loss: 0.09730633503037293\n",
            "\n",
            "\n",
            "Epoch 742 -\n",
            "Updated K params after epoch 742: [-0.18555357 -0.98839063]\n",
            "Train loss: 0.10907565811456295, Test loss: 0.0971513595057337\n",
            "\n",
            "\n",
            "Epoch 743 -\n",
            "Updated K params after epoch 743: [-0.18554883 -0.98850611]\n",
            "Train loss: 0.10890399674029927, Test loss: 0.09699696641663495\n",
            "\n",
            "\n",
            "Epoch 744 -\n",
            "Updated K params after epoch 744: [-0.18554414 -0.9886214 ]\n",
            "Train loss: 0.10873292963761176, Test loss: 0.09684315233351415\n",
            "\n",
            "\n",
            "Epoch 745 -\n",
            "Updated K params after epoch 745: [-0.18553947 -0.98873652]\n",
            "Train loss: 0.10856245351454649, Test loss: 0.09668991385396873\n",
            "\n",
            "\n",
            "Epoch 746 -\n",
            "Updated K params after epoch 746: [-0.18553484 -0.98885146]\n",
            "Train loss: 0.10839256510454799, Test loss: 0.09653724760249283\n",
            "\n",
            "\n",
            "Epoch 747 -\n",
            "Updated K params after epoch 747: [-0.18553024 -0.98896622]\n",
            "Train loss: 0.10822326116621823, Test loss: 0.09638515023021682\n",
            "\n",
            "\n",
            "Epoch 748 -\n",
            "Updated K params after epoch 748: [-0.18552567 -0.9890808 ]\n",
            "Train loss: 0.1080545384830782, Test loss: 0.09623361841464999\n",
            "\n",
            "\n",
            "Epoch 749 -\n",
            "Updated K params after epoch 749: [-0.18552114 -0.98919521]\n",
            "Train loss: 0.1078863938633322, Test loss: 0.09608264885942575\n",
            "\n",
            "\n",
            "Epoch 750 -\n",
            "Updated K params after epoch 750: [-0.18551664 -0.98930944]\n",
            "Train loss: 0.10771882413963453, Test loss: 0.09593223829405009\n",
            "\n",
            "\n",
            "Train accuracy: 0.9726526891522334, Test accuracy: 0.9781818181818182\n",
            "Train f1: 0.9688796680497925, Test f1: 0.9749999999999999\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Printing some final results -"
      ],
      "metadata": {
        "id": "s2378jsYQZmb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Hence, final metrics are- Train accuracy = 0.97, Test accuracy = 0.98, Train f1 = 0.97, Test f1 = 0.97 (rounded off to 2 decimal places)"
      ],
      "metadata": {
        "id": "HPGvshugPjkP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Final parameter values: {best_params}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d_47RA196YSc",
        "outputId": "50cd415a-015c-474b-cbd0-b34b9f8448a3"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final parameter values: {'W1': array([[ 0.10689448,  0.03742489,  0.04357396,  0.00988056],\n",
            "       [-0.46380323, -0.26090419, -0.24955799, -0.11067228],\n",
            "       [-0.08271358, -0.04601017, -0.03676133, -0.03911787],\n",
            "       [-0.03680734, -0.01803599, -0.01368764, -0.01896291],\n",
            "       [-0.37421405, -0.19593965, -0.21238063, -0.0754684 ],\n",
            "       [ 0.20940998,  0.12485714,  0.12607856,  0.05263991],\n",
            "       [ 0.57417769,  0.28250666,  0.30706175,  0.11286188],\n",
            "       [ 0.40230901,  0.21315116,  0.21148696,  0.08354074]]), 'b1': array([[-0.02090278],\n",
            "       [ 0.12946259],\n",
            "       [ 0.02676033],\n",
            "       [ 0.01405851],\n",
            "       [ 0.10419635],\n",
            "       [-0.05374983],\n",
            "       [-0.1455693 ],\n",
            "       [-0.10201892]]), 'W2': array([[ 0.08115792, -0.65140139, -0.14979401, -0.08747178, -0.53007166,\n",
            "         0.24373754,  0.69294725,  0.47855307]]), 'b2': array([[0.22692668]]), 'K': array([-0.18551664, -0.98930944])}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Plot of train, test loss versus epochs -\n",
        "epochs = list(range(1, epochs + 1))\n",
        "\n",
        "# Plotting the train and test loss values versus the number of epochs\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(epochs, tr_hist, label='Train Loss')\n",
        "plt.plot(epochs, te_hist, label='Test Loss')\n",
        "plt.xlabel('Number of Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Train and Test Loss versus Number of Epochs')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "0vmIGiNXaYf5",
        "outputId": "43c6dcf4-1df9-42c9-d517-292ddebc20cc"
      },
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Kk4ZGMKaB884"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}